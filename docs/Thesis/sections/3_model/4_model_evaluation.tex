\section{Evaluation}
\label{sec:model_evaluation}

Finally, REPLACE_WE evaluate REPLACE_OUR model by calculating different error metrics such as the root mean squared error as can be seen in Equation \ref{eq:rmse}, and the cross entropy loss. REPLACE_WE also calculate the accuracy of the model, which is the percentage of correctly predicted faults. REPLACE_WE also calculate the precision and recall of the model. The precision is the percentage of correctly predicted faults out of all predicted faults. The recall is the percentage of correctly predicted faults out of all faults in the data. REPLACE_WE also calculate the F1 score, which is the harmonic mean of the precision and recall. The F1 score is a good indicator of the overall performance of the model.

%\begin{equation}
%Actually RMSE not a good metric for Classification, Will mention it but not in this much detail
%\label{eq:rmse}
%RMSE = \sqrt{\dfrac{1}{n}\sum_{i=0}^{n} (pred_i - gt_i)^2}
%\end{equation}

\textbf{Explain Confusion Matrix}

\textbf{Explain Cohen Kappa Metric}

\textbf{Briefly explain Accuracy, Precision, Recall and F1}

\textbf{Explain Testing}