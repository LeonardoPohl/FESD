@InProceedings % InterPart
{
    InterPart,
    author    = {Zhou, Yuxiao and Habermann, Marc and Habibie, Ikhsanul and Tewari, Ayush and Theobalt, Christian and Xu, Feng},
    title     = {Monocular Real-Time Full Body Capture With Inter-Part Correlations},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {6},
    year      = {2021},
    pages     = {4811-4822}
}

@InProceedings % Dripe
{
    Dripe,
    author    = {Guesdon, Romain and Crispim-Junior, Carlos and Tougne, Laure},
    title     = {DriPE: A Dataset for Human Pose Estimation in Real-World Driving Settings},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops},
    month     = {10},
    year      = {2021},
    pages     = {2865-2874}
}

@article % A study of the effect of noise and occlusion on the accuracy of applied to 3D object recognition
{ 
    AddNoiseToVirtual,
    title       = {A study of the effect of noise and occlusion on the accuracy of convolutional neural networks applied to 3D object recognition},
    journal     = {Computer Vision and Image Understanding},
    volume      = {164},
    pages       = {124-134},
    year        = {2017},
    note        = {Deep Learning for Computer Vision},
    issn        = {1077-3142},
    doi         = {https://doi.org/10.1016/j.cviu.2017.06.006},
    url         = {https://www.sciencedirect.com/science/article/pii/S1077314217301182},
    author      = {Alberto Garcia-Garcia and Jose Garcia-Rodriguez and Sergio Orts-Escolano and Sergiu Oprea and Francisco Gomez-Donoso and Miguel Cazorla},
    keywords    = {Deep learning, 3D object recognition, Convolutional neural networks, Noise, Occlusion, Caffe},
    abstract    = {In this work, we carry out a study of the effect of adverse conditions, which characterize real-world scenes, on the accuracy of a Convolutional Neural Network applied to 3D object class recognition. Firstly, we discuss possible ways of representing 3D data to feed the network. In addition, we propose a set of representations to be tested. Those representations consist of a grid-like structure (fixed and adaptive) and a measure for the occupancy of each cell of the grid (binary and normalized point density). After that, we propose and implement a Convolutional Neural Network for 3D object recognition using Caffe. At last, we carry out an in-depth study of the performance of the network over a 3D CAD model dataset, the Princeton ModelNet project, synthetically simulating occlusions and noise models featured by common RGB-D sensors. The results show that the volumetric representations for 3D data play a key role on the recognition process and Convolutional Neural Network can be considerably robust to noise and occlusions if a proper representation is chosen.}
}

@inproceedings % Blensor 1
{
    BlenSor-1,
    author = {Gschwandtner, Michael and Kwitt, Roland and Uhl, Andreas and Pree, Wolfgang},
    title = {BlenSor: Blender Sensor Simulation Toolbox},
    year = {2011},
    isbn = {9783642240300},
    publisher = {Springer-Verlag},
    address = {Berlin, Heidelberg},
    abstract = {This paper introduces a novel software package for the simulation of various types of range scanners. The goal is to provide researchers in the fields of obstacle detection, range data segmentation, obstacle tracking or surface reconstruction with a versatile and powerful software package that is easy to use and allows to focus on algorithmic improvements rather than on building the software framework around it. The simulation environment and the actual simulations can be efficiently distributed with a single compact file. Our proposed approach facilitates easy regeneration of published results, hereby highlighting the value of reproducible research.},
    booktitle = {Proceedings of the 7th International Conference on Advances in Visual Computing - Volume Part II},
    pages = {199â€“208},
    numpages = {10},
    location = {Las Vegas, NV, USA},
    series = {ISVC'11}
}

@phdthesis % Blensor 2
{
    Blensor-2,
    author = {Gschwandtner, Michael},
    title = {Support Framework for Obstacle Detection on Autonomous Trains},
    school = {Department of Computer Sciences, University of Salzburg},
    year = {2013}
}

@misc{MultiSourceOperation,
  doi = {10.48550/ARXIV.1701.07372},
  url = {https://arxiv.org/abs/1701.07372},
  author = {Kadkhodamohammadi, Abdolrahim and Gangi, Afshin and de Mathelin, Michel and Padoy, Nicolas},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {A Multi-view RGB-D Approach for Human Pose Estimation in Operating Rooms},
  publisher = {arXiv},
  year = {2017},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@ARTICLE{%NoiseMetrics
    NoiseMetrics,  
    author={Zhou Wang and Bovik, A.C. and Sheikh, H.R. and Simoncelli, E.P.},  
    journal={IEEE Transactions on Image Processing},   
    title={Image quality assessment: from error visibility to structural similarity},     
    year={2004},  
    volume={13}, 
    number={4}, 
    pages={600-612},  
    doi={10.1109/TIP.2003.819861}
}

% Monocular Depth Estimation
 
@article{mono_depthestimation_bts,
  doi = {10.48550/ARXIV.1907.10326},
  url = {https://arxiv.org/abs/1907.10326},
  author = {Lee, Jin Han and Han, Myung-Kyu and Ko, Dong Wook and Suh, Il Hong},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {From Big to Small: Multi-Scale Local Planar Guidance for Monocular Depth Estimation},
  publisher = {arXiv},
  year = {2019},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{mono_depthestimation_depthformer,
  doi = {10.48550/ARXIV.2203.14211},
  url = {https://arxiv.org/abs/2203.14211},
  author = {Li, Zhenyu and Chen, Zehui and Liu, Xianming and Jiang, Junjun},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {DepthFormer: Exploiting Long-Range Correlation and Local Information for Accurate Monocular Depth Estimation},
  publisher = {arXiv},
  year = {2022},
  copyright = {Creative Commons Attribution 4.0 International}
}

@misc{mono_depthestimation_glpn,
  doi = {10.48550/ARXIV.2201.07436},
  url = {https://arxiv.org/abs/2201.07436},
  author = {Kim, Doyeon and Ga, Woonghyun and Ahn, Pyungwhan and Joo, Donggyu and Chun, Sehwan and Kim, Junmo},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth},
  publisher = {arXiv},
  year = {2022},
  copyright = {Creative Commons Attribution 4.0 International}
}

 
 % Multiview Depth Estimation
 
 @misc{multiview_depthestimation_fusion,
  doi = {10.48550/ARXIV.2112.08177},
  url = {https://arxiv.org/abs/2112.08177},
  author = {Bae, Gwangbin and Budvytis, Ignas and Cipolla, Roberto},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Multi-View Depth Estimation by Fusing Single-View Depth Probability with Multi-View Geometry},
  publisher = {arXiv},
  year = {2021},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@INPROCEEDINGS{multiview_depthestimation_epipolar,  
author={Long, Xiaoxiao and Liu, Lingjie and Li, Wei and Theobalt, Christian and Wang, Wenping},  
booktitle={2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},   
title={Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks},   
year={2021},  
volume={},  
number={},  
pages={8254-8263},  
doi={10.1109/CVPR46437.2021.00816}
}

@Article{multiview_depthestimation_RealSense_and_any,
author={Tu, Li-fen
and Peng, Qi},
title={Method of Using RealSense Camera to Estimate the Depth Map of Any Monocular Camera},
journal={Journal of Electrical and Computer Engineering},
year={2021},
month={May},
day={18},
publisher={Hindawi},
volume={2021},
pages={9152035},
abstract={Robot detection, recognition, positioning, and other applications require not only real-time video image information but also the distance from the target to the camera, that is, depth information. This paper proposes a method to automatically generate any monocular camera depth map based on RealSense camera data. By using this method, any current single-camera detection system can be upgraded online. Without changing the original system, the depth information of the original monocular camera can be obtained simply, and the transition from 2D detection to 3D detection can be realized. In order to verify the effectiveness of the proposed method, a hardware system was constructed using the Micro-vision RS-A14K-GC8 industrial camera and the Intel RealSense D415 depth camera, and the depth map fitting algorithm proposed in this paper was used to test the system. The results show that, except for a few depth-missing areas, the results of other areas with depth are still good, which can basically describe the distance difference between the target and the camera. In addition, in order to verify the scalability of the method, a new hardware system was constructed with different cameras, and images were collected in a complex farmland environment. The generated depth map was good, which could basically describe the distance difference between the target and the camera.},
issn={2090-0147},
doi={10.1155/2021/9152035},
url={https://doi.org/10.1155/2021/9152035}
}

% Depth Completion

@misc{depthcompletion_SemAttNet,
  doi = {10.48550/ARXIV.2204.13635},
  url = {https://arxiv.org/abs/2204.13635},
  author = {Nazir, Danish and Liwicki, Marcus and Stricker, Didier and Afzal, Muhammad Zeshan},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {SemAttNet: Towards Attention-based Semantic Aware Guided Depth Completion},
  publisher = {arXiv},
  year = {2022},
  copyright = {Creative Commons Attribution Share Alike 4.0 International}
}

@misc{depthcompletion_PENet,
  doi = {10.48550/ARXIV.2103.00783},
  url = {https://arxiv.org/abs/2103.00783},
  author = {Hu, Mu and Wang, Shuling and Li, Bin and Ning, Shiyu and Fan, Li and Gong, Xiaojin},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {PENet: Towards Precise and Efficient Image Guided Depth Completion},
  publisher = {arXiv},  year = {2021},
  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International}
}

@misc{depthcompletion_NLSPN,
  doi = {10.48550/ARXIV.2007.10042},
  url = {https://arxiv.org/abs/2007.10042},
  author = {Park, Jinsun and Joo, Kyungdon and Hu, Zhe and Liu, Chi-Kuei and Kweon, In So},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Non-Local Spatial Propagation Network for Depth Completion},
  publisher = {arXiv},
  year = {2020},
  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International}
}

@misc{depthcompletion_FusionNet,
  doi = {10.48550/ARXIV.1902.05356},
  url = {https://arxiv.org/abs/1902.05356},
  author = {Van Gansbeke, Wouter and Neven, Davy and De Brabandere, Bert and Van Gool, Luc},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Sparse and noisy LiDAR completion with RGB guidance and uncertainty},
  publisher = {arXiv},
  year = {2019},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

% Cameras

@misc{multi_azure_kin, 
title={Synchronize multiple Azure Kinect DK devices}, 
url={https://docs.microsoft.com/en-us/azure/kinect-dk/multi-camera-sync}, 
journal={Microsoft Docs}, 
author={Sych, Tetyana}
} 

 @misc{realsense_multi, 
 title={Multi-camera configurations - D400 series stereo cameras}, 
 url={https://dev.intelrealsense.com/docs/multiple-depth-cameras-configuration}, 
 journal={IntelÂ® RealSenseâ„¢ Developer Documentation}, 
 author={Nie, John Woodfill}
 } 
 
@Article{cameras_multi_lidar,
author={Patil, Ashok Kumar
and Balasubramanyam, Adithya
and Ryu, Jae Yeong
and B N, Pavan Kumar
and Chakravarthi, Bharatesh
and Chai, Young Ho},
title={Fusion of Multiple Lidars and Inertial Sensors for the Real-Time Pose Tracking of Human Motion},
journal={Sensors (Basel, Switzerland)},
year={2020},
month={Sep},
day={18},
publisher={MDPI},
volume={20},
number={18},
pages={5342},
keywords={activity recognition; human motion; inertial sensor; lidar; locomotion; motion reconstruction; position estimation; position tracking; Actigraphy; *Computers; Humans; *Motion; Orientation; *Posture},
abstract={Today, enhancement in sensing technology enables the use of multiple sensors to track human motion/activity precisely. Tracking human motion has various applications, such as fitness training, healthcare, rehabilitation, human-computer interaction, virtual reality, and activity recognition. Therefore, the fusion of multiple sensors creates new opportunities to develop and improve an existing system. This paper proposes a pose-tracking system by fusing multiple three-dimensional (3D) light detection and ranging (lidar) and inertial measurement unit (IMU) sensors. The initial step estimates the human skeletal parameters proportional to the target user's height by extracting the point cloud from lidars. Next, IMUs are used to capture the orientation of each skeleton segment and estimate the respective joint positions. In the final stage, the displacement drift in the position is corrected by fusing the data from both sensors in real time. The installation setup is relatively effortless, flexible for sensor locations, and delivers results comparable to the state-of-the-art pose-tracking system. We evaluated the proposed system regarding its accuracy in the user's height estimation, full-body joint position estimation, and reconstruction of the 3D avatar. We used a publicly available dataset for the experimental evaluation wherever possible. The results reveal that the accuracy of height and the position estimation is well within an acceptable range of {\textpm}3-5 cm. The reconstruction of the motion based on the publicly available dataset and our data is precise and realistic.},
note={32961918[pmid]},
note={PMC7570691[pmcid]},
note={s20185342[PII]},
issn={1424-8220},
doi={10.3390/s20185342},
url={https://pubmed.ncbi.nlm.nih.gov/32961918},
url={https://doi.org/10.3390/s20185342},
language={eng}
}

% Software tools

@InProceedings{pcl,
  author    = {Radu Bogdan Rusu and Steve Cousins},
  title     = {{3D is here: Point Cloud Library (PCL)}},
  booktitle = {{IEEE International Conference on Robotics and Automation (ICRA)}},
  month     = {May 9-13},
  year      = {2011},
  address   = {Shanghai, China},
  publisher = {IEEE}
}

@ARTICLE{camera_calibration,  
    author  = {Zhang, Z.},  
    journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},   
    title   = {A flexible new technique for camera calibration},   
    year    = {2000},  
    volume  = {22},  
    number  = {11},  
    pages   = {1330-1334},  
    doi     = {10.1109/34.888718}
}

@inproceedings{floor-detection,
author = {Anagnostopoulos, Ioannis and PÄƒtrÄƒucean, Viorica and Brilakis, Ioannis and Vela, Patricio},
year = {2016},
month = {05},
pages = {2302-2311},
title = {Detection of Walls, Floors, and Ceilings in Point Cloud Data},
doi = {10.1061/9780784479827.229}
}

@phdthesis{NDT,
author = {Magnusson, Martin},
year = {2009},
month = {12},
pages = {},
title = {The Three-Dimensional Normal-Distributions Transform --- an Efficient Representation for Registration, Surface Analysis, and Loop Detection}
}