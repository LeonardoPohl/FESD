{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FESDModel\n",
    "\n",
    "FESD - Fault estimation for skeleton detection - is a suite that aims at finding faults in joints of skeletons, which are detected by human pose estimatiors.\n",
    "\n",
    "FESDData is the sister project to this notebook, which aims at recording depth and rgb data, as well as populating the data with human poses from variing human pose estimators.\n",
    "\n",
    "Furthermore, FESTData augments all data based on joint confidence.\n",
    "\n",
    "FFESDModel aims to develop and evaluate a model based on the faulty and augmented joint data as well as RGBD data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries\n",
    "\n",
    "We need a range of libraries which are imported here. We also define some constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num cuda GPUs: 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from time import time\n",
    "import datetime\n",
    "\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "from model import FESD, FESDv2, train, val, test\n",
    "\n",
    "from utils.mode import Mode\n",
    "from utils import get_model_iter, get_model_iter_all\n",
    "\n",
    "from data import Frame\n",
    "\n",
    "\n",
    "num_gpus = torch.cuda.device_count()\n",
    "is_cuda = torch.cuda.is_available()\n",
    "print(f\"Num cuda GPUs: {num_gpus}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Build the model according to the chosen mode"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model\n",
    "\n",
    "In the following we define the training function and train a network on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize = 60\n",
    "im_size = 64\n",
    "epochs = 100\n",
    "\n",
    "# gradient clipping margin\n",
    "clip = 0.5\n",
    "\n",
    "test_exercises = ['E-0.00', 'E-0.01', 'E-1.00', 'E-1.02', 'E-2.00', 'E-2.03', 'E-3.00', 'E-3.02']\n",
    "\n",
    "all_modes = True\n",
    "if not all_modes:\n",
    "    mode = Mode.FULL_BODY\n",
    "use_v2 = False\n",
    "\n",
    "model_id = f\"{'v2' if use_v2 else 'v1'}{'' if all_modes else '_' + mode.to_str()}_bs_{batchsize}_is_{im_size}_e_{epochs}\"\n",
    "\n",
    "CE = torch.nn.CrossEntropyLoss()\n",
    "if is_cuda:\n",
    "    CE = CE.cuda()\n",
    "\n",
    "model_iterator = []\n",
    "if all_modes:\n",
    "    model_iterator = get_model_iter_all(is_cuda, use_v2, test_exercises, epochs, batchsize, im_size, clip)\n",
    "else:\n",
    "    model_iterator = get_model_iter(mode, is_cuda, use_v2, test_exercises, epochs, batchsize, im_size, clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body_parts = np.array([\"Head\", \"Torso\", \"Left arm\", \"Right arm\", \"Left leg\", \"Right leg\"])\n",
    "\n",
    "for pack in model_iterator[0][4]:\n",
    "  rgb, depth, pose, gt, session = pack\n",
    "  print(rgb.shape, depth.shape, pose.shape)\n",
    "  \n",
    "# torch.onnx.export(model_iterator[0][1].module, merged_image, \"fesd.onnx\", verbose=True, input_names=[\"Merged Image\"], output_names=[\"Error Label\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "366c8376ab1a4b2b95cdd109c73c1ade",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---   1 ---\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m tic \u001b[39m=\u001b[39m time()\n\u001b[0;32m     29\u001b[0m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mempty_cache()\n\u001b[1;32m---> 31\u001b[0m train(train_loader, model, optimizer, CE, scheduler, clip, epoch, is_cuda, mode, df_model, use_v2)\n\u001b[0;32m     32\u001b[0m test(test_loader, model, CE, epoch, is_cuda, mode, df_model, use_v2)\n\u001b[0;32m     34\u001b[0m crit_1 \u001b[39m=\u001b[39m df_model[\u001b[39m\"\u001b[39m\u001b[39mepoch\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m==\u001b[39m epoch\n",
      "File \u001b[1;32md:\\FESD\\FESDModel\\model\\train.py:16\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(train_loader, model, optimizer, criterion, scheduler, clip, epoch, is_cuda, mode, df, use_v2)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m(train_loader: DataLoader, model, optimizer, criterion, scheduler, clip, epoch, is_cuda, mode, df, use_v2):\n\u001b[0;32m     14\u001b[0m   loss_record \u001b[39m=\u001b[39m AvgMeter()\n\u001b[1;32m---> 16\u001b[0m   \u001b[39mfor\u001b[39;00m i, pack \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader, start\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m     17\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     19\u001b[0m     \u001b[39mif\u001b[39;00m use_v2:      \n",
      "File \u001b[1;32md:\\FESD\\FESDModel\\.env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32md:\\FESD\\FESDModel\\.env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32md:\\FESD\\FESDModel\\.env\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32md:\\FESD\\FESDModel\\.env\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32md:\\FESD\\FESDModel\\data\\dataset.py:85\u001b[0m, in \u001b[0;36mFESDDataset.__getitem__\u001b[1;34m(self, i)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrandomize_augmentation_params:\n\u001b[0;32m     83\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maugmentation_params\u001b[39m.\u001b[39mRandomize()\n\u001b[1;32m---> 85\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframe \u001b[39m=\u001b[39m load_frame(recording_dir\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrecording_dir, session\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrecording_jsons[session], frame_id\u001b[39m=\u001b[39;49mindex, params\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maugmentation_params, mode\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmode, use_v2\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49muse_v2)\n\u001b[0;32m     87\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_v2:\n\u001b[0;32m     88\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_frame_v2()\n",
      "File \u001b[1;32md:\\FESD\\FESDModel\\data\\frame_loader.py:144\u001b[0m, in \u001b[0;36mload_frame\u001b[1;34m(recording_dir, session, frame_id, params, mode, use_v2)\u001b[0m\n\u001b[0;32m    141\u001b[0m rgb, depth \u001b[39m=\u001b[39m rgb\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mfloat32), depth\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mfloat32)\n\u001b[0;32m    143\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(file\u001b[39m=\u001b[39mrecording_dir \u001b[39m/\u001b[39m  session[\u001b[39m'\u001b[39m\u001b[39mSkeleton\u001b[39m\u001b[39m'\u001b[39m], mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m file:\n\u001b[1;32m--> 144\u001b[0m   skeleton_json \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39;49mload(file)[frame_id \u001b[39m*\u001b[39m \u001b[39m10\u001b[39m]\n\u001b[0;32m    145\u001b[0m   pose_2d, pose_3d, errors, bounding_boxes_2d, bounding_boxes_3d \u001b[39m=\u001b[39m load_skeletons(skeleton_json, params\u001b[39m.\u001b[39mflip, mode, use_v2)\n\u001b[0;32m    146\u001b[0m im_size \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(np\u001b[39m.\u001b[39mfloor(\u001b[39mmax\u001b[39m(\u001b[39mabs\u001b[39m(bounding_boxes_2d[\u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m] \u001b[39m-\u001b[39m bounding_boxes_2d[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]), \n\u001b[0;32m    147\u001b[0m                            \u001b[39mabs\u001b[39m(bounding_boxes_2d[\u001b[39m1\u001b[39m][\u001b[39m1\u001b[39m] \u001b[39m-\u001b[39m bounding_boxes_2d[\u001b[39m0\u001b[39m][\u001b[39m1\u001b[39m])))) \u001b[39m+\u001b[39m params\u001b[39m.\u001b[39mcrop_pad\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\json\\__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[1;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(fp, \u001b[39m*\u001b[39m, \u001b[39mcls\u001b[39m\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, object_hook\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, parse_float\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    275\u001b[0m         parse_int\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, parse_constant\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, object_pairs_hook\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw):\n\u001b[0;32m    276\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[0;32m    277\u001b[0m \u001b[39m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[0;32m    278\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[39m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 293\u001b[0m     \u001b[39mreturn\u001b[39;00m loads(fp\u001b[39m.\u001b[39;49mread(),\n\u001b[0;32m    294\u001b[0m         \u001b[39mcls\u001b[39m\u001b[39m=\u001b[39m\u001b[39mcls\u001b[39m, object_hook\u001b[39m=\u001b[39mobject_hook,\n\u001b[0;32m    295\u001b[0m         parse_float\u001b[39m=\u001b[39mparse_float, parse_int\u001b[39m=\u001b[39mparse_int,\n\u001b[0;32m    296\u001b[0m         parse_constant\u001b[39m=\u001b[39mparse_constant, object_pairs_hook\u001b[39m=\u001b[39mobject_pairs_hook, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_columns = [\"epoch\", \"iteration\", \"joint_id\",\n",
    "                  \"gts\", \"preds\", \"confidences\", \n",
    "                  \"Avg loss\", \"loss\", \"accuracy\", \n",
    "                  \"tp\", \"tn\", \"fp\", \"fn\", \"precision\", \"recall\", \"f1\", \n",
    "                  \"cohens_kappa\", \"learning_rate\",\n",
    "                  \"train_test\", \"exercise\", \"simplified\", \"mode\", \"use_v2\"]\n",
    "                  \n",
    "df_model = pd.DataFrame(columns=model_columns)\n",
    "pb = tqdm(range(1, epochs + 1), desc='Epoch')\n",
    "\n",
    "model_result_dir = f\"./results/{model_id}\"\n",
    "dir_fallback = \"_are_you_kidding_me_with_this_many_models\"\n",
    "\n",
    "try:\n",
    "  Path(model_result_dir).mkdir(parents=True)\n",
    "except:\n",
    "  for i in dir_fallback:\n",
    "    model_result_dir += f\"{i}\"\n",
    "    try:\n",
    "      Path(model_result_dir).mkdir(parents=True)\n",
    "      break\n",
    "    except:\n",
    "      pass\n",
    "\n",
    "model_result_dir = Path(model_result_dir)\n",
    "\n",
    "for epoch in pb:\n",
    "    print(f\"--- {epoch:3d} ---\")\n",
    "    for mode, model, optimizer, scheduler, train_loader, test_loader in model_iterator:\n",
    "        tic = time()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        train(train_loader, model, optimizer, CE, scheduler, clip, epoch, is_cuda, mode, df_model, use_v2)\n",
    "        test(test_loader, model, CE, epoch, is_cuda, mode, df_model, use_v2)\n",
    "\n",
    "        crit_1 = df_model[\"epoch\"] == epoch\n",
    "        crit_2 = df_model[\"mode\"] == mode.name.lower()\n",
    "        crit_3 = df_model[\"train_test\"] == \"test\"\n",
    "        last_row = df_model[crit_1 & crit_2 & crit_3].mean(numeric_only=True)\n",
    "        last_row[\"p\"] = last_row[\"tp\"] + last_row[\"fp\"]\n",
    "        last_row[\"n\"] = last_row[\"tn\"] + last_row[\"fn\"]\n",
    "        last_row[\"positives\"] = last_row[\"p\"] / (last_row[\"n\"] + last_row[\"p\"])\n",
    "        pb.set_description(f'Epoch (mode: {mode.name.lower().replace(\"_\", \" \"):>10}, lr: {optimizer.param_groups[0][\"lr\"]:.5f}, loss: {last_row[\"Avg loss\"]:.3f})')\n",
    "        \n",
    "        print(f'Epoch (mode: {mode.name.lower().replace(\"_\", \" \"):>10}, lr: {optimizer.param_groups[0][\"lr\"]:.3f}, loss: {last_row[\"Avg loss\"]:.5f}, acc: {last_row[\"accuracy\"]:.3f}, f1: {last_row[\"f1\"]:.3f}, precision: {last_row[\"precision\"]:.3f}, recall: {last_row[\"recall\"]:.3f}, kappa: {last_row[\"cohens_kappa\"]:.3f}, p/(p + n): {last_row[\"positives\"]:.3f}, time: {time() - tic:.2f}s)')      \n",
    "\n",
    "        if (epoch) % 10 == 0:\n",
    "            torch.save(model.state_dict(), os.path.join(Path('checkpoints'), f\"{model_id}_{mode.name.lower()}_{epoch}_ckpt.pth\")) \n",
    "    \n",
    "for mode, model, _, _, _, _ in model_iterator:\n",
    "    torch.save(model.state_dict(), model_result_dir / f\"{mode.name.lower()}_last_ckpt.pth\") \n",
    "    print(f\"model saved {os.path.join(model_result_dir, f'last_ckpt.pth')}!\")\n",
    "\n",
    "df_model.to_parquet(model_result_dir / f'ModelAnalysis.parquet.gzip', compression='gzip') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "223e9e27cbc670f5c22c33ebc1577b54f1cca48f7f6b973b1c50992e7fee7cdc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
