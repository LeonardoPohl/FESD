{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FESDModel\n",
    "\n",
    "FESD - Fault estimation for skeleton detection - is a suite that aims at finding faults in joints of skeletons, which are detected by human pose estimatiors.\n",
    "\n",
    "FESDData is the sister project to this notebook, which aims at recording depth and rgb data, as well as populating the data with human poses from variing human pose estimators.\n",
    "\n",
    "Furthermore, FESTData augments all data based on joint confidence.\n",
    "\n",
    "FFESDModel aims to develop and evaluate a model based on the faulty and augmented joint data as well as RGBD data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries\n",
    "\n",
    "We need a range of libraries which are imported here. We also define some constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "h:\\FESD\\FESDModel\\env\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num cuda GPUs: 1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from time import time\n",
    "\n",
    "from data import FESDDataset\n",
    "from data import Frame, AugmentationParams\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "import cv2\n",
    "\n",
    "from model import RD3D\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from utils import AvgMeter, clip_gradient, get_scheduler\n",
    "\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(f\"Num cuda GPUs: {num_gpus}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RECORDING_DIR = Path('H:/Recordings/')\n",
    "CHECKPOINT_DIR = Path('checkpoints')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "Firstly we need to import all the recordings into the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(file=\"Exercises.json\", mode='r') as file:\n",
    "  exercises_json = json.load(file)['Exercises']\n",
    "\n",
    "with open(file=\"JointErrors.json\", mode='r') as file:\n",
    "  joint_error_json = json.load(file)\n",
    "\n",
    "with open(file=\"SkeletonErrors.json\", mode='r') as file:\n",
    "  skeleton_error_json = json.load(file)\n",
    "\n",
    "len(exercises_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recordings Found: 25\n",
      "Total Frames: 7500\n"
     ]
    }
   ],
   "source": [
    "batchsize = 10\n",
    "train_size = 352\n",
    "\n",
    "dataset = FESDDataset(RECORDING_DIR, train_size)\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batchsize)\n",
    "# build dataloader\n",
    "n_data = len(train_loader.dataset)\n",
    "CE = torch.nn.BCEWithLogitsLoss().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 480, 640])\n",
      "All Missing Joints\n",
      "tensor([1, 1, 1, 1, 1], dtype=torch.int8)\n",
      "All Wrong Joints\n",
      "tensor([[ -1.4217, 167.1556,   0.2603],\n",
      "        [ -1.1519, 170.9957,   0.2511]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "dataset.randomize_augmentation_params = False\n",
    "dataset.reset_augmentation_params()\n",
    "frame, rgb, depth, pose_2d, errors = dataset[0]\n",
    "frame.show()\n",
    "\n",
    "print(\"All Missing Joints\")\n",
    "print(errors[errors==1])\n",
    "\n",
    "print(\"All Wrong Joints\")\n",
    "print(pose_2d[errors==2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model\n",
    "\n",
    "The model proposed by RD3D is based on resnet50 so we copy a pretrained resnet50 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = torchvision.models.resnet50(pretrained=True)\n",
    "\n",
    "model = RD3D(32, copy.deepcopy(resnet))\n",
    "print(model)\n",
    "\n",
    "model = nn.DataParallel(model).cuda()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "\n",
    "In the following we define the training function and train a network on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "def train(train_loader, model, optimizer, criterion, scheduler, epoch, epochs):\n",
    "    # multi-scale training  \n",
    "\n",
    "    #model.train()\n",
    "    loss_record = AvgMeter()\n",
    "    for i, pack in enumerate(train_loader, start=1):\n",
    "        # optimizer.zero_grad()\n",
    "\n",
    "        # # RD3D\n",
    "        # rgb = frame.rgb\n",
    "        # depth = frame.depth\n",
    "\n",
    "        # images = torch.cat([rgb, depth], 2)\n",
    "\n",
    "        # # Seperate Network\n",
    "        # pose = frame.poses_2d\n",
    "\n",
    "        # gt = frame.errors\n",
    "        \n",
    "        \n",
    "\n",
    "    #     images = images.cuda()\n",
    "    #     gts = gts.cuda()\n",
    "    #     depths = depths.cuda()\n",
    "\n",
    "    #     # multi-scale training samples\n",
    "    #     trainsize = int(round(opt.trainsize * rate / 32) * 32)\n",
    "    #     if rate != 1:\n",
    "    #         images = F.upsample(images, size=(trainsize, trainsize), mode='bilinear', align_corners=True)\n",
    "    #         images = images.unsqueeze(2)\n",
    "    #         gts = F.upsample(gts, size=(trainsize, trainsize), mode='bilinear', align_corners=True)\n",
    "\n",
    "    #         depths = F.upsample(depths, size=(trainsize, trainsize), mode='bilinear', align_corners=True)\n",
    "    #         depths = depths.unsqueeze(2)\n",
    "    #         images = torch.cat([images, depths], 2)\n",
    "\n",
    "    #     if rate == 1:\n",
    "    #         images = images.unsqueeze(2)\n",
    "    #         depths = depths.unsqueeze(2)\n",
    "    #         images = torch.cat([images, depths], 2)\n",
    "\n",
    "    #     # forward\n",
    "    #     pred_s = model(images)\n",
    "    #     # TODO Calculate different loss based on the error label\n",
    "    #     loss = criterion(pred_s, gts)\n",
    "\n",
    "    #     loss.backward()\n",
    "    #     clip_gradient(optimizer, opt.clip)\n",
    "    #     optimizer.step()\n",
    "    #     scheduler.step()\n",
    "    #     if rate == 1:\n",
    "            \n",
    "    #         loss_record.update(loss.data, opt.batchsize)\n",
    "      \n",
    "        if i % 100 == 0 or i == len(train_loader):\n",
    "            logger.info('Epoch [{:03d}/{:03d}], Step [{:04d}/{:04d}], Loss: {:.4f}'.\n",
    "                        format(epoch, epochs, i, len(train_loader),\n",
    "                               loss_record.show()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch number\n",
    "epochs = 50\n",
    "# optimizer\n",
    "optim = 'adamW'\n",
    "# learning rate\n",
    "learning_rate = 0.000125\n",
    "# learning rate scheduler. can be step, poly or cosine\n",
    "lr_scheduler = 'cosine'\n",
    "# warmup epoch\n",
    "warmup_epoch = -1\n",
    "# warmup multiplier\n",
    "warmup_multiplier = 100\n",
    "# for step scheduler. where to decay lr, can be a list\n",
    "lr_decay_epochs = [120, 160, 200]\n",
    "# for step scheduler. step size to decay lr\n",
    "lr_decay_steps = 20 \n",
    "# for step scheduler. decay rate for learning rate\n",
    "lr_decay_rate = 0.1\n",
    "# weight decay\n",
    "weight_decay = 0.0001\n",
    "# momentum for SGD\n",
    "momentum = 0.9\n",
    "# gradient clipping margin\n",
    "clip = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if optim == 'adam':\n",
    "    optimizer = torch.optim.Adam(model.parameters(), learning_rate, weight_decay=weight_decay)\n",
    "elif optim == 'adamW':\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), learning_rate, weight_decay=weight_decay)\n",
    "elif optim == 'sdg':\n",
    "    optimizer = torch.optim.SGD(model.parameters(), learning_rate / 10.0 * batchsize, momentum=momentum, weight_decay=weight_decay)\n",
    "\n",
    "scheduler = get_scheduler(optimizer, len(train_loader), lr_scheduler, lr_decay_epochs, lr_decay_steps, lr_decay_rate, epochs, warmup_epoch, warmup_multiplier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# routine\n",
    "for epoch in range(1, epochs + 1):\n",
    "    tic = time()\n",
    "    train(train_loader, model, optimizer, CE, scheduler, epoch, epochs)\n",
    "    print(f'epoch {epoch}, total time {time() - tic:.2f}, learning_rate {optimizer.param_groups[0][\"lr\"]}')\n",
    "\n",
    "    if (epoch) % 10 == 0:\n",
    "        # torch.save(model.state_dict(), os.path.join(opt.output_dir, f\"RD3D_{epoch}_ckpt.pth\"))\n",
    "        print(f'epoch {epoch}, total time {time() - tic:.2f}, learning_rate {optimizer.param_groups[0][\"lr\"]}')\n",
    "\n",
    "        print(\"checkpoint saved {}!\".format(os.path.join(opt.output_dir, f\"{epoch}_ckpt.pth\")))\n",
    "# torch.save(model.state_dict(), os.path.join(CHECKPOINT_DIR, f\"last_ckpt.pth\"))\n",
    "print(f\"model saved {os.path.join(CHECKPOINT_DIR, f'last_ckpt.pth')}!\")\n",
    "checkpoint = os.path.join(opt.output_dir, f\"last_ckpt.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, pack in enumerate(train_loader, start=1):\n",
    "  print(pack)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "223e9e27cbc670f5c22c33ebc1577b54f1cca48f7f6b973b1c50992e7fee7cdc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
