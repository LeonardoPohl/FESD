{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FESDModel\n",
    "\n",
    "FESD - Fault estimation for skeleton detection - is a suite that aims at finding faults in joints of skeletons, which are detected by human pose estimatiors.\n",
    "\n",
    "FESDData is the sister project to this notebook, which aims at recording depth and rgb data, as well as populating the data with human poses from variing human pose estimators.\n",
    "\n",
    "Furthermore, FESTData augments all data based on joint confidence.\n",
    "\n",
    "FFESDModel aims to develop and evaluate a model based on the faulty and augmented joint data as well as RGBD data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries\n",
    "\n",
    "We need a range of libraries which are imported here. We also define some constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import NamedTuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import tqdm\n",
    "\n",
    "import cv2\n",
    "import pyrealsense2 as rs\n",
    "\n",
    "sns.set_style()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "RECORDING_DIR = Path('H:/Recordings/')\n",
    "FIGURES_DIR = Path('../docs/Thesis/figures/')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "Firstly we need to import all the recordings into the notebook.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Metadata\n",
    "\n",
    "It is important to load the metadata, such as the session parameters, the exercises and the recording paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recording_jsons = []\n",
    "for file in os.listdir(RECORDING_DIR):\n",
    "  if (file.endswith('.json')):\n",
    "    with open(file=os.path.join(RECORDING_DIR, file), mode='r') as file:\n",
    "      data = json.load(file)\n",
    "      recording_jsons.append(data)\n",
    "\n",
    "len(recording_jsons)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we load the Exercises from the exercise file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(file=\"Exercises.json\", mode='r') as file:\n",
    "  exercises_json = json.load(file)['Exercises']\n",
    "\n",
    "with open(file=\"JointErrors.json\", mode='r') as file:\n",
    "  joint_error_json = json.load(file)\n",
    "\n",
    "with open(file=\"SkeletonErrors.json\", mode='r') as file:\n",
    "  skeleton_error_json = json.load(file)\n",
    "\n",
    "len(exercises_json)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Frame\n",
    "\n",
    "Here we define the load frame functions. For now we focus on nuitrack recordings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id_2_name(i: int):\n",
    "  return 'frame_' + str(i) + '.yml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Frame:\n",
    "    rgb: np.ndarray\n",
    "    depth: np.ndarray\n",
    "    poses_2d: np.ndarray\n",
    "    poses_3d: np.ndarray\n",
    "    errors: np.ndarray\n",
    "\n",
    "@dataclass\n",
    "class AugmentationParams:\n",
    "    flip: bool = False\n",
    "    crop: bool = False\n",
    "    crop_random: bool = False\n",
    "    crop_pad: int = 0\n",
    "    gaussian: bool = False\n",
    "    seed: int = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Unsure if this is a problem but bounding boxes will change making crop size different\n",
    "#       Possible solution: If multiple frames are used to represent a frame, use the same bounding box for all frames\n",
    "#                          For query frames, use the bounding box of the first frame and return list of list of frames as video\n",
    "# Question: Should the videos be overlapping in frames?\n",
    "def load_skeletons(skeletons_json, flip: bool=False) -> (np.ndarray, np.ndarray, list[tuple[float, float, float]], list[tuple[float, float, float]]):\n",
    "  poses = []\n",
    "  pose_errors = []\n",
    "  bounding_boxes_2d = [(np.inf, np.inf, np.inf), (0, 0, 0)]\n",
    "  bounding_boxes_3d = [(np.inf, np.inf, np.inf), (0, 0, 0)]\n",
    "  for person in skeletons_json:\n",
    "    joints = np.ndarray(shape=[0, 6])\n",
    "    errors = []    \n",
    "    origin = person['Skeleton'][4]\n",
    "\n",
    "    for joint in person['Skeleton']:\n",
    "      if (joint['error'] != 1):\n",
    "        bounding_boxes_2d[0] = np.minimum(bounding_boxes_2d[0], [joint['u'], joint['v'], joint['d']])\n",
    "        bounding_boxes_2d[1] = np.maximum(bounding_boxes_2d[1], [joint['u'], joint['v'], joint['d']])\n",
    "        bounding_boxes_3d[0] = np.minimum(bounding_boxes_3d[0], [joint['x'], joint['y'], joint['z']])\n",
    "        bounding_boxes_3d[1] = np.maximum(bounding_boxes_3d[1], [joint['x'], joint['y'], joint['z']])\n",
    "      joints = np.append(joints, [[\n",
    "        joint['u'] - origin['u'],\n",
    "        joint['v'] - origin['v'],\n",
    "        joint['d'] - origin['d'],\n",
    "        joint['x'] - origin['x'],\n",
    "        joint['y'] - origin['y'],\n",
    "        joint['z'] - origin['z']\n",
    "      ]], axis=0) * (-1 if flip else 1)\n",
    "\n",
    "      errors.append(1 if person['error'] == 1 else joint['error'])\n",
    "              \n",
    "    poses.append(joints)\n",
    "    pose_errors.append(errors)\n",
    "  \n",
    "  return np.asarray(poses), np.asarray(pose_errors), bounding_boxes_2d, bounding_boxes_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_frames(session: json, start: int, stop: int, params: AugmentationParams = AugmentationParams()) -> list[Frame]:\n",
    "  frames = []\n",
    "\n",
    "  if (start > stop):\n",
    "    start, stop = stop, start\n",
    "  if (start < 0):\n",
    "    start = 0\n",
    "  if (stop > session['Frames']):\n",
    "    stop = session['Frames'] \n",
    "  \n",
    "  params.seed = np.random.randint(0, 100000)\n",
    "  \n",
    "  for frame_id in range(start, stop):\n",
    "    frames.append(load_frame(session, frame_id, params))\n",
    "  \n",
    "  return frames\n",
    "\n",
    "def load_frame(session: json, frame_id: int, params: AugmentationParams = AugmentationParams()) -> Frame:\n",
    "  frame_path = RECORDING_DIR /  session['Cameras'][0]['FileName'] / id_2_name(frame_id)\n",
    "  print(frame_path)\n",
    "  frame_file = cv2.FileStorage(str(frame_path), cv2.FileStorage_READ)\n",
    "  frame = np.asarray( frame_file.getNode('frame').mat()[:,:] )\n",
    "  rgb, depth = np.split(frame, [3], axis=2)\n",
    "\n",
    "  with open(file=RECORDING_DIR /  session['Skeleton'], mode='r') as file:\n",
    "    skeleton_json = json.load(file)[frame_id]\n",
    "  poses, errors, bounding_boxes_2d, bounding_boxes_3d = load_skeletons(skeleton_json, params.flip)\n",
    "  pose_2d, pose_3d = np.split(poses, 2, axis=2)\n",
    "    \n",
    "  if (params.flip):\n",
    "    rgb = np.flip(rgb, axis=1)\n",
    "    depth = np.flip(depth, axis=1)\n",
    "\n",
    "  if params.crop or params.crop_random:\n",
    "    min_x = max(0, int(np.floor(bounding_boxes_2d[0][1])) - params.crop_pad)\n",
    "    min_y = max(0, int(np.floor(bounding_boxes_2d[0][0])) - params.crop_pad)\n",
    "\n",
    "    max_x = min(rgb.shape[1], int(np.ceil(bounding_boxes_2d[1][1])) + params.crop_pad)\n",
    "    max_y = min(rgb.shape[0], int(np.ceil(bounding_boxes_2d[1][0])) + params.crop_pad)\n",
    "\n",
    "    if (params.crop_random):\n",
    "      if (params.seed == -1):\n",
    "        seed = np.random.randint(0, 100000)\n",
    "      else:\n",
    "        seed = params.seed\n",
    "      np.random.seed(seed)\n",
    "      min_x = np.random.randint(0, min_x)\n",
    "      min_y = np.random.randint(0, min_y)\n",
    "      max_x = np.random.randint(max_x, rgb.shape[1])\n",
    "      max_y = np.random.randint(max_y, rgb.shape[0])\n",
    "\n",
    "    rgb = rgb[min_x:max_x, min_y:max_y]\n",
    "    depth = depth[min_x:max_x, min_y:max_y] \n",
    "  \n",
    "  if (params.gaussian):\n",
    "    rgb = cv2.GaussianBlur(rgb, (5, 5), 0)\n",
    "    depth = cv2.GaussianBlur(depth, (5, 5), 0)\n",
    "\n",
    "  return Frame(rgb, depth, pose_2d, pose_3d, errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H:\\Recordings\\Session_2023-02-28T22.58.23\\Frames\\frame_25.yml\n",
      "All Missing Joints\n",
      "[[-349.36 -304.77 -2.62]\n",
      " [-349.36 -304.77 -2.62]\n",
      " [-349.36 -304.77 -2.62]\n",
      " [-349.36 -304.77 -2.62]\n",
      " [-349.36 -304.77 -2.62]]\n",
      "All Wrong Joints\n",
      "[[1.26 168.93 0.26]\n",
      " [-0.73 172.46 0.25]]\n"
     ]
    }
   ],
   "source": [
    "frame = load_frame(session=recording_jsons[0], frame_id=25, params=AugmentationParams(flip=False, crop=False, crop_random=False, crop_pad=0, gaussian=True))\n",
    "float_formatter = \"{:.2f}\".format\n",
    "np.set_printoptions(formatter={'float_kind':float_formatter})\n",
    "# print(\"All Correct Joints\")\n",
    "# print(frame.poses_2d[frame.errors==0])\n",
    "print(\"All Missing Joints\")\n",
    "print(frame.poses_2d[frame.errors==1])\n",
    "print(\"All Wrong Joints\")\n",
    "print(frame.poses_2d[frame.errors==2])\n",
    "\n",
    "depth_norm = frame.depth / 5\n",
    "depth_im = np.dstack((depth_norm, depth_norm, depth_norm)) \n",
    "im = np.hstack((frame.rgb, depth_im))\n",
    "\n",
    "cv2.imwrite(str(FIGURES_DIR / 'Model' / 'Augmentation' / 'GaussianBlur.png'), 255*im)\n",
    "cv2.imshow('depth', im)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H:\\Recordings\\Session_2023-02-28T22.58.23\\Frames\\frame_0.yml\n",
      "crop 286 147 423 477\n",
      "crop_random 112 78 440 610 51096\n",
      "H:\\Recordings\\Session_2023-02-28T22.58.23\\Frames\\frame_1.yml\n",
      "crop 286 147 423 477\n",
      "crop_random 112 78 440 610 51096\n",
      "H:\\Recordings\\Session_2023-02-28T22.58.23\\Frames\\frame_2.yml\n",
      "crop 286 147 423 477\n",
      "crop_random 112 78 440 610 51096\n",
      "H:\\Recordings\\Session_2023-02-28T22.58.23\\Frames\\frame_3.yml\n",
      "crop 286 147 422 477\n",
      "crop_random 112 78 439 610 51096\n",
      "H:\\Recordings\\Session_2023-02-28T22.58.23\\Frames\\frame_4.yml\n",
      "crop 286 147 422 477\n",
      "crop_random 112 78 439 610 51096\n",
      "H:\\Recordings\\Session_2023-02-28T22.58.23\\Frames\\frame_5.yml\n",
      "crop 286 147 422 477\n",
      "crop_random 112 78 439 610 51096\n",
      "H:\\Recordings\\Session_2023-02-28T22.58.23\\Frames\\frame_6.yml\n",
      "crop 286 147 422 477\n",
      "crop_random 112 78 439 610 51096\n",
      "H:\\Recordings\\Session_2023-02-28T22.58.23\\Frames\\frame_7.yml\n",
      "crop 286 147 422 477\n",
      "crop_random 112 78 439 610 51096\n",
      "H:\\Recordings\\Session_2023-02-28T22.58.23\\Frames\\frame_8.yml\n",
      "crop 287 147 422 477\n",
      "crop_random 112 78 439 610 51096\n",
      "H:\\Recordings\\Session_2023-02-28T22.58.23\\Frames\\frame_9.yml\n",
      "crop 287 147 422 477\n",
      "crop_random 112 78 439 610 51096\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "a = AugmentationParams(flip=True, crop=False, crop_random=False, crop_pad=0, gaussian=True)\n",
    "frames = load_frames(session=recording_jsons[0], start=0, stop=10, params=a)\n",
    "for frame in frames:\n",
    "  print(frames[0].rgb.shape == frame.rgb.shape)\n",
    "  print(frames[0].depth.shape == frame.depth.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 3.49257782e+02,  3.04989929e+02,  2.61749840e+00],\n",
       "        [-1.84570312e-01, -1.57876678e+02, -1.63670063e-01],\n",
       "        [ 4.53582764e-01,  1.26848038e+02,  1.31840229e-01],\n",
       "        [ 2.45849609e-01, -4.82750244e+01, -1.82793140e-02],\n",
       "        [-0.00000000e+00, -0.00000000e+00, -0.00000000e+00],\n",
       "        [-2.71636963e-01, -1.06412277e+02, -1.03270292e-01],\n",
       "        [-4.05320740e+01,  1.04062683e+02,  9.98923779e-02],\n",
       "        [ 5.69803772e+01, -4.21243286e+01, -3.98993492e-03],\n",
       "        [-6.95727234e+01, -1.72396545e+01,  4.21814919e-02],\n",
       "        [ 7.21361389e+01,  2.93243713e+01, -4.98199463e-02],\n",
       "        [ 3.49257782e+02,  3.04989929e+02,  2.61749840e+00],\n",
       "        [-2.71636963e-01, -1.06412277e+02, -1.03270292e-01],\n",
       "        [ 4.05550842e+01,  1.02272476e+02,  8.75868797e-02],\n",
       "        [-5.27978516e+01, -3.88483887e+01,  5.90515137e-03],\n",
       "        [ 5.99971619e+01, -2.09994202e+01,  2.67663002e-02],\n",
       "        [-6.14588928e+01,  3.31505737e+01, -3.33003998e-02],\n",
       "        [ 3.49257782e+02,  3.04989929e+02,  2.61749840e+00],\n",
       "        [ 2.54044189e+01,  9.76605225e+00,  4.63178158e-02],\n",
       "        [-1.69927673e+01, -1.09090118e+02, -1.15725994e-02],\n",
       "        [ 1.01470947e+00,  1.67784637e+02,  2.55832195e-01],\n",
       "        [ 3.49257782e+02,  3.04989929e+02,  2.61749840e+00],\n",
       "        [-2.56338806e+01,  1.04048767e+01,  5.10828495e-02],\n",
       "        [ 1.61321106e+01, -1.09205811e+02, -2.27441788e-02],\n",
       "        [-1.91772461e-01,  1.71189026e+02,  2.46545315e-01],\n",
       "        [ 3.49257782e+02,  3.04989929e+02,  2.61749840e+00]]])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poses_2d"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "\n",
    "In the following we define the training function and train a network on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "from RD3D import train\n",
    "from RD3D.model.rd3d import RD3D\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from RD3D.model.rd3d import RD3D\n",
    "from RD3D.data import get_loader\n",
    "from RD3D.utils.func import AvgMeter, clip_gradient\n",
    "from RD3D.utils.lr_scheduler import get_scheduler\n",
    "from RD3D.utils.logger import setup_logger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "def train_salient(train_loader, model, optimizer, criterion, scheduler, epoch, opt):\n",
    "    # multi-scale training  \n",
    "    size_rates = [0.75, 1, 1.25]\n",
    "\n",
    "    model.train()\n",
    "    loss_record = AvgMeter()\n",
    "    for i, pack in enumerate(train_loader, start=1):\n",
    "        for rate in size_rates:\n",
    "            optimizer.zero_grad()\n",
    "            images, gts, depths = pack\n",
    "            images = images.cuda()\n",
    "            gts = gts.cuda()\n",
    "            depths = depths.cuda()\n",
    "\n",
    "            # multi-scale training samples\n",
    "            trainsize = int(round(opt.trainsize * rate / 32) * 32)\n",
    "            if rate != 1:\n",
    "                images = F.upsample(images, size=(trainsize, trainsize), mode='bilinear', align_corners=True)\n",
    "                images = images.unsqueeze(2)\n",
    "                gts = F.upsample(gts, size=(trainsize, trainsize), mode='bilinear', align_corners=True)\n",
    "\n",
    "                depths = F.upsample(depths, size=(trainsize, trainsize), mode='bilinear', align_corners=True)\n",
    "                depths = depths.unsqueeze(2)\n",
    "                images = torch.cat([images, depths], 2)\n",
    "\n",
    "            if rate == 1:\n",
    "                images = images.unsqueeze(2)\n",
    "                depths = depths.unsqueeze(2)\n",
    "                images = torch.cat([images, depths], 2)\n",
    "\n",
    "            # forward\n",
    "            pred_s = model(images)\n",
    "            # TODO Calculate different loss based on the error label\n",
    "            loss = criterion(pred_s, gts)\n",
    "\n",
    "            loss.backward()\n",
    "            clip_gradient(optimizer, opt.clip)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            if rate == 1:\n",
    "                loss_record.update(loss.data, opt.batchsize)\n",
    "\n",
    "        if i % 100 == 0 or i == len(train_loader):\n",
    "            logger.info('Epoch [{:03d}/{:03d}], Step [{:04d}/{:04d}], Loss: {:.4f}'.\n",
    "                        format(epoch, opt.epochs, i, len(train_loader),\n",
    "                               loss_record.show()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "223e9e27cbc670f5c22c33ebc1577b54f1cca48f7f6b973b1c50992e7fee7cdc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
