{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FESDModel\n",
    "\n",
    "FESD - Fault estimation for skeleton detection - is a suite that aims at finding faults in joints of skeletons, which are detected by human pose estimatiors.\n",
    "\n",
    "FESDData is the sister project to this notebook, which aims at recording depth and rgb data, as well as populating the data with human poses from variing human pose estimators.\n",
    "\n",
    "Furthermore, FESTData augments all data based on joint confidence.\n",
    "\n",
    "FFESDModel aims to develop and evaluate a model based on the faulty and augmented joint data as well as RGBD data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries\n",
    "\n",
    "We need a range of libraries which are imported here. We also define some constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import tqdm\n",
    "\n",
    "import cv2\n",
    "import pyrealsense2 as rs\n",
    "\n",
    "sns.set_style()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RECORDING_DIR = Path('H:/Recordings/')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "Firstly we need to import all the recordings into the notebook.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Metadata\n",
    "\n",
    "It is important to load the metadata, such as the session parameters, the exercises and the recording paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recording_jsons = []\n",
    "for file in os.listdir(RECORDING_DIR):\n",
    "  if (file.endswith('.json')):\n",
    "    with open(file=os.path.join(RECORDING_DIR, file), mode='r') as file:\n",
    "      data = json.load(file)\n",
    "      recording_jsons.append(data)\n",
    "\n",
    "len(recording_jsons)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we load the Exercises from the exercise file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(file=\"Exercises.json\", mode='r') as file:\n",
    "  exercises_json = json.load(file)['Exercises']\n",
    "\n",
    "with open(file=\"JointErrors.json\", mode='r') as file:\n",
    "  joint_error_json = json.load(file)\n",
    "\n",
    "with open(file=\"SkeletonErrors.json\", mode='r') as file:\n",
    "  skeleton_error_json = json.load(file)\n",
    "\n",
    "len(exercises_json)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Frame\n",
    "\n",
    "Here we define the load frame functions. For now we focus on nuitrack recordings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id_2_name(i: int):\n",
    "  return 'frame_' + str(i) + '.yml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_skeletons(skeletons_json, flip: bool=False) -> (np.ndarray, np.ndarray, list[tuple[float, float, float]], list[tuple[float, float, float]]):\n",
    "  poses = []\n",
    "  pose_errors = []\n",
    "  bounding_boxes_2d = [(np.inf, np.inf, np.inf), (0, 0, 0)]\n",
    "  bounding_boxes_3d = [(np.inf, np.inf, np.inf), (0, 0, 0)]\n",
    "  for person in skeletons_json:\n",
    "    joints = np.ndarray(shape=[0, 6])\n",
    "    errors = []    \n",
    "    origin = person['Skeleton'][4]\n",
    "\n",
    "    for joint in person['Skeleton']:\n",
    "      if (joint['error'] != 1):\n",
    "        bounding_boxes_2d[0] = np.minimum(bounding_boxes_2d[0], [joint['u'], joint['v'], joint['d']])\n",
    "        bounding_boxes_2d[1] = np.maximum(bounding_boxes_2d[1], [joint['u'], joint['v'], joint['d']])\n",
    "        bounding_boxes_3d[0] = np.minimum(bounding_boxes_3d[0], [joint['x'], joint['y'], joint['z']])\n",
    "        bounding_boxes_3d[1] = np.maximum(bounding_boxes_3d[1], [joint['x'], joint['y'], joint['z']])\n",
    "      joints = np.append(joints, [[\n",
    "        joint['u'] - origin['u'],\n",
    "        joint['v'] - origin['v'],\n",
    "        joint['d'] - origin['d'],\n",
    "        joint['x'] - origin['x'],\n",
    "        joint['y'] - origin['y'],\n",
    "        joint['z'] - origin['z']\n",
    "      ]], axis=0) * (-1 if flip else 1)\n",
    "\n",
    "      errors.append(1 if person['error'] == 1 else joint['error'])\n",
    "              \n",
    "    poses.append(joints)\n",
    "    pose_errors.append(errors)\n",
    "  \n",
    "  print(bounding_boxes_2d)\n",
    "  print(bounding_boxes_3d)\n",
    "\n",
    "  return np.asarray(poses), np.asarray(pose_errors), bounding_boxes_2d, bounding_boxes_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_frame(session: json, frame_id: int, flip: bool=False, crop: bool=False, crop_random: bool=False, crop_pad: int=0) -> (np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray):\n",
    "  frame_path = RECORDING_DIR /  session['Cameras'][0]['FileName'] / id_2_name(frame_id)\n",
    "  frame_file = cv2.FileStorage(str(frame_path), cv2.FileStorage_READ)\n",
    "  frame = np.asarray( frame_file.getNode('frame').mat()[:,:] )\n",
    "  rgb, depth = np.split(frame, [3], axis=2)\n",
    "\n",
    "  with open(file=RECORDING_DIR /  session['Skeleton'], mode='r') as file:\n",
    "    skeleton_json = json.load(file)[frame_id]\n",
    "  poses, errors, bounding_boxes_2d, bounding_boxes_3d = load_skeletons(skeleton_json, flip)\n",
    "  pose_2d, pose_3d = np.split(poses, 2, axis=2)\n",
    "\n",
    "  if (flip):\n",
    "    rgb = np.flip(rgb, axis=1)\n",
    "    depth = np.flip(depth, axis=1)\n",
    "\n",
    "  if (crop):\n",
    "    min_x = max(0, int(np.floor(bounding_boxes_2d[0][0])) - crop_pad)\n",
    "    min_y = max(0, int(np.floor(bounding_boxes_2d[0][1])) - crop_pad)\n",
    "    max_x = min(rgb.shape[0], int(np.ceil(bounding_boxes_2d[1][0])) + crop_pad)\n",
    "    max_y = min(rgb.shape[1], int(np.ceil(bounding_boxes_2d[1][1])) + crop_pad)\n",
    "\n",
    "    if (crop_random):\n",
    "      min_x = np.random.randint(0, min_x)\n",
    "      min_y = np.random.randint(0, min_y)\n",
    "      max_x = np.random.randint(max_x, rgb.shape[0])\n",
    "      max_y = np.random.randint(max_y, rgb.shape[1])\n",
    "    \n",
    "    rgb = rgb[min_x:max_x, min_y:max_y]\n",
    "    depth = depth[min_x:max_x, min_y:max_y] \n",
    "  \n",
    "  return rgb, depth, pose_2d, pose_3d, errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([287.79888916, 147.11325073,   2.45382833]), array([421.3939209 , 476.17895508,   2.87333059])]\n",
      "[array([-0.13871242, -1.12756002,  2.45382833]), array([0.43398166, 0.37994158, 2.87333059])]\n",
      "(215, 542, 3)\n",
      "(215, 542, 1)\n",
      "(1, 25, 3)\n",
      "(1, 25, 3)\n",
      "(1, 25)\n"
     ]
    }
   ],
   "source": [
    "rgb, depth, poses_2d, poses_3d, errors = load_frame(recording_jsons[0], 20, True, True, True, 10)\n",
    "print(rgb.shape)\n",
    "print(depth.shape)\n",
    "print(poses_2d.shape)\n",
    "print(poses_3d.shape)\n",
    "print(errors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 3.49257782e+02,  3.04989929e+02,  2.61749840e+00],\n",
       "        [-1.84570312e-01, -1.57876678e+02, -1.63670063e-01],\n",
       "        [ 4.53582764e-01,  1.26848038e+02,  1.31840229e-01],\n",
       "        [ 2.45849609e-01, -4.82750244e+01, -1.82793140e-02],\n",
       "        [-0.00000000e+00, -0.00000000e+00, -0.00000000e+00],\n",
       "        [-2.71636963e-01, -1.06412277e+02, -1.03270292e-01],\n",
       "        [-4.05320740e+01,  1.04062683e+02,  9.98923779e-02],\n",
       "        [ 5.69803772e+01, -4.21243286e+01, -3.98993492e-03],\n",
       "        [-6.95727234e+01, -1.72396545e+01,  4.21814919e-02],\n",
       "        [ 7.21361389e+01,  2.93243713e+01, -4.98199463e-02],\n",
       "        [ 3.49257782e+02,  3.04989929e+02,  2.61749840e+00],\n",
       "        [-2.71636963e-01, -1.06412277e+02, -1.03270292e-01],\n",
       "        [ 4.05550842e+01,  1.02272476e+02,  8.75868797e-02],\n",
       "        [-5.27978516e+01, -3.88483887e+01,  5.90515137e-03],\n",
       "        [ 5.99971619e+01, -2.09994202e+01,  2.67663002e-02],\n",
       "        [-6.14588928e+01,  3.31505737e+01, -3.33003998e-02],\n",
       "        [ 3.49257782e+02,  3.04989929e+02,  2.61749840e+00],\n",
       "        [ 2.54044189e+01,  9.76605225e+00,  4.63178158e-02],\n",
       "        [-1.69927673e+01, -1.09090118e+02, -1.15725994e-02],\n",
       "        [ 1.01470947e+00,  1.67784637e+02,  2.55832195e-01],\n",
       "        [ 3.49257782e+02,  3.04989929e+02,  2.61749840e+00],\n",
       "        [-2.56338806e+01,  1.04048767e+01,  5.10828495e-02],\n",
       "        [ 1.61321106e+01, -1.09205811e+02, -2.27441788e-02],\n",
       "        [-1.91772461e-01,  1.71189026e+02,  2.46545315e-01],\n",
       "        [ 3.49257782e+02,  3.04989929e+02,  2.61749840e+00]]])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poses_2d"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "\n",
    "In the following we define the training function and train a network on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from RD3D import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "def train_salient(train_loader, model, optimizer, criterion, scheduler, epoch, opt):\n",
    "    # multi-scale training  \n",
    "    size_rates = [0.75, 1, 1.25]\n",
    "\n",
    "    model.train()\n",
    "    loss_record = AvgMeter()\n",
    "    for i, pack in enumerate(train_loader, start=1):\n",
    "        for rate in size_rates:\n",
    "            optimizer.zero_grad()\n",
    "            images, gts, depths = pack\n",
    "            images = images.cuda()\n",
    "            gts = gts.cuda()\n",
    "            depths = depths.cuda()\n",
    "\n",
    "            # multi-scale training samples\n",
    "            trainsize = int(round(opt.trainsize * rate / 32) * 32)\n",
    "            if rate != 1:\n",
    "                images = F.upsample(images, size=(trainsize, trainsize), mode='bilinear', align_corners=True)\n",
    "                images = images.unsqueeze(2)\n",
    "                gts = F.upsample(gts, size=(trainsize, trainsize), mode='bilinear', align_corners=True)\n",
    "\n",
    "                depths = F.upsample(depths, size=(trainsize, trainsize), mode='bilinear', align_corners=True)\n",
    "                depths = depths.unsqueeze(2)\n",
    "                images = torch.cat([images, depths], 2)\n",
    "\n",
    "            if rate == 1:\n",
    "                images = images.unsqueeze(2)\n",
    "                depths = depths.unsqueeze(2)\n",
    "                images = torch.cat([images, depths], 2)\n",
    "\n",
    "            # forward\n",
    "            pred_s = model(images)\n",
    "            # TODO Calculate different loss based on the error label\n",
    "            loss = criterion(pred_s, gts)\n",
    "\n",
    "            loss.backward()\n",
    "            clip_gradient(optimizer, opt.clip)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            if rate == 1:\n",
    "                loss_record.update(loss.data, opt.batchsize)\n",
    "\n",
    "        if i % 100 == 0 or i == len(train_loader):\n",
    "            logger.info('Epoch [{:03d}/{:03d}], Step [{:04d}/{:04d}], Loss: {:.4f}'.\n",
    "                        format(epoch, opt.epochs, i, len(train_loader),\n",
    "                               loss_record.show()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "223e9e27cbc670f5c22c33ebc1577b54f1cca48f7f6b973b1c50992e7fee7cdc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
