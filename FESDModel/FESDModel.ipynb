{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FESDModel\n",
    "\n",
    "FESD - Fault estimation for skeleton detection - is a suite that aims at finding faults in joints of skeletons, which are detected by human pose estimatiors.\n",
    "\n",
    "FESDData is the sister project to this notebook, which aims at recording depth and rgb data, as well as populating the data with human poses from variing human pose estimators.\n",
    "\n",
    "Furthermore, FESTData augments all data based on joint confidence.\n",
    "\n",
    "FFESDModel aims to develop and evaluate a model based on the faulty and augmented joint data as well as RGBD data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries\n",
    "\n",
    "We need a range of libraries which are imported here. We also define some constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "h:\\FESD\\FESDModel\\env\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num cuda GPUs: 1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from time import time\n",
    "\n",
    "from data import FESDDataset\n",
    "from data import Frame, AugmentationParams\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "import cv2\n",
    "\n",
    "from model import FESD, train, val, test\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "\n",
    "from utils import AvgMeter, clip_gradient, get_scheduler\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(f\"Num cuda GPUs: {num_gpus}\")\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files/Graphviz/bin/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RECORDING_DIR = Path('H:/Recordings/')\n",
    "CHECKPOINT_DIR = Path('checkpoints')\n",
    "\n",
    "# Writer will output to ./runs/ directory by default\n",
    "writer = SummaryWriter()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "Firstly we need to import all the recordings into the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(file=\"Exercises.json\", mode='r') as file:\n",
    "  exercises_json = json.load(file)['Exercises']\n",
    "\n",
    "with open(file=\"JointErrors.json\", mode='r') as file:\n",
    "  joint_error_json = json.load(file)\n",
    "\n",
    "with open(file=\"SkeletonErrors.json\", mode='r') as file:\n",
    "  skeleton_error_json = json.load(file)\n",
    "\n",
    "len(exercises_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recordings Found: 21\n",
      "Total Frames: 6300\n",
      "Recordings Found: 4\n",
      "Total Frames: 1200\n",
      "40\n",
      "158\n",
      "1200\n"
     ]
    }
   ],
   "source": [
    "batchsize = 32\n",
    "train_size = 300\n",
    "validation_split = .8\n",
    "\n",
    "dataset = FESDDataset(RECORDING_DIR, train_size, test=False)\n",
    "\n",
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "train_sampler = torch.utils.data.sampler.SubsetRandomSampler(train_indices)\n",
    "valid_sampler = torch.utils.data.sampler.SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batchsize,  sampler=train_sampler)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset, batch_size=batchsize, sampler=valid_sampler)\n",
    "\n",
    "dataset_test = FESDDataset(RECORDING_DIR, train_size, test=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset_test)\n",
    "\n",
    "print(len(train_loader))\n",
    "print(len(validation_loader))\n",
    "print(len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 300, 300]) torch.Size([1, 300, 300]) torch.Size([3, 25]) torch.Size([100])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,\n",
       "         0., 2., 1., 0., 0., 2., 1.]),\n",
       " tensor([0.4754, 0.4754, 0.4754, 0.4754, 0.4754, 0.4754, 0.4754, 0.4754, 0.4754,\n",
       "         0.4754, 0.4754, 0.4754, 0.4754, 0.4754, 0.4754, 0.4754, 0.4754, 0.4754,\n",
       "         0.4754, 0.4754, 0.4754, 0.4754, 0.4754, 0.4754, 0.4754]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.randomize_augmentation_params = False\n",
    "dataset.reset_augmentation_params()\n",
    "rgb, depth, pose_2d, gt = dataset[0]\n",
    "dataset.frame.show()\n",
    "\n",
    "print(rgb.shape, depth.shape, pose_2d.shape, gt.shape)\n",
    "dataset.gt2err(gt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "h:\\FESD\\FESDModel\\env\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "h:\\FESD\\FESDModel\\env\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FESD(\n",
      "  (rgb_conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (rgb_pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (rgb_conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (rgb_pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (rgb_conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (rgb_pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (depth_conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (depth_pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (depth_conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (depth_pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (depth_conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (depth_pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (joint_conv1): Conv1d(3, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (joint_conv2): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (joint_conv3): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (fc1): Linear(in_features=86144, out_features=1024, bias=True)\n",
      "  (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (fc3): Linear(in_features=512, out_features=100, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "resnet = torchvision.models.resnet50(pretrained=True)\n",
    "\n",
    "model = FESD()\n",
    "print(model)\n",
    "\n",
    "model = nn.DataParallel(model).cuda()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "\n",
    "In the following we define the training function and train a network on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch number\n",
    "epochs = 100\n",
    "# optimizer\n",
    "optim = 'adam'\n",
    "# learning rate\n",
    "learning_rate = 0.000025\n",
    "# learning rate scheduler. can be step, poly or cosine\n",
    "lr_scheduler = 'cosine'\n",
    "# warmup epoch\n",
    "warmup_epoch = -1\n",
    "# warmup multiplier\n",
    "warmup_multiplier = 100\n",
    "# for step scheduler. where to decay lr, can be a list\n",
    "lr_decay_epochs = [120, 160, 200]\n",
    "# for step scheduler. step size to decay lr\n",
    "lr_decay_steps = 20 \n",
    "# for step scheduler. decay rate for learning rate\n",
    "lr_decay_rate = 0.01\n",
    "# weight decay\n",
    "weight_decay = 0.0001\n",
    "# momentum for SGD\n",
    "momentum = 0.9\n",
    "# gradient clipping margin\n",
    "clip = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data = len(train_loader.dataset)\n",
    "CE = torch.nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "if optim == 'adam':\n",
    "    optimizer = torch.optim.Adam(model.parameters(), learning_rate, weight_decay=weight_decay)\n",
    "elif optim == 'adamW':\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), learning_rate, weight_decay=weight_decay)\n",
    "elif optim == 'sdg':\n",
    "    optimizer = torch.optim.SGD(model.parameters(), learning_rate / 10.0 * batchsize, momentum=momentum, weight_decay=weight_decay)\n",
    "\n",
    "scheduler = get_scheduler(optimizer, len(train_loader), lr_scheduler, lr_decay_epochs, lr_decay_steps, lr_decay_rate, epochs, warmup_epoch, warmup_multiplier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'encode'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, epochs \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m      6\u001b[0m     tic \u001b[39m=\u001b[39m time()\n\u001b[1;32m----> 7\u001b[0m     train(train_loader, model, optimizer, CE, scheduler, clip, epoch, epochs, writer)\n\u001b[0;32m      8\u001b[0m     val(validation_loader, model, CE, epoch, epochs, writer)\n\u001b[0;32m      9\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mepoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m, total time \u001b[39m\u001b[39m{\u001b[39;00mtime()\u001b[39m \u001b[39m\u001b[39m-\u001b[39m\u001b[39m \u001b[39mtic\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, learning_rate \u001b[39m\u001b[39m{\u001b[39;00moptimizer\u001b[39m.\u001b[39mparam_groups[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mh:\\FESD\\FESDModel\\model\\train.py:42\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(train_loader, model, optimizer, criterion, scheduler, clip, epoch, epochs, writer)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mor\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(train_loader):\n\u001b[0;32m     41\u001b[0m   writer\u001b[39m.\u001b[39madd_graph(model, (rgbs, depths, poses))\n\u001b[1;32m---> 42\u001b[0m   writer\u001b[39m.\u001b[39;49madd_text(\u001b[39m\"\u001b[39;49m\u001b[39m/Pred\u001b[39;49m\u001b[39m\"\u001b[39;49m, pred,i)\n\u001b[0;32m     43\u001b[0m   writer\u001b[39m.\u001b[39madd_text(\u001b[39m\"\u001b[39m\u001b[39m/GT\u001b[39m\u001b[39m\"\u001b[39m, gt, i)\n\u001b[0;32m     45\u001b[0m   \u001b[39mprint\u001b[39m(pred)\n",
      "File \u001b[1;32mh:\\FESD\\FESDModel\\env\\lib\\site-packages\\torch\\utils\\tensorboard\\writer.py:816\u001b[0m, in \u001b[0;36mSummaryWriter.add_text\u001b[1;34m(self, tag, text_string, global_step, walltime)\u001b[0m\n\u001b[0;32m    801\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Add text data to summary.\u001b[39;00m\n\u001b[0;32m    802\u001b[0m \n\u001b[0;32m    803\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    812\u001b[0m \u001b[39m    writer.add_text('rnn', 'This is an rnn', 10)\u001b[39;00m\n\u001b[0;32m    813\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    814\u001b[0m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_log_api_usage_once(\u001b[39m\"\u001b[39m\u001b[39mtensorboard.logging.add_text\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    815\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_file_writer()\u001b[39m.\u001b[39madd_summary(\n\u001b[1;32m--> 816\u001b[0m     text(tag, text_string), global_step, walltime\n\u001b[0;32m    817\u001b[0m )\n",
      "File \u001b[1;32mh:\\FESD\\FESDModel\\env\\lib\\site-packages\\torch\\utils\\tensorboard\\summary.py:638\u001b[0m, in \u001b[0;36mtext\u001b[1;34m(tag, text)\u001b[0m\n\u001b[0;32m    632\u001b[0m plugin_data \u001b[39m=\u001b[39m SummaryMetadata\u001b[39m.\u001b[39mPluginData(\n\u001b[0;32m    633\u001b[0m     plugin_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m, content\u001b[39m=\u001b[39mTextPluginData(version\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mSerializeToString()\n\u001b[0;32m    634\u001b[0m )\n\u001b[0;32m    635\u001b[0m smd \u001b[39m=\u001b[39m SummaryMetadata(plugin_data\u001b[39m=\u001b[39mplugin_data)\n\u001b[0;32m    636\u001b[0m tensor \u001b[39m=\u001b[39m TensorProto(\n\u001b[0;32m    637\u001b[0m     dtype\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDT_STRING\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m--> 638\u001b[0m     string_val\u001b[39m=\u001b[39m[text\u001b[39m.\u001b[39;49mencode(encoding\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mutf_8\u001b[39m\u001b[39m\"\u001b[39m)],\n\u001b[0;32m    639\u001b[0m     tensor_shape\u001b[39m=\u001b[39mTensorShapeProto(dim\u001b[39m=\u001b[39m[TensorShapeProto\u001b[39m.\u001b[39mDim(size\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)]),\n\u001b[0;32m    640\u001b[0m )\n\u001b[0;32m    641\u001b[0m \u001b[39mreturn\u001b[39;00m Summary(\n\u001b[0;32m    642\u001b[0m     value\u001b[39m=\u001b[39m[Summary\u001b[39m.\u001b[39mValue(tag\u001b[39m=\u001b[39mtag \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/text_summary\u001b[39m\u001b[39m\"\u001b[39m, metadata\u001b[39m=\u001b[39msmd, tensor\u001b[39m=\u001b[39mtensor)]\n\u001b[0;32m    643\u001b[0m )\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'encode'"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "dataset.randomize_augmentation_params = False\n",
    "dataset.reset_augmentation_params()\n",
    "# routine\n",
    "for epoch in range(1, epochs + 1):\n",
    "    tic = time()\n",
    "    train(train_loader, model, optimizer, CE, scheduler, clip, epoch, epochs, writer)\n",
    "    val(validation_loader, model, CE, epoch, epochs, writer)\n",
    "    print(f'epoch {epoch}, total time {time() - tic:.2f}, learning_rate {optimizer.param_groups[0][\"lr\"]}')\n",
    "\n",
    "    if (epoch) % 10 == 0:\n",
    "        torch.save(model.state_dict(), os.path.join(CHECKPOINT_DIR, f\"{epoch}_ckpt.pth\"))\n",
    "\n",
    "        print(\"checkpoint saved {}!\".format(os.path.join(CHECKPOINT_DIR, f\"{epoch}_ckpt.pth\")))\n",
    "        \n",
    "torch.save(model.state_dict(), os.path.join(CHECKPOINT_DIR, f\"last_ckpt.pth\"))\n",
    "print(f\"model saved {os.path.join(CHECKPOINT_DIR, f'last_ckpt.pth')}!\")\n",
    "checkpoint = os.path.join(CHECKPOINT_DIR, f\"last_ckpt.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.1502, -8.1108, -8.4876, -7.5053, -8.1098, -8.7455, -8.4399, -8.8540,\n",
       "         -8.3526, -2.0714,  2.2664, -9.5404, -9.1294, -8.4943, -8.1654, -8.7219,\n",
       "          2.3526, -7.6469, -6.7728,  2.6668,  2.2479, -7.7610, -7.5209,  2.8238,\n",
       "          2.2679]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rgb, depth, pose_2d, errors = dataset[0]\n",
    "model(rgb, depth, pose_2d)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "223e9e27cbc670f5c22c33ebc1577b54f1cca48f7f6b973b1c50992e7fee7cdc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
