{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FESDModel\n",
    "\n",
    "FESD - Fault estimation for skeleton detection - is a suite that aims at finding faults in joints of skeletons, which are detected by human pose estimatiors.\n",
    "\n",
    "FESDData is the sister project to this notebook, which aims at recording depth and rgb data, as well as populating the data with human poses from variing human pose estimators.\n",
    "\n",
    "Furthermore, FESTData augments all data based on joint confidence.\n",
    "\n",
    "FFESDModel aims to develop and evaluate a model based on the faulty and augmented joint data as well as RGBD data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries\n",
    "\n",
    "We need a range of libraries which are imported here. We also define some constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import tqdm\n",
    "\n",
    "from dataset import FESDDataset\n",
    "from utils import Frame, AugmentationParams\n",
    "\n",
    "import cv2\n",
    "import pyrealsense2 as rs\n",
    "\n",
    "sns.set_style()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "RECORDING_DIR = Path('H:/Recordings/')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "Firstly we need to import all the recordings into the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(file=\"Exercises.json\", mode='r') as file:\n",
    "  exercises_json = json.load(file)['Exercises']\n",
    "\n",
    "with open(file=\"JointErrors.json\", mode='r') as file:\n",
    "  joint_error_json = json.load(file)\n",
    "\n",
    "with open(file=\"SkeletonErrors.json\", mode='r') as file:\n",
    "  skeleton_error_json = json.load(file)\n",
    "\n",
    "len(exercises_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recordings Found: 25\n",
      "Total Frames: 7500\n",
      "H:\\Recordings\\Session_2023-02-28T22.58.23\\Frames\\frame_0.yml\n",
      "H:\\Recordings\\Session_2023-02-28T22.58.23\\Frames\\frame_0.yml\n",
      "All Missing Joints\n",
      "All Wrong Joints\n",
      "[[ -1.42166138 167.15560913   0.2603097 ]\n",
      " [ -1.1519165  170.9956665    0.25113821]]\n"
     ]
    }
   ],
   "source": [
    "dataset = FESDDataset(RECORDING_DIR, 500)\n",
    "\n",
    "dataset.augmentation_params = AugmentationParams(flip=False, crop=False, crop_random=False, crop_pad=0, gaussian=False)\n",
    "sample_frame = dataset[0]\n",
    "sample_frame.show()\n",
    "\n",
    "dataset.augmentation_params = AugmentationParams(flip=False, crop=True, crop_random=False, crop_pad=0, gaussian=False)\n",
    "sample_frame = dataset[0]\n",
    "sample_frame.show()\n",
    "\n",
    "print(\"All Missing Joints\")\n",
    "np.where(np.any(sample_frame.errors==1))\n",
    "print(\"All Wrong Joints\")\n",
    "print(sample_frame.poses_2d[sample_frame.errors==2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0], dtype=int64),)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(np.any(sample_frame.errors==1))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "\n",
    "In the following we define the training function and train a network on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'RD3D.model.rd3d'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mRD3D\u001b[39;00m \u001b[39mimport\u001b[39;00m train\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mRD3D\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrd3d\u001b[39;00m \u001b[39mimport\u001b[39;00m RD3D\n\u001b[0;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n",
      "File \u001b[1;32mh:\\FESD\\FESDModel\\RD3D\\train.py:12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctional\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mF\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mRD3D\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrd3d\u001b[39;00m \u001b[39mimport\u001b[39;00m RD3D\n\u001b[0;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mRD3D\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrd3d_plus\u001b[39;00m \u001b[39mimport\u001b[39;00m RD3D_plus\n\u001b[0;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mRD3D\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m get_loader\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'RD3D.model.rd3d'"
     ]
    }
   ],
   "source": [
    "from RD3D import train\n",
    "from RD3D.model.rd3d import RD3D\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from RD3D.model.rd3d import RD3D\n",
    "from RD3D.data import get_loader\n",
    "from RD3D.utils.func import AvgMeter, clip_gradient\n",
    "from RD3D.utils.lr_scheduler import get_scheduler\n",
    "from RD3D.utils.logger import setup_logger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "def train_salient(train_loader, model, optimizer, criterion, scheduler, epoch, opt):\n",
    "    # multi-scale training  \n",
    "    size_rates = [0.75, 1, 1.25]\n",
    "\n",
    "    model.train()\n",
    "    loss_record = AvgMeter()\n",
    "    for i, pack in enumerate(train_loader, start=1):\n",
    "        for rate in size_rates:\n",
    "            optimizer.zero_grad()\n",
    "            images, gts, depths = pack\n",
    "            images = images.cuda()\n",
    "            gts = gts.cuda()\n",
    "            depths = depths.cuda()\n",
    "\n",
    "            # multi-scale training samples\n",
    "            trainsize = int(round(opt.trainsize * rate / 32) * 32)\n",
    "            if rate != 1:\n",
    "                images = F.upsample(images, size=(trainsize, trainsize), mode='bilinear', align_corners=True)\n",
    "                images = images.unsqueeze(2)\n",
    "                gts = F.upsample(gts, size=(trainsize, trainsize), mode='bilinear', align_corners=True)\n",
    "\n",
    "                depths = F.upsample(depths, size=(trainsize, trainsize), mode='bilinear', align_corners=True)\n",
    "                depths = depths.unsqueeze(2)\n",
    "                images = torch.cat([images, depths], 2)\n",
    "\n",
    "            if rate == 1:\n",
    "                images = images.unsqueeze(2)\n",
    "                depths = depths.unsqueeze(2)\n",
    "                images = torch.cat([images, depths], 2)\n",
    "\n",
    "            # forward\n",
    "            pred_s = model(images)\n",
    "            # TODO Calculate different loss based on the error label\n",
    "            loss = criterion(pred_s, gts)\n",
    "\n",
    "            loss.backward()\n",
    "            clip_gradient(optimizer, opt.clip)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            if rate == 1:\n",
    "                loss_record.update(loss.data, opt.batchsize)\n",
    "\n",
    "        if i % 100 == 0 or i == len(train_loader):\n",
    "            logger.info('Epoch [{:03d}/{:03d}], Step [{:04d}/{:04d}], Loss: {:.4f}'.\n",
    "                        format(epoch, opt.epochs, i, len(train_loader),\n",
    "                               loss_record.show()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "223e9e27cbc670f5c22c33ebc1577b54f1cca48f7f6b973b1c50992e7fee7cdc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
