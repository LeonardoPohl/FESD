{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FESDModel\n",
    "\n",
    "FESD - Fault estimation for skeleton detection - is a suite that aims at finding faults in joints of skeletons, which are detected by human pose estimatiors.\n",
    "\n",
    "FESDData is the sister project to this notebook, which aims at recording depth and rgb data, as well as populating the data with human poses from variing human pose estimators.\n",
    "\n",
    "Furthermore, FESTData augments all data based on joint confidence.\n",
    "\n",
    "FFESDModel aims to develop and evaluate a model based on the faulty and augmented joint data as well as RGBD data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries\n",
    "\n",
    "We need a range of libraries which are imported here. We also define some constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num cuda GPUs: 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from time import time\n",
    "\n",
    "from data import FESDDataset\n",
    "from data import Frame, AugmentationParams\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "import cv2\n",
    "\n",
    "from model import FESD, train, val, test\n",
    "import copy\n",
    "\n",
    "import scipy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import datetime\n",
    "\n",
    "from utils import AvgMeter, clip_gradient, get_scheduler\n",
    "from utils.mode import Mode\n",
    "from utils import err2gt, gt2err\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "num_gpus = torch.cuda.device_count()\n",
    "is_cuda = torch.cuda.is_available()\n",
    "print(f\"Num cuda GPUs: {num_gpus}\")\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files/Graphviz/bin/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "RECORDING_DIR = Path('D:/Recordings/')\n",
    "CHECKPOINT_DIR = Path('checkpoints')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "Firstly we need to import all the recordings into the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(file=\"Exercises.json\", mode='r') as file:\n",
    "  exercises_json = json.load(file)['Exercises']\n",
    "\n",
    "with open(file=\"JointErrors.json\", mode='r') as file:\n",
    "  joint_error_json = json.load(file)\n",
    "\n",
    "with open(file=\"SkeletonErrors.json\", mode='r') as file:\n",
    "  skeleton_error_json = json.load(file)\n",
    "\n",
    "len(exercises_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recordings Found: 17\n",
      "Total Frames: 510\n",
      "Recordings Found: 9\n",
      "Total Frames: 270\n",
      "51\n",
      "270\n"
     ]
    }
   ],
   "source": [
    "batchsize = 10\n",
    "train_size = 300\n",
    "\n",
    "dataset_train = FESDDataset(RECORDING_DIR, train_size)\n",
    "dataset_train.randomize_augmentation_params = True\n",
    "\n",
    "dataset_test = FESDDataset(RECORDING_DIR, train_size, test=True)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset_train, batch_size=batchsize)\n",
    "test_loader = torch.utils.data.DataLoader(dataset_test)\n",
    "\n",
    "print(len(train_loader))\n",
    "print(len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_names_all = [\"-\", \"Head\", \"Neck\", \"Torso\", \"Waist\", \"Left collar\", \"Left shoulder\", \"Left elbow\", \"Left wrist\", \"Left hand\", \"-\", \"Right collar\", \"Right shoulder\", \"Right elbow\", \"Right wrist\", \"Right hand\", \"-\", \"Left hip\", \"Left knee\", \"Left ankle\", \"-\", \"Right hip\", \"Right knee\", \"Right ankle\", \"-\"]\n",
    "joint_names = [i for i in joint_names_all if i != '-']\n",
    "\n",
    "body_halves = np.array([\"Upper Half\", \"Lower Half\"])\n",
    "limbs = np.array([\"Head\", \"Torso\", \"Left arm\", \"Right arm\", \"Left leg\", \"Right leg\"])\n",
    "\n",
    "upper_body_i = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13]\n",
    "lower_body_i = [14, 15, 16, 17, 18, 19]\n",
    "\n",
    "torso_i = [2, 3, 4]\n",
    "head_i = [0, 1]\n",
    "left_arm_i = [5, 6, 7, 8]\n",
    "right_arm_i = [10, 11, 12, 13]\n",
    "left_leg_i = [14, 15, 16]\n",
    "right_leg_i = [17, 18, 19]\n",
    "\n",
    "joint_errors = []\n",
    "for je in joint_error_json:\n",
    "  joint_errors.append(je[\"Name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1.])\n",
      "(tensor([1.]), tensor([0.7311]))\n"
     ]
    }
   ],
   "source": [
    "i = 500\n",
    "mode = Mode.FULL_BODY\n",
    "dataset_train.mode = mode\n",
    "rgb, depth, pose_2d, gt, session = dataset_train[i]\n",
    "dataset_train.frame.show()\n",
    "print(gt)\n",
    "print(gt2err(gt, mode))\n",
    "if mode == Mode.HALF_BODY:\n",
    "  print(body_halves[np.nonzero(gt2err(gt, mode)[0].tolist())])\n",
    "elif mode == Mode.LIMBS:\n",
    "  print(limbs[np.nonzero(gt2err(gt, mode)[0].tolist())])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis\n",
    "\n",
    "Now that the data is loaded we can analyse it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['Type', 'Session', 'Difficulty', 'Exercise', 'Frame', 'Joint', 'Error', 'mode']\n",
    "df_data = pd.DataFrame(columns=cols)\n",
    "\n",
    "for i in tqdm(range(len(dataset_train))):\n",
    "  for m in [Mode.FULL_BODY, Mode.HALF_BODY, Mode.LIMBS, Mode.JOINTS]:\n",
    "    dataset_train.mode = m\n",
    "    rgb, depth, pose_2d, gt, session = dataset_train[i]\n",
    "    \n",
    "    gt = gt2err(gt, m)[0]\n",
    "    session, frame_i = dataset_train.get_index(i)\n",
    "    frame = dataset_train.frame\n",
    "    session_name = frame.session[\"Name\"]\n",
    "    exercise = frame.session['Session Parameters']['Exercise']\n",
    "    difficulty = int(exercise[2])\n",
    "\n",
    "    for j in range(len(gt)):\n",
    "      row = [\"Train\", session_name, difficulty, exercise, frame_i, j, gt.tolist()[j], m]\n",
    "      df_data.loc[len(df_data)] = row\n",
    "\n",
    "for i in tqdm(range(len(dataset_test))):\n",
    "  for m in [Mode.FULL_BODY, Mode.HALF_BODY, Mode.LIMBS, Mode.JOINTS]:\n",
    "    dataset_test.mode = m\n",
    "    _, _, _, gt, _ = dataset_test[i]\n",
    "    gt = gt2err(gt, m)[0]\n",
    "    session, frame_i = dataset_test.get_index(i)\n",
    "    frame = dataset_test.frame\n",
    "    session_name = frame.session[\"Name\"]\n",
    "    exercise = frame.session['Session Parameters']['Exercise']\n",
    "    difficulty = int(exercise[2])\n",
    "\n",
    "    for j in range(len(gt)):\n",
    "      row = [\"Test\", session_name, difficulty, exercise, frame_i, j, gt.tolist()[j], m]\n",
    "      df_data.loc[len(df_data)] = row\n",
    "  \n",
    "dataset_train.mode = mode\n",
    "dataset_test.mode = mode\n",
    "df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_joints = df_data[df_data['mode'] == Mode.JOINTS]\n",
    "df_data_limbs = df_data[df_data['mode'] == Mode.LIMBS]\n",
    "df_data_half_body = df_data[df_data['mode'] == Mode.HALF_BODY]\n",
    "df_data_full_body = df_data[df_data['mode'] == Mode.FULL_BODY]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Distribution Per Joint\n",
    "\n",
    "Here we investigate the distribution of errors based on Body regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_joints[\"pose_id\"] = df_data_joints[\"Session\"] + \"_\" + df_data_joints[\"Frame\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_joints[\"Upper Body\"] = df_data_joints[\"Joint\"].apply(lambda x: x in upper_body_i)\n",
    "df_data_joints[\"Lower Body\"] = df_data_joints[\"Joint\"].apply(lambda x: x in lower_body_i)\n",
    "\n",
    "df_data_joints[\"Torso\"] = df_data_joints[\"Joint\"].apply(lambda x: x in torso_i)\n",
    "df_data_joints[\"Head\"] = df_data_joints[\"Joint\"].apply(lambda x: x in head_i)\n",
    "df_data_joints[\"Left Arm\"] = df_data_joints[\"Joint\"].apply(lambda x: x in left_arm_i)\n",
    "df_data_joints[\"Right Arm\"] = df_data_joints[\"Joint\"].apply(lambda x: x in right_arm_i)\n",
    "df_data_joints[\"Left Leg\"] = df_data_joints[\"Joint\"].apply(lambda x: x in left_leg_i)\n",
    "df_data_joints[\"Right Leg\"] = df_data_joints[\"Joint\"].apply(lambda x: x in right_leg_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_joints[\"body_half\"] = df_data_joints[\"Joint\"].apply(lambda x: \"Upper Body\" if x in [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13] else \"Lower Body\")\n",
    "df_data_joints[\"body_part\"] = df_data_joints[\"Joint\"].apply(lambda x: \"Left Arm\" if x in [5, 6, 7, 8] else \"Right Arm\" if x in [10, 11, 12, 13] else \"Left Leg\" if x in [14, 15, 16] else \"Right Leg\" if x in [27, 28, 19] else \"Torso\" if x in [2, 3, 4, 9] else \"Head\" if x in [0, 1] else \"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_joints[\"error_simple\"] = df_data_joints[\"Error\"].apply(lambda x: 1 if x > 0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (6,4))\n",
    "error_distr = df_data_joints.groupby(\"pose_id\")[\"error_simple\"].sum().reset_index()[\"error_simple\"]\n",
    "error_distr.plot.hist(bins = 20, density=True, alpha = 0.7)\n",
    "error_distr.plot(kind = \"kde\", alpha = 0.7)\n",
    "quant = error_distr.quantile(0.8)\n",
    "ax.axvline(quant, color=\"red\", alpha = .7, ymax = 1, linestyle = \":\")\n",
    "ax.set_title(\"Distribution of number of joints with error per pose\")\n",
    "ax.set_xlabel(\"Number of Joints with Error\")\n",
    "ax.set_ylabel(\"Number of poses\")\n",
    "ax.set_xlim(0, 20)\n",
    "ax.set_xticks(range(0, 21, 5))\n",
    "ax.set_yticks([])\n",
    "plt.savefig(\"figures/distribution_of_joint_errors_per_pose.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize = (6,8), nrows=2, ncols=1)\n",
    "\n",
    "err_dist_upper = df_data_joints[df_data_joints[\"body_half\"] == \"Upper Body\"].groupby(\"pose_id\")[\"error_simple\"].sum().reset_index()[\"error_simple\"]\n",
    "err_dist_lower = df_data_joints[df_data_joints[\"body_half\"] == \"Lower Body\"].groupby(\"pose_id\")[\"error_simple\"].sum().reset_index()[\"error_simple\"]\n",
    "\n",
    "err_dist_upper.plot.hist(bins = range(16), density=True, ax = axs[0])\n",
    "err_dist_upper.plot(kind = \"kde\", ax = axs[0])\n",
    "\n",
    "err_dist_lower.plot.hist(bins = range(11), density=True, ax = axs[1])\n",
    "err_dist_lower.plot(kind = \"kde\", ax = axs[1])\n",
    "\n",
    "fig.suptitle(\"Distribution of number of joints with error per pose per body half\")\n",
    "axs[0].set_title(\"Upper Body\")\n",
    "axs[1].set_title(\"Lower Body\")\n",
    "\n",
    "axs[0].set_xlim(0, 15)\n",
    "axs[0].set_xticks(range(0, 16, 5))\n",
    "axs[1].set_xlim(0, 10)\n",
    "axs[1].set_xticks(range(0, 11, 5))\n",
    "\n",
    "quant = err_dist_upper.quantile(0.8)\n",
    "axs[0].axvline(quant, color=\"red\", alpha = .7, ymax = 1, linestyle = \":\")\n",
    "\n",
    "quant = err_dist_lower.quantile(0.8)\n",
    "axs[1].axvline(quant, color=\"red\", alpha = .7, ymax = 1, linestyle = \":\")\n",
    "\n",
    "axs[1].set_xlabel(\"Number of Joints with Error\")\n",
    "for ax in axs:\n",
    "  ax.set_ylabel(\"Number of poses\")\n",
    "  ax.set_yticks([])\n",
    "\n",
    "plt.savefig(\"figures/distribution_of_joint_errors_per_pose_per_body_half.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize = (6,8), nrows=3, ncols=2)\n",
    "\n",
    "err_dist_torso = df_data_joints[df_data_joints[\"body_part\"] == \"Torso\"].groupby(\"pose_id\")[\"error_simple\"].sum().reset_index()[\"error_simple\"]\n",
    "err_dist_head = df_data_joints[df_data_joints[\"body_part\"] == \"Head\"].groupby(\"pose_id\")[\"error_simple\"].sum().reset_index()[\"error_simple\"]\n",
    "err_dist_left_arm = df_data_joints[df_data_joints[\"body_part\"] == \"Left Arm\"].groupby(\"pose_id\")[\"error_simple\"].sum().reset_index()[\"error_simple\"]\n",
    "err_dist_right_arm = df_data_joints[df_data_joints[\"body_part\"] == \"Right Arm\"].groupby(\"pose_id\")[\"error_simple\"].sum().reset_index()[\"error_simple\"]\n",
    "err_dist_left_leg = df_data_joints[df_data_joints[\"body_part\"] == \"Left Leg\"].groupby(\"pose_id\")[\"error_simple\"].sum().reset_index()[\"error_simple\"]\n",
    "err_dist_right_leg = df_data_joints[df_data_joints[\"body_part\"] == \"Right Leg\"].groupby(\"pose_id\")[\"error_simple\"].sum().reset_index()[\"error_simple\"]\n",
    "\n",
    "err_dist_torso.plot.hist(bins = range(6), density=True, ax = axs[0][0])\n",
    "err_dist_torso.plot(kind = \"kde\", ax = axs[0][0])\n",
    "\n",
    "err_dist_head.plot.hist(bins = range(6), density=True, ax = axs[0][1])\n",
    "err_dist_head.plot(kind = \"kde\", ax = axs[0][1])\n",
    "\n",
    "err_dist_left_arm.plot.hist(bins = range(6), density=True, ax = axs[1][0])\n",
    "err_dist_left_arm.plot(kind = \"kde\", ax = axs[1][0])\n",
    "\n",
    "err_dist_right_arm.plot.hist(bins = range(6), density=True, ax = axs[1][1])\n",
    "err_dist_right_arm.plot(kind = \"kde\", ax = axs[1][1])\n",
    "\n",
    "err_dist_left_leg.plot.hist(bins = range(6), density=True, ax = axs[2][0])\n",
    "err_dist_left_leg.plot(kind = \"kde\", ax = axs[2][0])\n",
    "\n",
    "err_dist_right_leg.plot.hist(bins = range(6), density=True, ax = axs[2][1])\n",
    "err_dist_right_leg.plot(kind = \"kde\", ax = axs[2][1])\n",
    "\n",
    "fig.suptitle(\"Distribution of number of joints with error per pose per body part\")\n",
    "\n",
    "axs[0][0].set_title(\"Torso\")\n",
    "axs[0][1].set_title(\"Head\")\n",
    "axs[1][0].set_title(\"Left Arm\")\n",
    "axs[1][1].set_title(\"Right Arm\")\n",
    "axs[2][0].set_title(\"Left Leg\")\n",
    "axs[2][1].set_title(\"Right Leg\")\n",
    "\n",
    "axs[2][0].set_xlabel(\"Number of Joints with Error\")\n",
    "axs[2][1].set_xlabel(\"Number of Joints with Error\")\n",
    "\n",
    "quant = err_dist_torso.quantile(0.8)\n",
    "axs[0][0].axvline(quant, color=\"red\", alpha = .7, ymax = 1, linestyle = \":\")\n",
    "\n",
    "quant = err_dist_head.quantile(0.8)\n",
    "axs[0][1].axvline(quant, color=\"red\", alpha = .7, ymax = 1, linestyle = \":\")\n",
    "\n",
    "quant = err_dist_left_arm.quantile(0.8)\n",
    "axs[1][0].axvline(quant, color=\"red\", alpha = .7, ymax = 1, linestyle = \":\")\n",
    "\n",
    "quant = err_dist_right_arm.quantile(0.8)\n",
    "axs[1][1].axvline(quant, color=\"red\", alpha = .7, ymax = 1, linestyle = \":\")\n",
    "\n",
    "quant = err_dist_left_leg.quantile(0.8)\n",
    "axs[2][0].axvline(quant, color=\"red\", alpha = .7, ymax = 1, linestyle = \":\")\n",
    "\n",
    "quant = err_dist_right_leg.quantile(0.8)\n",
    "axs[2][1].axvline(quant, color=\"red\", alpha = .7, ymax = 1, linestyle = \":\")\n",
    "\n",
    "for ax in axs:\n",
    "  ax[0].set_ylabel(\"Number of poses\")\n",
    "  ax[1].set_ylabel(\"\")\n",
    "  for a in ax:\n",
    "    a.set_xlim(0, 5)\n",
    "    a.set_xticks(range(0, 6, 1))\n",
    "    a.set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figures/distribution_of_joint_errors_per_pose_per_body_part.png\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Distribution\n",
    "\n",
    "Here we investigate the distribution of errors over the dataset for different body regions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Joints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_joints[\"Joint Name\"] = df_data_joints[\"Joint\"].apply(lambda x: joint_names[x])\n",
    "df_data_joints[\"Simple Error\"] = df_data_joints[\"Error\"] != 0\n",
    "df_data_joints[\"Difficulty Name\"] = df_data_joints[\"Difficulty\"].apply(lambda x: \"Trivial\" if x == 0 else \"Easy\" if x == 1 else \"Medium\" if x == 2 else \"Hard\")\n",
    "df_data_joints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df_data_joints[[\"Error\", \"Frame\"]].groupby(\"Error\").count().plot.pie(subplots=True, figsize=(4, 4), title=\"Error Distribution\", labels=joint_errors, autopct='%1.1f%%', pctdistance=1.25, fontsize=12, explode=(.1,.1,.1,.1), labeldistance=None)\n",
    "\n",
    "ax[0].set_ylabel(\"\")\n",
    "ax[0].legend(bbox_to_anchor=(1, 1.02), loc='center left')\n",
    "\n",
    "plt.savefig(\"figures/dist_joints/Error_Distribution.png\")\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_by_difficulty = df_data_joints[['Difficulty', \"Difficulty Name\", \"Simple Error\"]].groupby([\"Difficulty Name\"]).mean().sort_values(by=\"Difficulty\")[\"Simple Error\"]\n",
    "ax = error_by_difficulty.plot.bar()\n",
    "ax.set_ylabel(\"Error Rate\")\n",
    "ax.set_xlabel(\"Difficulty\")\n",
    "ax.set_title(\"Error Rate by Difficulty\")\n",
    "ax.set_ylim(0, 1)\n",
    "labels = ['Trivial', 'Easy', 'Medium', 'Hard']\n",
    "ax.set_xticks([0, 1, 2, 3])\n",
    "ax.set_xticklabels(labels, rotation=0, ha='center')\n",
    "plt.savefig(\"figures/dist_joints/Error_Rate_by_Difficulty.png\")\n",
    "ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_group = df_data_joints[['Difficulty', \"Difficulty Name\", \"Error\"]].groupby([\"Difficulty Name\", \"Error\"]).count()[\"Difficulty\"].unstack().T\n",
    "error_distribution_by_difficulty = (err_group / err_group.sum()).T\n",
    "error_distribution_by_difficulty['sort_key'] = pd.CategoricalIndex(error_distribution_by_difficulty, [3, 0, 2, 1])\n",
    "error_distribution_by_difficulty.sort_values('sort_key', inplace=True)\n",
    "error_distribution_by_difficulty.drop('sort_key', axis=1, inplace=True)\n",
    "\n",
    "axs = error_distribution_by_difficulty.T.plot.pie(subplots=True, figsize=(10, 10), layout=(2, 2), legend=False, title=\"Error Distribution by Difficulty\", labels=joint_errors, autopct='%1.1f%%', pctdistance=1.25, fontsize=12, explode=(.1,.1,.1,.1), sharex=False, sharey=False, labeldistance=None)\n",
    "\n",
    "axs[0, 0].set_xlabel(\"Trivial\")\n",
    "axs[0, 0].set_ylabel(\"\")\n",
    "axs[0, 1].set_xlabel(\"Easy\")\n",
    "axs[0, 1].set_ylabel(\"\")\n",
    "axs[0, 1].legend(bbox_to_anchor=(1, 1.02), loc='center left')\n",
    "axs[1, 0].set_xlabel(\"Medium\")\n",
    "axs[1, 0].set_ylabel(\"\")\n",
    "axs[1, 1].set_xlabel(\"Hard\")\n",
    "axs[1, 1].set_ylabel(\"\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figures/dist_joints/Error_Distribution_by_Difficulty.png\")\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = df_data_joints[[\"Joint\", \"Simple Error\"]].groupby([\"Joint\"]).sum()\n",
    "s[\"Sorted Names\"] = joint_names\n",
    "s = s.sort_values(by=\"Simple Error\", ascending=False)\n",
    "\n",
    "err_joints = df_data_joints[[\"Joint\", \"Joint Name\", \"Error\"]]\n",
    "error_distribution_by_joint = err_joints.groupby([\"Joint\", \"Error\"]).count().unstack().reindex(s.index).T\n",
    "\n",
    "error_distribution_by_joint = (error_distribution_by_joint / error_distribution_by_joint.sum()).T.fillna(0)\n",
    "error_distribution_by_joint.columns = error_distribution_by_joint.columns.droplevel()\n",
    "\n",
    "error_distribution_by_joint.index = s[\"Sorted Names\"].tolist()\n",
    "\n",
    "edj = error_distribution_by_joint.T\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax1 = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "axs = edj.plot.pie(ax=ax1, subplots=True, figsize=(10, 10), layout=(4, 5), legend=False, title=\"Error Distribution by Joint\", fontsize=12, sharex=False, sharey=False, labeldistance=None, explode=(.05,.05,.05,.1))\n",
    "\n",
    "for ax in axs.flatten():\n",
    "  ax.set_xlabel(ax.get_ylabel())\n",
    "  ax.set_ylabel(\"\")\n",
    "\n",
    "plt.legend(joint_errors, loc='upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "ax1.set_title(\"Error Distribution by Joint\")\n",
    "plt.savefig(\"figures/dist_joints/Error_Distribution_by_Joint.png\")\n",
    "plt.plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full Body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_full_body[\"Joint Name\"] = df_data_full_body[\"Joint\"].apply(lambda x: joint_names[x])\n",
    "df_data_full_body[\"Difficulty Name\"] = df_data_full_body[\"Difficulty\"].apply(lambda x: \"Trivial\" if x == 0 else \"Easy\" if x == 1 else \"Medium\" if x == 2 else \"Hard\")\n",
    "df_data_full_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_by_difficulty = df_data_full_body[['Difficulty', \"Difficulty Name\", \"Error\"]].groupby([\"Difficulty Name\"]).mean().sort_values(by=\"Difficulty\")[\"Error\"]\n",
    "ax = error_by_difficulty.plot.bar()\n",
    "ax.set_ylabel(\"Error Rate\")\n",
    "ax.set_xlabel(\"Difficulty\")\n",
    "ax.set_title(\"Error Rate by Difficulty\")\n",
    "ax.set_ylim(0, 1)\n",
    "labels = ['Trivial', 'Easy', 'Medium', 'Hard']\n",
    "ax.set_xticks([0, 1, 2, 3])\n",
    "ax.set_xticklabels(labels, rotation=0, ha='center')\n",
    "plt.savefig(\"figures/dist_full_body/Error_Rate_by_Difficulty.png\")\n",
    "ax"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Body Halves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_half_body[\"Joint Name\"] = df_data_half_body[\"Joint\"].apply(lambda x: body_halves[x])\n",
    "df_data_half_body[\"Difficulty Name\"] = df_data_half_body[\"Difficulty\"].apply(lambda x: \"Trivial\" if x == 0 else \"Easy\" if x == 1 else \"Medium\" if x == 2 else \"Hard\")\n",
    "#df_data_half_body[\"Error\"] = df_data_half_body[[\"Joint Name\", \"Error\"]].apply(lambda x: x[\"Joint Name\"] if x[\"Error\"] == 1 else \"No Error\", axis=1)\n",
    "df_data_half_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_by_difficulty = df_data_half_body[['Difficulty', \"Difficulty Name\", \"Error\"]].groupby([\"Difficulty Name\"]).mean().sort_values(by=\"Difficulty\")[\"Error\"]\n",
    "ax = error_by_difficulty.plot.bar()\n",
    "ax.set_ylabel(\"Error Rate\")\n",
    "ax.set_xlabel(\"Difficulty\")\n",
    "ax.set_title(\"Error Rate by Difficulty\")\n",
    "ax.set_ylim(0, 1)\n",
    "labels = ['Trivial', 'Easy', 'Medium', 'Hard']\n",
    "ax.set_xticks([0, 1, 2, 3])\n",
    "ax.set_xticklabels(labels, rotation=0, ha='center')\n",
    "plt.savefig(\"figures/dist_half_body/Error_Rate_by_Difficulty.png\")\n",
    "ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = df_data_half_body[[\"Joint\", \"Error\"]].groupby([\"Joint\"]).sum()\n",
    "s[\"Sorted Names\"] = body_halves\n",
    "s = s.sort_values(by=\"Error\", ascending=False)\n",
    "\n",
    "err_joints = df_data_half_body[[\"Joint\", \"Joint Name\", \"Error\"]]\n",
    "error_distribution_by_joint = err_joints.groupby([\"Joint\", \"Error\"]).count().unstack().reindex(s.index).T\n",
    "\n",
    "error_distribution_by_joint = (error_distribution_by_joint / error_distribution_by_joint.sum()).T.fillna(0)\n",
    "error_distribution_by_joint.columns = error_distribution_by_joint.columns.droplevel()\n",
    "\n",
    "error_distribution_by_joint.index = s[\"Sorted Names\"].tolist()\n",
    "\n",
    "edj = error_distribution_by_joint.T\n",
    "\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "ax1 = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "axs = edj.plot.pie(ax=ax1, subplots=True, layout=(1, 2), legend=False, title=\"Error Distribution by Joint\", fontsize=12, sharex=False, sharey=False, labeldistance=None, explode=(.05,.1), autopct='%1.1f%%', pctdistance=1.25)\n",
    "\n",
    "for ax in axs.flatten():\n",
    "  ax.set_xlabel(ax.get_ylabel())\n",
    "  ax.set_ylabel(\"\")\n",
    "\n",
    "plt.legend([\"No Error\", \"Error\"], loc='upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "ax1.set_title(\"Error Distribution by Joint\")\n",
    "plt.savefig(\"figures/dist_half_body/Error_Distribution_by_Joint.png\")\n",
    "plt.plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_limbs[\"Joint Name\"] = df_data_limbs[\"Joint\"].apply(lambda x: limbs[x])\n",
    "df_data_limbs[\"Difficulty Name\"] = df_data_limbs[\"Difficulty\"].apply(lambda x: \"Trivial\" if x == 0 else \"Easy\" if x == 1 else \"Medium\" if x == 2 else \"Hard\")\n",
    "df_data_limbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_by_difficulty = df_data_limbs[['Difficulty', \"Difficulty Name\", \"Error\"]].groupby([\"Difficulty Name\"]).mean().sort_values(by=\"Difficulty\")[\"Error\"]\n",
    "ax = error_by_difficulty.plot.bar()\n",
    "ax.set_ylabel(\"Error Rate\")\n",
    "ax.set_xlabel(\"Difficulty\")\n",
    "ax.set_title(\"Error Rate by Difficulty\")\n",
    "ax.set_ylim(0, 1)\n",
    "labels = ['Trivial', 'Easy', 'Medium', 'Hard']\n",
    "ax.set_xticks([0, 1, 2, 3])\n",
    "ax.set_xticklabels(labels, rotation=0, ha='center')\n",
    "plt.savefig(\"figures/dist_limbs/Error_Rate_by_Difficulty.png\")\n",
    "ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = df_data_limbs[[\"Joint\", \"Error\"]].groupby([\"Joint\"]).sum()\n",
    "s[\"Sorted Names\"] = limbs\n",
    "s = s.sort_values(by=\"Error\", ascending=False)\n",
    "\n",
    "err_joints = df_data_limbs[[\"Joint\", \"Joint Name\", \"Error\"]]\n",
    "error_distribution_by_joint = err_joints.groupby([\"Joint\", \"Error\"]).count().unstack().reindex(s.index).T\n",
    "\n",
    "error_distribution_by_joint = (error_distribution_by_joint / error_distribution_by_joint.sum()).T.fillna(0)\n",
    "error_distribution_by_joint.columns = error_distribution_by_joint.columns.droplevel()\n",
    "\n",
    "error_distribution_by_joint.index = s[\"Sorted Names\"].tolist()\n",
    "\n",
    "edj = error_distribution_by_joint.T\n",
    "\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "ax1 = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "axs = edj.plot.pie(ax=ax1, subplots=True, layout=(2, 3), legend=False, title=\"Error Distribution by Joint\", fontsize=12, sharex=False, sharey=False, labeldistance=None, explode=(.05,.1), autopct='%1.1f%%', pctdistance=1.35)\n",
    "\n",
    "for ax in axs.flatten():\n",
    "  ax.set_xlabel(ax.get_ylabel())\n",
    "  ax.set_ylabel(\"\")\n",
    "\n",
    "plt.legend([\"No Error\", \"Error\"], loc='upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "ax1.set_title(\"Error Distribution by Joint\")\n",
    "plt.savefig(\"figures/dist_limbs/Error_Distribution_by_Joint.png\")\n",
    "plt.plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Build the model according to the chosen mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_modes = True\n",
    "mode = Mode.FULL_BODY\n",
    "\n",
    "if all_modes:\n",
    "  model_full_body = nn.DataParallel(FESD(Mode.FULL_BODY.get_num_layers()))\n",
    "  model_half_body = nn.DataParallel(FESD(Mode.HALF_BODY.get_num_layers()))\n",
    "  model_limbs     = nn.DataParallel(FESD(Mode.LIMBS.get_num_layers()))\n",
    "  model_joints    = nn.DataParallel(FESD(Mode.JOINTS.get_num_layers()))\n",
    "else:\n",
    "  model = nn.DataParallel(FESD(mode.get_num_layers()))\n",
    "\n",
    "if is_cuda:\n",
    "  if all_modes:\n",
    "    model_full_body = model_full_body.cuda()\n",
    "    model_half_body = model_half_body.cuda()\n",
    "    model_limbs     = model_limbs.cuda()\n",
    "    model_joints    = model_joints.cuda()\n",
    "  else:\n",
    "    model = model.cuda()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model\n",
    "\n",
    "In the following we define the training function and train a network on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (all_modes):\n",
    "  train_loader_full_body  = torch.utils.data.DataLoader(FESDDataset(RECORDING_DIR, train_size, mode=Mode.FULL_BODY), batch_size=batchsize)\n",
    "  test_loader_full_body   = torch.utils.data.DataLoader(FESDDataset(RECORDING_DIR, train_size, mode=Mode.FULL_BODY, test=True))\n",
    "  train_loader_half_body  = torch.utils.data.DataLoader(FESDDataset(RECORDING_DIR, train_size, mode=Mode.HALF_BODY), batch_size=batchsize)\n",
    "  test_loader_half_body   = torch.utils.data.DataLoader(FESDDataset(RECORDING_DIR, train_size, mode=Mode.HALF_BODY, test=True))\n",
    "  train_loader_limbs      = torch.utils.data.DataLoader(FESDDataset(RECORDING_DIR, train_size, mode=Mode.LIMBS), batch_size=batchsize)\n",
    "  test_loader_limbs       = torch.utils.data.DataLoader(FESDDataset(RECORDING_DIR, train_size, mode=Mode.LIMBS, test=True))\n",
    "  train_loader_joints     = torch.utils.data.DataLoader(FESDDataset(RECORDING_DIR, train_size, mode=Mode.JOINTS), batch_size=batchsize)\n",
    "  test_loader_joints      = torch.utils.data.DataLoader(FESDDataset(RECORDING_DIR, train_size, mode=Mode.JOINTS, test=True))\n",
    "\n",
    "else:\n",
    "  train_loader = torch.utils.data.DataLoader(dataset_train, batch_size=batchsize)\n",
    "  test_loader = torch.utils.data.DataLoader(dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch number\n",
    "epochs = 100\n",
    "# optimizer\n",
    "optim = 'adam'\n",
    "# learning rate\n",
    "learning_rate = 0.000025\n",
    "# learning rate scheduler. can be step, poly or cosine\n",
    "lr_scheduler = 'cosine'\n",
    "# warmup epoch\n",
    "warmup_epoch = -1\n",
    "# warmup multiplier\n",
    "warmup_multiplier = 100\n",
    "# for step scheduler. where to decay lr, can be a list\n",
    "lr_decay_epochs = [120, 160, 200]\n",
    "# for step scheduler. step size to decay lr\n",
    "lr_decay_steps = 20 \n",
    "# for step scheduler. decay rate for learning rate\n",
    "lr_decay_rate = 0.01\n",
    "# weight decay\n",
    "weight_decay = 0.0001\n",
    "# momentum for SGD\n",
    "momentum = 0.9\n",
    "# gradient clipping margin\n",
    "clip = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data = len(train_loader.dataset)\n",
    "CE = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "if mode == Mode.FULL_BODY:\n",
    "    CE = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "if is_cuda:\n",
    "    CE = CE.cuda()\n",
    "\n",
    "if all_modes:\n",
    "    if optim == 'adam':\n",
    "        optimizer_full_body = torch.optim.Adam(model_full_body.parameters(),    learning_rate, weight_decay=weight_decay)\n",
    "        optimizer_half_body = torch.optim.Adam(model_half_body.parameters(),    learning_rate, weight_decay=weight_decay)\n",
    "        optimizer_limbs     = torch.optim.Adam(model_limbs.parameters(),        learning_rate, weight_decay=weight_decay)\n",
    "        optimizer_joints    = torch.optim.Adam(model_joints.parameters(),       learning_rate, weight_decay=weight_decay)\n",
    "    elif optim == 'adamW':\n",
    "        optimizer_full_body = torch.optim.AdamW(model_full_body.parameters(),   learning_rate, weight_decay=weight_decay)\n",
    "        optimizer_half_body = torch.optim.AdamW(model_half_body.parameters(),   learning_rate, weight_decay=weight_decay)\n",
    "        optimizer_limbs     = torch.optim.AdamW(model_limbs.parameters(),       learning_rate, weight_decay=weight_decay)\n",
    "        optimizer_joints    = torch.optim.AdamW(model_joints.parameters(),      learning_rate, weight_decay=weight_decay)\n",
    "    elif optim == 'sdg':\n",
    "        optimizer_full_body = torch.optim.SGD(model_full_body.parameters(),     learning_rate / 10.0 * batchsize, momentum=momentum, weight_decay=weight_decay)\n",
    "        optimizer_half_body = torch.optim.SGD(model_half_body.parameters(),     learning_rate / 10.0 * batchsize, momentum=momentum, weight_decay=weight_decay)\n",
    "        optimizer_limbs     = torch.optim.SGD(model_limbs.parameters(),         learning_rate / 10.0 * batchsize, momentum=momentum, weight_decay=weight_decay)\n",
    "        optimizer_joints    = torch.optim.SGD(model_joints.parameters(),        learning_rate / 10.0 * batchsize, momentum=momentum, weight_decay=weight_decay)\n",
    "\n",
    "    scheduler_full_body     = get_scheduler(optimizer_full_body, len(train_loader_full_body), lr_scheduler, lr_decay_epochs, lr_decay_steps, lr_decay_rate, epochs, warmup_epoch,     warmup_multiplier)\n",
    "    scheduler_half_body     = get_scheduler(optimizer_half_body, len(train_loader_half_body), lr_scheduler, lr_decay_epochs, lr_decay_steps, lr_decay_rate, epochs, warmup_epoch, warmup_multiplier)\n",
    "    scheduler_limbs         = get_scheduler(optimizer_limbs, len(train_loader_limbs), lr_scheduler, lr_decay_epochs, lr_decay_steps, lr_decay_rate, epochs, warmup_epoch, warmup_multiplier)\n",
    "    scheduler_joints        = get_scheduler(optimizer_joints, len(train_loader_joints), lr_scheduler, lr_decay_epochs, lr_decay_steps, lr_decay_rate, epochs, warmup_epoch, warmup_multiplier)\n",
    "else:\n",
    "    if optim == 'adam':\n",
    "        optimizer = torch.optim.Adam(model.parameters(), learning_rate, weight_decay=weight_decay)\n",
    "    elif optim == 'adamW':\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), learning_rate, weight_decay=weight_decay)\n",
    "    elif optim == 'sdg':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), learning_rate / 10.0 * batchsize, momentum=momentum, weight_decay=weight_decay)\n",
    "\n",
    "    scheduler = get_scheduler(optimizer, len(train_loader), lr_scheduler, lr_decay_epochs, lr_decay_steps, lr_decay_rate, epochs, warmup_epoch, warmup_multiplier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_modes:\n",
    "    model_iterator = list(zip([Mode.FULL_BODY,          Mode.HALF_BODY,         Mode.LIMBS,         Mode.JOINTS], \n",
    "                              [model_full_body,         model_half_body,        model_limbs,        model_joints], \n",
    "                              [optimizer_full_body,     optimizer_half_body,    optimizer_limbs,    optimizer_joints], \n",
    "                              [scheduler_full_body,     scheduler_half_body,    scheduler_limbs,    scheduler_joints],\n",
    "                              [train_loader_full_body,  train_loader_half_body, train_loader_limbs, train_loader_joints], \n",
    "                              [test_loader_full_body,   test_loader_half_body,  test_loader_limbs,  test_loader_joints]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_columns = [\"epoch\", \"iteration\", \"joint_id\",\n",
    "                  \"gts\", \"preds\", \"confidences\", \n",
    "                  \"Avg loss\", \"loss\", \"accuracy\", \n",
    "                  \"tp\", \"tn\", \"fp\", \"fn\", \"precision\", \"recall\", \"f1\", \n",
    "                  \"cohens_kappa\", \"learning_rate\",\n",
    "                  \"train_test\", \"exercise\", \"simplified\", \"mode\"]\n",
    "                  \n",
    "df_model = pd.DataFrame(columns=model_columns)\n",
    "pb = tqdm(range(1, epochs + 1), desc='Epoch')\n",
    "\n",
    "for epoch in pb:\n",
    "    if all_modes:    \n",
    "        print(f\"--- {epoch:3d} ---\")\n",
    "        for mode, model, optimizer, scheduler, train_loader, _ in model_iterator:\n",
    "            tic = time()\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            loss = train(train_loader, model, optimizer, CE, scheduler, clip, epoch, epochs, is_cuda, mode, df_model)  \n",
    "            pb.set_description(f'Epoch (mode: {mode.name.lower().replace(\"_\", \" \"):>10}, lr: {optimizer.param_groups[0][\"lr\"]:.3e}, loss: {loss:.3e})')\n",
    "        \n",
    "            print(f'Epoch (mode: {mode.name.lower().replace(\"_\", \" \"):>10}, lr: {optimizer.param_groups[0][\"lr\"]:.3e}, loss: {loss:.3e})')\n",
    "\n",
    "            if (epoch) % 10 == 0:\n",
    "                torch.save(model.state_dict(), os.path.join(CHECKPOINT_DIR, f\"{mode.name.lower()}_{epoch}_ckpt.pth\")) \n",
    "    else:\n",
    "        tic = time()\n",
    "        torch.cuda.empty_cache()\n",
    "        loss = train(train_loader, model, optimizer, CE, scheduler, clip, epoch, epochs, is_cuda, mode, df_model)\n",
    "\n",
    "        pb.set_description(f'Epoch (mode: {mode.name.lower().replace(\"_\", \" \"):>10}, lr: {optimizer.param_groups[0][\"lr\"]:.3e}, loss: {loss:.3e})')\n",
    "        \n",
    "        if (epoch) % 10 == 0:\n",
    "            torch.save(model.state_dict(), os.path.join(CHECKPOINT_DIR, f\"{epoch}_ckpt.pth\"))\n",
    "    \n",
    "if all_modes:\n",
    "    for mode, model, _, _, _, _ in model_iterator:\n",
    "        torch.save(model.state_dict(), os.path.join(CHECKPOINT_DIR, f\"{mode.name.lower()}_last_ckpt.pth\")) \n",
    "        print(f\"model saved {os.path.join(CHECKPOINT_DIR, f'last_ckpt.pth')}!\")\n",
    "else:\n",
    "    torch.save(model.state_dict(), os.path.join(CHECKPOINT_DIR, f\"last_ckpt.pth\"))\n",
    "    print(f\"model saved {os.path.join(CHECKPOINT_DIR, f'last_ckpt.pth')}!\")\n",
    "    checkpoint = os.path.join(CHECKPOINT_DIR, f\"last_ckpt.pth\")\n",
    "\n",
    "df_model.to_parquet('ModelAnalysis.parquet.gzip', compression='gzip') "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_modes:\n",
    "  for mode, model, _, _, _, test_loader in model_iterator:\n",
    "    test(test_loader, model, CE, is_cuda, mode, df_model)\n",
    "else: \n",
    "  test(test_loader, model, CE, is_cuda, mode, df_model)\n",
    "  \n",
    "df_model.to_parquet('ModelAnalysis.parquet.gzip', compression='gzip') "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "Here we create all graphs and analysis for the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['epoch', 'iteration', 'joint_id', 'gts', 'preds', 'confidences',\n",
      "       'Avg loss', 'loss', 'accuracy', 'tp', 'tn', 'fp', 'fn', 'precision',\n",
      "       'recall', 'f1', 'cohens_kappa', 'learning_rate', 'train_test',\n",
      "       'exercise', 'simplified', 'mode', 'difficulty'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Load the results of training and testing\n",
    "\n",
    "df_model = pd.read_parquet('ModelAnalysis.parquet.gzip')\n",
    "df_model[\"difficulty\"] = df_model.apply(lambda x: int(x[\"exercise\"][2]), axis=1)\n",
    "print(df_model.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Body Evaluation\n",
    "\n",
    "The evaluation of the whole body as an error boolean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_full_body = df_model[df_model[\"mode\"] == \"FULL_BODY\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Half Body Evaluation\n",
    "\n",
    "The evaluation of the body split into two parts (upper and lower body), each as an error boolean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         full_body\n",
       "1         full_body\n",
       "2         full_body\n",
       "3         full_body\n",
       "4         full_body\n",
       "            ...    \n",
       "311455       joints\n",
       "311456       joints\n",
       "311457       joints\n",
       "311458       joints\n",
       "311459       joints\n",
       "Name: mode, Length: 311460, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model_compare = pd.read_parquet('df.parquet.gzip')\n",
    "df_model.compare(df_model_compare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: joint_id, dtype: int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model_half_body = df_model[df_model[\"mode\"] == \"HALF_BODY\"]\n",
    "df_model_half_body[\"joint_names\"] = df_model_half_body[\"joint_id\"].apply(lambda x: body_halves[x], axis=1)\n",
    "df_model_half_body[\"joint_id\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limb Evaluation\n",
    "\n",
    "The evaluation of the body split into limbs (Head and lower body), each as an error boolean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model[\"joint_name\"] = df_model.apply(lambda x: joint_names[x[\"joint_id\"]], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model.groupby([\"epoch\", \"difficulty\"]).mean()[\"accuracy\"].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_model[df_model['train_test'] == 'train']\n",
    "df_test = df_model[df_model['train_test'] == 'test']\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[df_train[\"simplified\"] == True].groupby(\"epoch\").mean(numeric_only=True)[\"accuracy\"].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[df_test['simplified'] == True].groupby([\"epoch\", \"joint_name\"]).sum().sort_values(\"accuracy\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = 0\n",
    "for j in range(0, 20):\n",
    "  print(gt, pred)\n",
    "  loss += CE(gt[:,j], pred[:,j])\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.array(gts[0])\n",
    "gts[0]\n",
    "gt = torch.tensor(np.array(json.loads(gts[5])))\n",
    "pred = torch.tensor(np.array(json.loads(preds[5])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gts) * 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_test_cp = df_train_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gts = np.array(df_train[df_train['simplified'] == False][\"gts\"].tolist())\n",
    "preds = np.array(df_train[df_train['simplified'] == False][\"preds\"].tolist())\n",
    "exercises = np.array(df_train[df_train['simplified'] == False][\"exercise\"].tolist())\n",
    "\n",
    "model_columns_test = [\"epoch\", \"iteration\", \"joint_id\",\n",
    "                  \"gts\", \"preds\", \"confidences\", \n",
    "                  \"Avg loss\", \"loss\", \"accuracy\", \n",
    "                  \"tp\", \"tn\", \"fp\", \"fn\", \"precision\", \"recall\", \"f1\", \n",
    "                  \"cohens_kappa\", \"learning_rate\",\n",
    "                  \"train_test\", \"exercise\", \"simplified\"]\n",
    "\n",
    "df_train_test = pd.DataFrame(columns=model_columns_test)\n",
    "\n",
    "for it, (gt, pred, exercise) in tqdm(enumerate(zip(gts, preds, exercises)), total=len(gts)):\n",
    "  i = it % len(train_loader)\n",
    "  epoch = it // len(train_loader)\n",
    "\n",
    "  gt = torch.tensor(np.array(json.loads(gt)))\n",
    "  pred = torch.tensor(np.array(json.loads(pred)))\n",
    "  \n",
    "  loss = 0\n",
    "\n",
    "  for j in range(0, 20):\n",
    "    loss += CE(gt[:,j], pred[:,j])\n",
    "\n",
    "  loss_record = AvgMeter()\n",
    "\n",
    "  loss_record.update(loss.data, 1)\n",
    "\n",
    "  val(pred, gt, loss_record, loss, np.NaN, epoch, epochs, i, len(train_loader), \"train\",  exercise, df_train_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gts = np.array(df_test[df_test['simplified'] == False][\"gts\"].tolist())\n",
    "preds = np.array(df_test[df_test['simplified'] == False][\"preds\"].tolist())\n",
    "exercises = np.array(df_test[df_test['simplified'] == False][\"exercise\"].tolist())\n",
    "\n",
    "model_columns_test = [\"epoch\", \"iteration\", \"joint_id\",\n",
    "                  \"gts\", \"preds\", \"confidences\", \n",
    "                  \"Avg loss\", \"loss\", \"accuracy\", \n",
    "                  \"tp\", \"tn\", \"fp\", \"fn\", \"precision\", \"recall\", \"f1\", \n",
    "                  \"cohens_kappa\", \"learning_rate\",\n",
    "                  \"train_test\", \"exercise\", \"simplified\"]\n",
    "\n",
    "df_test_test = pd.DataFrame(columns=model_columns_test)\n",
    "\n",
    "for it, (gt, pred, exercise) in tqdm(enumerate(zip(gts, preds, exercises)), total=len(gts)):\n",
    "  i = it % len(train_loader)\n",
    "  epoch = it // len(train_loader)\n",
    "\n",
    "  gt = torch.tensor(np.array(json.loads(gt)))\n",
    "  pred = torch.tensor(np.array(json.loads(pred)))\n",
    "  \n",
    "  loss = 0\n",
    "\n",
    "  for j in range(0, 20):\n",
    "    loss += CE(gt[:,j], pred[:,j])\n",
    "\n",
    "  loss_record = AvgMeter()\n",
    "\n",
    "  loss_record.update(loss.data, 1)\n",
    "\n",
    "  val(pred, gt, loss_record, loss, np.NaN, epoch, epochs, i, len(train_loader), \"test\",  exercise, df_test_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_test[\"joint_name\"] = df_test_test.apply(lambda x: joint_names[x[\"joint_id\"]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_test[df_test_test[\"exercise\"] == \"E-1.01\"][df_test_test[\"simplified\"] == True].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "38*20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tps = df_test_test[df_test_test[\"simplified\"] == True].groupby([\"exercise\"]).sum(numeric_only=True)[[\"tp\", \"tn\", \"fp\", \"fn\", \"accuracy\"]]\n",
    "tps[\"positives\"] = tps[\"tp\"] + tps[\"fn\"]\n",
    "tps[\"negatives\"] = tps[\"tn\"] + tps[\"fp\"]\n",
    "tps[\"accuracy\"] = tps[\"accuracy\"] / (tps[\"positives\"] + tps[\"negatives\"])\n",
    "tps.sort_values('accuracy', ascending= False).sort_index(level='exercise', sort_remaining=False)\n",
    "pd.set_option('display.max_rows', None)\n",
    "tps = tps.sort_values('accuracy', ascending= False).sort_index(level='exercise', sort_remaining=False)\n",
    "tps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_test[df_train_test[\"simplified\"] == True][df_train_test[\"train_test\"] == 'train'].groupby([\"joint_id\"]).mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "223e9e27cbc670f5c22c33ebc1577b54f1cca48f7f6b973b1c50992e7fee7cdc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
