{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FESDModel\n",
    "\n",
    "FESD - Fault estimation for skeleton detection - is a suite that aims at finding faults in joints of skeletons, which are detected by human pose estimatiors.\n",
    "\n",
    "FESDData is the sister project to this notebook, which aims at recording depth and rgb data, as well as populating the data with human poses from variing human pose estimators.\n",
    "\n",
    "Furthermore, FESTData augments all data based on joint confidence.\n",
    "\n",
    "FFESDModel aims to develop and evaluate a model based on the faulty and augmented joint data as well as RGBD data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries\n",
    "\n",
    "We need a range of libraries which are imported here. We also define some constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "h:\\FESD\\FESDModel\\env\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num cuda GPUs: 1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from time import time\n",
    "\n",
    "from data import FESDDataset\n",
    "from data import Frame, AugmentationParams\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "import cv2\n",
    "\n",
    "from model import FESD, train\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from utils import AvgMeter, clip_gradient, get_scheduler\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(f\"Num cuda GPUs: {num_gpus}\")\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files/Graphviz/bin/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RECORDING_DIR = Path('H:/Recordings/')\n",
    "CHECKPOINT_DIR = Path('checkpoints')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "Firstly we need to import all the recordings into the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(file=\"Exercises.json\", mode='r') as file:\n",
    "  exercises_json = json.load(file)['Exercises']\n",
    "\n",
    "with open(file=\"JointErrors.json\", mode='r') as file:\n",
    "  joint_error_json = json.load(file)\n",
    "\n",
    "with open(file=\"SkeletonErrors.json\", mode='r') as file:\n",
    "  skeleton_error_json = json.load(file)\n",
    "\n",
    "len(exercises_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recordings Found: 25\n",
      "Total Frames: 7500\n"
     ]
    }
   ],
   "source": [
    "batchsize = 10\n",
    "train_size = 300\n",
    "\n",
    "dataset = FESDDataset(RECORDING_DIR, train_size)\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batchsize, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 300, 300]) torch.Size([1, 300, 300]) torch.Size([3, 25]) torch.Size([25])\n",
      "All Missing Joints\n",
      "5\n",
      "All Wrong Joints\n",
      "tensor([[ -1.4217,  -1.1519],\n",
      "        [167.1556, 170.9957],\n",
      "        [  0.2603,   0.2511]])\n"
     ]
    }
   ],
   "source": [
    "dataset.randomize_augmentation_params = False\n",
    "dataset.reset_augmentation_params()\n",
    "rgb, depth, pose_2d, errors = dataset[0]\n",
    "dataset.frame.show()\n",
    "\n",
    "print(rgb.shape, depth.shape, pose_2d.shape, errors.shape)\n",
    "\n",
    "print(\"All Missing Joints\")\n",
    "print(len(errors[errors==1]))\n",
    "\n",
    "print(\"All Wrong Joints\")\n",
    "print(pose_2d[:,errors==2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model\n",
    "\n",
    "The model proposed by RD3D is based on resnet50 so we copy a pretrained resnet50 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "h:\\FESD\\FESDModel\\env\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "h:\\FESD\\FESDModel\\env\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FESD(\n",
      "  (rgb_conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (rgb_pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (rgb_conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (rgb_pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (rgb_conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (rgb_pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (depth_conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (depth_pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (depth_conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (depth_pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (depth_conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (depth_pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (joint_conv1): Conv1d(3, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (joint_conv2): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (joint_conv3): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (fc1): Linear(in_features=86144, out_features=1024, bias=True)\n",
      "  (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (fc3): Linear(in_features=512, out_features=25, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "resnet = torchvision.models.resnet50(pretrained=True)\n",
    "\n",
    "model = FESD()\n",
    "print(model)\n",
    "\n",
    "model = nn.DataParallel(model).cuda()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "\n",
    "In the following we define the training function and train a network on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch number\n",
    "epochs = 50\n",
    "# optimizer\n",
    "optim = 'adam'\n",
    "# learning rate\n",
    "learning_rate = 0.000025\n",
    "# learning rate scheduler. can be step, poly or cosine\n",
    "lr_scheduler = 'cosine'\n",
    "# warmup epoch\n",
    "warmup_epoch = -1\n",
    "# warmup multiplier\n",
    "warmup_multiplier = 100\n",
    "# for step scheduler. where to decay lr, can be a list\n",
    "lr_decay_epochs = [120, 160, 200]\n",
    "# for step scheduler. step size to decay lr\n",
    "lr_decay_steps = 20 \n",
    "# for step scheduler. decay rate for learning rate\n",
    "lr_decay_rate = 0.01\n",
    "# weight decay\n",
    "weight_decay = 0.0001\n",
    "# momentum for SGD\n",
    "momentum = 0.9\n",
    "# gradient clipping margin\n",
    "clip = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [001/050],                   Step [0100/0750],                   Loss: 10.3897,                   MSE: 4.9838,                   RMSE: 2.2043\n",
      "Epoch [001/050],                   Step [0200/0750],                   Loss: 10.2731,                   MSE: 6.3999,                   RMSE: 2.5202\n",
      "Epoch [001/050],                   Step [0300/0750],                   Loss: 10.2650,                   MSE: 7.3576,                   RMSE: 2.6999\n",
      "Epoch [001/050],                   Step [0400/0750],                   Loss: 9.4654,                   MSE: 9.6512,                   RMSE: 3.0986\n",
      "Epoch [001/050],                   Step [0500/0750],                   Loss: 9.5169,                   MSE: 10.0759,                   RMSE: 3.1570\n",
      "Epoch [001/050],                   Step [0600/0750],                   Loss: 9.7352,                   MSE: 11.0111,                   RMSE: 3.3005\n",
      "Epoch [001/050],                   Step [0700/0750],                   Loss: 9.3312,                   MSE: 12.5221,                   RMSE: 3.5196\n",
      "Epoch [001/050],                   Step [0750/0750],                   Loss: 9.5071,                   MSE: 12.9621,                   RMSE: 3.5942\n",
      "epoch 1, total time 693.74, learning_rate 2.4977239944844513e-05\n",
      "Epoch [002/050],                   Step [0100/0750],                   Loss: 9.5126,                   MSE: 14.4774,                   RMSE: 3.7974\n",
      "Epoch [002/050],                   Step [0200/0750],                   Loss: 9.4324,                   MSE: 15.6479,                   RMSE: 3.9464\n",
      "Epoch [002/050],                   Step [0300/0750],                   Loss: 9.2114,                   MSE: 16.0824,                   RMSE: 3.9967\n",
      "Epoch [002/050],                   Step [0400/0750],                   Loss: 10.0839,                   MSE: 16.9185,                   RMSE: 4.0834\n",
      "Epoch [002/050],                   Step [0500/0750],                   Loss: 9.0310,                   MSE: 18.6016,                   RMSE: 4.3037\n",
      "Epoch [002/050],                   Step [0600/0750],                   Loss: 8.8976,                   MSE: 18.8127,                   RMSE: 4.3219\n",
      "Epoch [002/050],                   Step [0700/0750],                   Loss: 9.1068,                   MSE: 21.6373,                   RMSE: 4.6385\n",
      "Epoch [002/050],                   Step [0750/0750],                   Loss: 8.8688,                   MSE: 22.6786,                   RMSE: 4.7517\n",
      "epoch 2, total time 683.79, learning_rate 2.4909046116063206e-05\n",
      "Epoch [003/050],                   Step [0100/0750],                   Loss: 9.0918,                   MSE: 21.2034,                   RMSE: 4.5899\n",
      "Epoch [003/050],                   Step [0200/0750],                   Loss: 9.2892,                   MSE: 23.3718,                   RMSE: 4.8155\n",
      "Epoch [003/050],                   Step [0300/0750],                   Loss: 8.9728,                   MSE: 21.6670,                   RMSE: 4.6333\n",
      "Epoch [003/050],                   Step [0400/0750],                   Loss: 8.9424,                   MSE: 24.4511,                   RMSE: 4.9311\n",
      "Epoch [003/050],                   Step [0500/0750],                   Loss: 8.9009,                   MSE: 27.0969,                   RMSE: 5.1989\n",
      "Epoch [003/050],                   Step [0600/0750],                   Loss: 8.9168,                   MSE: 26.0418,                   RMSE: 5.0781\n",
      "Epoch [003/050],                   Step [0700/0750],                   Loss: 8.9956,                   MSE: 28.2700,                   RMSE: 5.2955\n",
      "Epoch [003/050],                   Step [0750/0750],                   Loss: 9.2512,                   MSE: 25.5569,                   RMSE: 5.0318\n",
      "epoch 3, total time 683.99, learning_rate 2.479567719620675e-05\n",
      "Epoch [004/050],                   Step [0100/0750],                   Loss: 9.0622,                   MSE: 27.2320,                   RMSE: 5.2050\n",
      "Epoch [004/050],                   Step [0200/0750],                   Loss: 8.7278,                   MSE: 28.1294,                   RMSE: 5.2836\n",
      "Epoch [004/050],                   Step [0300/0750],                   Loss: 9.3130,                   MSE: 30.1109,                   RMSE: 5.4684\n",
      "Epoch [004/050],                   Step [0400/0750],                   Loss: 8.9946,                   MSE: 31.8886,                   RMSE: 5.6135\n",
      "Epoch [004/050],                   Step [0500/0750],                   Loss: 8.6376,                   MSE: 34.3525,                   RMSE: 5.8408\n",
      "Epoch [004/050],                   Step [0600/0750],                   Loss: 8.9910,                   MSE: 32.5666,                   RMSE: 5.6753\n",
      "Epoch [004/050],                   Step [0700/0750],                   Loss: 9.0220,                   MSE: 37.6480,                   RMSE: 6.1193\n",
      "Epoch [004/050],                   Step [0750/0750],                   Loss: 8.8535,                   MSE: 37.5403,                   RMSE: 6.1101\n",
      "epoch 4, total time 674.11, learning_rate 2.4637563232420086e-05\n",
      "Epoch [005/050],                   Step [0100/0750],                   Loss: 8.9303,                   MSE: 35.0901,                   RMSE: 5.8841\n",
      "Epoch [005/050],                   Step [0200/0750],                   Loss: 8.6348,                   MSE: 36.7214,                   RMSE: 6.0265\n",
      "Epoch [005/050],                   Step [0300/0750],                   Loss: 8.7925,                   MSE: 38.6555,                   RMSE: 6.1824\n",
      "Epoch [005/050],                   Step [0400/0750],                   Loss: 8.8795,                   MSE: 36.8755,                   RMSE: 6.0387\n",
      "Epoch [005/050],                   Step [0500/0750],                   Loss: 8.7735,                   MSE: 43.1743,                   RMSE: 6.5419\n",
      "Epoch [005/050],                   Step [0600/0750],                   Loss: 8.8344,                   MSE: 44.5667,                   RMSE: 6.6434\n",
      "Epoch [005/050],                   Step [0700/0750],                   Loss: 8.7004,                   MSE: 46.8560,                   RMSE: 6.8192\n",
      "Epoch [005/050],                   Step [0750/0750],                   Loss: 8.8074,                   MSE: 48.6847,                   RMSE: 6.9603\n",
      "epoch 5, total time 713.93, learning_rate 2.4435304005125804e-05\n",
      "Epoch [006/050],                   Step [0100/0750],                   Loss: 9.0630,                   MSE: 49.7460,                   RMSE: 7.0326\n",
      "Epoch [006/050],                   Step [0200/0750],                   Loss: 8.9919,                   MSE: 46.4789,                   RMSE: 6.7924\n",
      "Epoch [006/050],                   Step [0300/0750],                   Loss: 8.7217,                   MSE: 49.3500,                   RMSE: 6.9950\n",
      "Epoch [006/050],                   Step [0400/0750],                   Loss: 8.9591,                   MSE: 49.2884,                   RMSE: 6.9947\n",
      "Epoch [006/050],                   Step [0500/0750],                   Loss: 8.6699,                   MSE: 53.8572,                   RMSE: 7.3065\n",
      "Epoch [006/050],                   Step [0600/0750],                   Loss: 8.9361,                   MSE: 58.3365,                   RMSE: 7.5881\n",
      "Epoch [006/050],                   Step [0700/0750],                   Loss: 8.7026,                   MSE: 58.9143,                   RMSE: 7.6493\n",
      "Epoch [006/050],                   Step [0750/0750],                   Loss: 8.7448,                   MSE: 62.6894,                   RMSE: 7.8925\n",
      "epoch 6, total time 689.91, learning_rate 2.4189666752852198e-05\n",
      "Epoch [007/050],                   Step [0100/0750],                   Loss: 8.7909,                   MSE: 66.2777,                   RMSE: 8.1197\n",
      "Epoch [007/050],                   Step [0200/0750],                   Loss: 9.1506,                   MSE: 58.3941,                   RMSE: 7.5982\n",
      "Epoch [007/050],                   Step [0300/0750],                   Loss: 8.9326,                   MSE: 61.9057,                   RMSE: 7.8383\n",
      "Epoch [007/050],                   Step [0400/0750],                   Loss: 9.0327,                   MSE: 59.7118,                   RMSE: 7.6977\n",
      "Epoch [007/050],                   Step [0500/0750],                   Loss: 8.6994,                   MSE: 71.5502,                   RMSE: 8.4302\n",
      "Epoch [007/050],                   Step [0600/0750],                   Loss: 8.6769,                   MSE: 66.2967,                   RMSE: 8.0925\n",
      "Epoch [007/050],                   Step [0700/0750],                   Loss: 8.6617,                   MSE: 74.7738,                   RMSE: 8.6150\n",
      "Epoch [007/050],                   Step [0750/0750],                   Loss: 8.8481,                   MSE: 83.1559,                   RMSE: 9.0831\n",
      "epoch 7, total time 660.55, learning_rate 2.3901583261834247e-05\n",
      "Epoch [008/050],                   Step [0100/0750],                   Loss: 8.8755,                   MSE: 60.4576,                   RMSE: 7.7292\n",
      "Epoch [008/050],                   Step [0200/0750],                   Loss: 8.6338,                   MSE: 76.4857,                   RMSE: 8.6959\n",
      "Epoch [008/050],                   Step [0300/0750],                   Loss: 8.7452,                   MSE: 77.4448,                   RMSE: 8.7660\n",
      "Epoch [008/050],                   Step [0400/0750],                   Loss: 8.6622,                   MSE: 88.6098,                   RMSE: 9.3787\n",
      "Epoch [008/050],                   Step [0500/0750],                   Loss: 8.7472,                   MSE: 66.7613,                   RMSE: 8.0935\n",
      "Epoch [008/050],                   Step [0600/0750],                   Loss: 8.8164,                   MSE: 68.4193,                   RMSE: 8.2124\n",
      "Epoch [008/050],                   Step [0700/0750],                   Loss: 8.6974,                   MSE: 82.8569,                   RMSE: 9.0537\n",
      "Epoch [008/050],                   Step [0750/0750],                   Loss: 8.8794,                   MSE: 86.8003,                   RMSE: 9.2857\n",
      "epoch 8, total time 727.73, learning_rate 2.3572146331429345e-05\n",
      "Epoch [009/050],                   Step [0100/0750],                   Loss: 8.6334,                   MSE: 81.2067,                   RMSE: 8.9619\n",
      "Epoch [009/050],                   Step [0200/0750],                   Loss: 8.8725,                   MSE: 82.1951,                   RMSE: 9.0299\n",
      "Epoch [009/050],                   Step [0300/0750],                   Loss: 8.7998,                   MSE: 82.3952,                   RMSE: 9.0108\n",
      "Epoch [009/050],                   Step [0400/0750],                   Loss: 8.6365,                   MSE: 95.2965,                   RMSE: 9.6790\n",
      "Epoch [009/050],                   Step [0500/0750],                   Loss: 8.6702,                   MSE: 89.8866,                   RMSE: 9.4354\n",
      "Epoch [009/050],                   Step [0600/0750],                   Loss: 8.5855,                   MSE: 102.1104,                   RMSE: 10.0679\n",
      "Epoch [009/050],                   Step [0700/0750],                   Loss: 8.9452,                   MSE: 100.7694,                   RMSE: 9.9779\n",
      "Epoch [009/050],                   Step [0750/0750],                   Loss: 8.8969,                   MSE: 91.1606,                   RMSE: 9.4974\n",
      "epoch 9, total time 677.04, learning_rate 2.3202605628755292e-05\n",
      "Epoch [010/050],                   Step [0100/0750],                   Loss: 8.6043,                   MSE: 110.8397,                   RMSE: 10.4735\n",
      "Epoch [010/050],                   Step [0200/0750],                   Loss: 8.7479,                   MSE: 117.2623,                   RMSE: 10.7839\n",
      "Epoch [010/050],                   Step [0300/0750],                   Loss: 8.6940,                   MSE: 115.7314,                   RMSE: 10.7224\n",
      "Epoch [010/050],                   Step [0400/0750],                   Loss: 8.8201,                   MSE: 106.9518,                   RMSE: 10.2907\n",
      "Epoch [010/050],                   Step [0500/0750],                   Loss: 8.8914,                   MSE: 97.5020,                   RMSE: 9.8378\n",
      "Epoch [010/050],                   Step [0600/0750],                   Loss: 8.6189,                   MSE: 120.1266,                   RMSE: 10.9067\n",
      "Epoch [010/050],                   Step [0700/0750],                   Loss: 8.6287,                   MSE: 122.4376,                   RMSE: 11.0135\n",
      "Epoch [010/050],                   Step [0750/0750],                   Loss: 8.7402,                   MSE: 128.2059,                   RMSE: 11.2842\n",
      "epoch 10, total time 665.75, learning_rate 2.2794362948274607e-05\n",
      "checkpoint saved checkpoints\\10_ckpt.pth!\n",
      "Epoch [011/050],                   Step [0100/0750],                   Loss: 8.7265,                   MSE: 137.0741,                   RMSE: 11.6681\n",
      "Epoch [011/050],                   Step [0200/0750],                   Loss: 8.9226,                   MSE: 120.9601,                   RMSE: 10.9639\n",
      "Epoch [011/050],                   Step [0300/0750],                   Loss: 8.6280,                   MSE: 132.1347,                   RMSE: 11.4601\n",
      "Epoch [011/050],                   Step [0400/0750],                   Loss: 9.4938,                   MSE: 122.8411,                   RMSE: 10.8996\n",
      "Epoch [011/050],                   Step [0500/0750],                   Loss: 8.6493,                   MSE: 110.1831,                   RMSE: 10.4302\n",
      "Epoch [011/050],                   Step [0600/0750],                   Loss: 8.6632,                   MSE: 123.5779,                   RMSE: 11.0574\n",
      "Epoch [011/050],                   Step [0700/0750],                   Loss: 8.5304,                   MSE: 112.8230,                   RMSE: 10.5525\n",
      "Epoch [011/050],                   Step [0750/0750],                   Loss: 9.0009,                   MSE: 120.4190,                   RMSE: 10.9214\n",
      "epoch 11, total time 664.43, learning_rate 2.2348966894307946e-05\n",
      "Epoch [012/050],                   Step [0100/0750],                   Loss: 9.0417,                   MSE: 137.2847,                   RMSE: 11.6548\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "dataset.randomize_augmentation_params = False\n",
    "dataset.reset_augmentation_params()\n",
    "# routine\n",
    "for epoch in range(1, epochs + 1):\n",
    "    tic = time()\n",
    "    train(train_loader, model, optimizer, CE, scheduler, clip, epoch, epochs)\n",
    "    print(f'epoch {epoch}, total time {time() - tic:.2f}, learning_rate {optimizer.param_groups[0][\"lr\"]}')\n",
    "\n",
    "    if (epoch) % 10 == 0:\n",
    "        torch.save(model.state_dict(), os.path.join(CHECKPOINT_DIR, f\"{epoch}_ckpt.pth\"))\n",
    "\n",
    "        print(\"checkpoint saved {}!\".format(os.path.join(CHECKPOINT_DIR, f\"{epoch}_ckpt.pth\")))\n",
    "        \n",
    "torch.save(model.state_dict(), os.path.join(CHECKPOINT_DIR, f\"last_ckpt.pth\"))\n",
    "print(f\"model saved {os.path.join(CHECKPOINT_DIR, f'last_ckpt.pth')}!\")\n",
    "checkpoint = os.path.join(opt.output_dir, f\"last_ckpt.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "223e9e27cbc670f5c22c33ebc1577b54f1cca48f7f6b973b1c50992e7fee7cdc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
