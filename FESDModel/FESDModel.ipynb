{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FESDModel\n",
    "\n",
    "FESD - Fault estimation for skeleton detection - is a suite that aims at finding faults in joints of skeletons, which are detected by human pose estimatiors.\n",
    "\n",
    "FESDData is the sister project to this notebook, which aims at recording depth and rgb data, as well as populating the data with human poses from variing human pose estimators.\n",
    "\n",
    "Furthermore, FESTData augments all data based on joint confidence.\n",
    "\n",
    "FFESDModel aims to develop and evaluate a model based on the faulty and augmented joint data as well as RGBD data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries\n",
    "\n",
    "We need a range of libraries which are imported here. We also define some constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import tqdm\n",
    "\n",
    "import cv2\n",
    "import pyrealsense2 as rs\n",
    "\n",
    "sns.set_style()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RECORDING_DIR = 'H:/Recordings/'\n",
    "EXERCISES_FILE_NAME = 'Exercises.json'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "Firstly we need to import all the recordings into the notebook.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Metadata\n",
    "\n",
    "It is important to load the metadata, such as the session parameters, the exercises and the recording paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recording_jsons = []\n",
    "for file in os.listdir(RECORDING_DIR):\n",
    "  if (file.endswith('.json') and \n",
    "      not file.endswith('Skeleton.json') and \n",
    "      not file.endswith(EXERCISES_FILE_NAME)):\n",
    "    with open(file=os.path.join(RECORDING_DIR, file), mode='r') as file:\n",
    "      data = json.load(file)\n",
    "      recording_jsons.append(data)\n",
    "\n",
    "len(recording_jsons)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we load the Exercises from the exercise file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exercises_json = []\n",
    "\n",
    "with open(file=os.path.join(RECORDING_DIR, EXERCISES_FILE_NAME), mode='r') as file:\n",
    "  exercises_json = json.load(file)['Exercises']\n",
    "\n",
    "len(exercises_json)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we extract all recordings from the sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recording_paths_rs = []\n",
    "recording_paths_orbbec = []\n",
    "\n",
    "for recording in recording_jsons:\n",
    "  for camera in recording['Cameras']:\n",
    "    if camera['Type'] == 'Realsense':\n",
    "      recording_paths_rs.append(camera['FileName'])\n",
    "\n",
    "len(recording_paths_rs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Frames from Realsense Camera\n",
    "\n",
    "Next we load the frames from the realsense recordings into arrays of CV matrixes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_recording_rs(recording_path_rs, num_frames: int):\n",
    "  pipeline = rs.pipeline()\n",
    "  config = rs.config()\n",
    "\n",
    "  rs.config.enable_device_from_file(config, os.path.join(RECORDING_DIR, recording_path_rs))\n",
    "  \n",
    "  # Configure the pipeline to stream the depth stream\n",
    "  # Change this parameters according to the recorded bag file resolution\n",
    "  config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)\n",
    "  config.enable_stream(rs.stream.color, 1280, 720, rs.format.rgb8, 30)\n",
    "\n",
    "  # Start streaming from file\n",
    "  pipeline.start(config)\n",
    "  device = pipeline.get_active_profile().get_device()\n",
    "  \n",
    "  device.as_playback().set_real_time(False)\n",
    "    \n",
    "  depth_scale = device.first_depth_sensor().get_depth_scale()\n",
    "  \n",
    "  frames_to_return = []\n",
    "  \n",
    "  align = rs.align(rs.stream.depth)\n",
    "\n",
    "  # Streaming loop\n",
    "  for i in tqdm.tqdm(range(num_frames)):\n",
    "\n",
    "    # Get frameset of depth\n",
    "    frames = pipeline.wait_for_frames()\n",
    "    frames = align.process(frames)\n",
    "    depth_frame = frames.get_depth_frame()\n",
    "    color_frame = frames.get_color_frame()\n",
    "    \n",
    "    if not depth_frame or not color_frame:\n",
    "      print(\"Warning: Frame missing!\")\n",
    "      continue\n",
    "\n",
    "    color_frame = np.asanyarray(color_frame.get_data())\n",
    "    color_frame = cv2.cvtColor(color_frame, cv2.COLOR_BGR2RGB) / 255\n",
    "\n",
    "    depth_frame = np.asanyarray(depth_frame.get_data())\n",
    "    depth_frame.shape = (480, 640, 1)\n",
    "    \n",
    "    frame = np.append(color_frame, depth_frame * depth_scale, axis=2)\n",
    "    \n",
    "    frames_to_return.append(frame)\n",
    "  pipeline.stop()\n",
    "\n",
    "  return frames_to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = {}\n",
    "\n",
    "for (i, recording_path_rs) in enumerate(recording_paths_rs):\n",
    "  print(f'Loading recording {i+1} of {len(recording_paths_rs)} for recording {recording_jsons[i][\"Name\"]}')\n",
    "  frames[recording_jsons[i]['Name']] = load_recording_rs(recording_path_rs, recording_jsons[i]['Frames'])\n",
    "  frames.append(load_recording_rs(recording_path_rs, recording_jsons[i]['Frames']))\n",
    "\n",
    "print(f\"Depth Range: {frames[0][0][:,:,3:].min()} - {np.ceil(frames[0][0][:,:,3:].max())}\")\n",
    "print(f\"Color Range: {frames[0][0][:,:,:3].min()} - {frames[0][0][:,:,:3].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.namedWindow('Samples', cv2.WINDOW_AUTOSIZE)\n",
    "\n",
    "sample_size = 300\n",
    "\n",
    "for vid in frames:\n",
    "  counter = 0\n",
    "  for frame in vid:\n",
    "    counter += 1\n",
    "    if counter > sample_size:\n",
    "      cv2.imshow('Samples', images)\n",
    "      break\n",
    "    \n",
    "    color_image = frame[:,:,:3]\n",
    "    depth_image = frame[:,:,3:]\n",
    "    depth_image.shape = depth_image.shape[:2]\n",
    "    \n",
    "    depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(depth_image, alpha=255/8), cv2.COLORMAP_INFERNO) / 255\n",
    "    \n",
    "    depth_colormap_dim = depth_colormap.shape\n",
    "    color_colormap_dim = color_image.shape\n",
    "    \n",
    "    images = np.hstack((color_image, depth_colormap))\n",
    "\n",
    "    cv2.imshow('Samples', images)\n",
    "    cv2.waitKey(1)\n",
    "        \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Skeleton data\n",
    "\n",
    "Next we load the human pose estimation data. The human pose estimation data is stored in a json."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should I load the skeleton data as an image with a heatmap based on the confidence rating?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skeletonNui_jsons = {}\n",
    "skeletonOPo_jsons = {}\n",
    "\n",
    "for recording in recording_jsons:\n",
    "  if \"Skeleton\" in recording:\n",
    "    skeleton_path = recording['Skeleton']\n",
    "    with open(file=os.path.join(RECORDING_DIR, skeleton_path), mode='r') as file:\n",
    "      if (skeleton_path.endswith('OPSkeleton.json')):\n",
    "        skeletonOPo_jsons[recording['Name']] = json.load(file)\n",
    "      elif (skeleton_path.endswith('NuiSkeleton.json')):\n",
    "        skeletonNui_jsons[recording['Name']] = json.load(file)\n",
    "\n",
    "len(recording_jsons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_joint_index_op = 0\n",
    "origin_joint_index_nui = 0\n",
    "\n",
    "def load_skeletons(skeleton_jsons, origin_joint_index):\n",
    "  skeleton_frames = []\n",
    "  for skeleton in skeleton_jsons:\n",
    "    skeleton_frames.append(skeleton['Skeletons'])\n",
    "  return skeletons\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "223e9e27cbc670f5c22c33ebc1577b54f1cca48f7f6b973b1c50992e7fee7cdc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
