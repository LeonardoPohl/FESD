{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FESDModel\n",
    "\n",
    "FESD - Fault estimation for skeleton detection - is a suite that aims at finding faults in joints of skeletons, which are detected by human pose estimatiors.\n",
    "\n",
    "FESDData is the sister project to this notebook, which aims at recording depth and rgb data, as well as populating the data with human poses from variing human pose estimators.\n",
    "\n",
    "Furthermore, FESTData augments all data based on joint confidence.\n",
    "\n",
    "FFESDModel aims to develop and evaluate a model based on the faulty and augmented joint data as well as RGBD data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries\n",
    "\n",
    "We need a range of libraries which are imported here. We also define some constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "h:\\FESD\\FESDModel\\env\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num cuda GPUs: 1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from time import time\n",
    "\n",
    "from data import FESDDataset\n",
    "from data import Frame, AugmentationParams\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "import cv2\n",
    "\n",
    "from model import FESD, train, val, test\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "\n",
    "from utils import AvgMeter, clip_gradient, get_scheduler\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(f\"Num cuda GPUs: {num_gpus}\")\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files/Graphviz/bin/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RECORDING_DIR = Path('H:/Recordings/')\n",
    "CHECKPOINT_DIR = Path('checkpoints')\n",
    "\n",
    "# Writer will output to ./runs/ directory by default\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "Firstly we need to import all the recordings into the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(file=\"Exercises.json\", mode='r') as file:\n",
    "  exercises_json = json.load(file)['Exercises']\n",
    "\n",
    "with open(file=\"JointErrors.json\", mode='r') as file:\n",
    "  joint_error_json = json.load(file)\n",
    "\n",
    "with open(file=\"SkeletonErrors.json\", mode='r') as file:\n",
    "  skeleton_error_json = json.load(file)\n",
    "\n",
    "len(exercises_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recordings Found: 21\n",
      "Total Frames: 5040\n",
      "Recordings Found: 21\n",
      "Total Frames: 1260\n",
      "Recordings Found: 4\n",
      "Total Frames: 1200\n",
      "158\n",
      "40\n",
      "1200\n"
     ]
    }
   ],
   "source": [
    "batchsize = 32\n",
    "train_size = 300\n",
    "train_eval_split = 0.8\n",
    "\n",
    "dataset_train = FESDDataset(RECORDING_DIR, train_size, test=False, val=False, train_eval_split=train_eval_split)\n",
    "dataset_eval = FESDDataset(RECORDING_DIR, train_size, test=False, val=True, train_eval_split=train_eval_split)\n",
    "dataset_test = FESDDataset(RECORDING_DIR, train_size, test=True)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset_train, batch_size=batchsize)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset_eval, batch_size=batchsize)\n",
    "test_loader = torch.utils.data.DataLoader(dataset_test)\n",
    "\n",
    "# werbung-gravuren\n",
    "print(len(train_loader))\n",
    "print(len(validation_loader))\n",
    "print(len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 300, 300]) torch.Size([1, 300, 300]) torch.Size([3, 25]) torch.Size([100])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,\n",
       "         0., 2., 1., 0., 0., 2., 1.]),\n",
       " tensor([0.4754, 0.4754, 0.4754, 0.4754, 0.4754, 0.4754, 0.4754, 0.4754, 0.4754,\n",
       "         0.4754, 0.4754, 0.4754, 0.4754, 0.4754, 0.4754, 0.4754, 0.4754, 0.4754,\n",
       "         0.4754, 0.4754, 0.4754, 0.4754, 0.4754, 0.4754, 0.4754]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rgb, depth, pose_2d, gt = dataset_train[0]\n",
    "dataset_train.frame.show()\n",
    "\n",
    "print(rgb.shape, depth.shape, pose_2d.shape, gt.shape)\n",
    "dataset_train.gt2err(gt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FESD(\n",
      "  (rgb_conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (rgb_pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (rgb_conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (rgb_pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (rgb_conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (rgb_pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (depth_conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (depth_pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (depth_conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (depth_pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (depth_conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (depth_pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (joint_conv1): Conv1d(3, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (joint_conv2): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (joint_conv3): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (fc1): Linear(in_features=86144, out_features=1024, bias=True)\n",
      "  (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (fc3): Linear(in_features=512, out_features=100, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = FESD()\n",
    "print(model)\n",
    "\n",
    "model = nn.DataParallel(model).cuda()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "\n",
    "In the following we define the training function and train a network on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch number\n",
    "epochs = 100\n",
    "# optimizer\n",
    "optim = 'adam'\n",
    "# learning rate\n",
    "learning_rate = 0.000025\n",
    "# learning rate scheduler. can be step, poly or cosine\n",
    "lr_scheduler = 'cosine'\n",
    "# warmup epoch\n",
    "warmup_epoch = -1\n",
    "# warmup multiplier\n",
    "warmup_multiplier = 100\n",
    "# for step scheduler. where to decay lr, can be a list\n",
    "lr_decay_epochs = [120, 160, 200]\n",
    "# for step scheduler. step size to decay lr\n",
    "lr_decay_steps = 20 \n",
    "# for step scheduler. decay rate for learning rate\n",
    "lr_decay_rate = 0.01\n",
    "# weight decay\n",
    "weight_decay = 0.0001\n",
    "# momentum for SGD\n",
    "momentum = 0.9\n",
    "# gradient clipping margin\n",
    "clip = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data = len(train_loader.dataset)\n",
    "CE = torch.nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "if optim == 'adam':\n",
    "    optimizer = torch.optim.Adam(model.parameters(), learning_rate, weight_decay=weight_decay)\n",
    "elif optim == 'adamW':\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), learning_rate, weight_decay=weight_decay)\n",
    "elif optim == 'sdg':\n",
    "    optimizer = torch.optim.SGD(model.parameters(), learning_rate / 10.0 * batchsize, momentum=momentum, weight_decay=weight_decay)\n",
    "\n",
    "scheduler = get_scheduler(optimizer, len(train_loader), lr_scheduler, lr_decay_epochs, lr_decay_steps, lr_decay_rate, epochs, warmup_epoch, warmup_multiplier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.5224e-01,  1.4850e-02,  2.6302e-01,  ..., -1.0463e-01,\n",
      "         -4.9282e-01,  4.0234e-02],\n",
      "        [-4.4138e-02,  1.0564e-01,  5.1399e-01,  ...,  2.7587e-02,\n",
      "         -9.9033e-02,  6.9375e-02],\n",
      "        [ 1.3489e-01,  4.4625e-02,  6.1241e-02,  ..., -2.7932e-02,\n",
      "         -4.1445e-01, -7.3512e-01],\n",
      "        ...,\n",
      "        [-5.3664e-02, -1.9742e-01,  5.5888e-01,  ..., -6.5075e-04,\n",
      "         -1.5839e-01, -1.0416e-01],\n",
      "        [ 5.8724e-01,  1.0727e-02,  9.0161e-02,  ...,  3.2002e-01,\n",
      "         -8.5460e-01,  1.0621e-01],\n",
      "        [ 1.3263e-01, -1.1415e-01,  4.5633e-02,  ..., -2.1619e-02,\n",
      "         -3.3400e-01, -2.9709e-01]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[0., 1., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 1., 0., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 1., 0., 0.]], device='cuda:0')\n",
      "Epoch [001/100],                   Step [0100/0158],                   Loss: 91.3749,                   MSE: 3.5700,                   RMSE: 1.8632\n",
      "tensor([[-0.6826,  1.8191, -1.8971,  ...,  1.8994, -0.6923, -1.2992],\n",
      "        [-1.1484,  1.6276, -0.7379,  ...,  2.6746, -1.0399, -1.9536],\n",
      "        [-2.1380,  1.8612, -0.2263,  ...,  2.1126, -0.7203, -2.6620],\n",
      "        ...,\n",
      "        [-0.9983,  1.7622, -0.1596,  ...,  1.5563, -1.4077, -1.1086],\n",
      "        [-0.2633,  2.2086, -0.8531,  ...,  1.2859, -0.8222, -1.4134],\n",
      "        [-0.6141,  0.9927, -1.3578,  ...,  1.2555, -1.0524, -1.2497]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[0., 1., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 1., 0., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 1., 0., 0.]], device='cuda:0')\n",
      "Epoch [001/100],                   Step [0158/0158],                   Loss: 88.6443,                   MSE: 5.0798,                   RMSE: 2.2461\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 352.00 MiB (GPU 0; 8.00 GiB total capacity; 4.94 GiB already allocated; 683.00 MiB free; 5.40 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m train(train_loader, model, optimizer, CE, scheduler, clip, epoch, epochs, writer)\n\u001b[0;32m      9\u001b[0m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mempty_cache()\n\u001b[1;32m---> 10\u001b[0m val(validation_loader, model, CE, epoch, epochs, writer)\n\u001b[0;32m     11\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mepoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m, total time \u001b[39m\u001b[39m{\u001b[39;00mtime()\u001b[39m \u001b[39m\u001b[39m-\u001b[39m\u001b[39m \u001b[39mtic\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, learning_rate \u001b[39m\u001b[39m{\u001b[39;00moptimizer\u001b[39m.\u001b[39mparam_groups[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[39mif\u001b[39;00m (epoch) \u001b[39m%\u001b[39m \u001b[39m10\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mh:\\FESD\\FESDModel\\model\\eval.py:22\u001b[0m, in \u001b[0;36mval\u001b[1;34m(eval_loader, model, criterion, epoch, epochs, writer)\u001b[0m\n\u001b[0;32m     18\u001b[0m poses \u001b[39m=\u001b[39m poses_2d\u001b[39m.\u001b[39mcuda()\n\u001b[0;32m     20\u001b[0m gt \u001b[39m=\u001b[39m errors\u001b[39m.\u001b[39mcuda()\n\u001b[1;32m---> 22\u001b[0m pred_s \u001b[39m=\u001b[39m model(rgbs, depths, poses)\n\u001b[0;32m     24\u001b[0m \u001b[39m# TODO Calculate different loss based on the error label\u001b[39;00m\n\u001b[0;32m     25\u001b[0m loss \u001b[39m=\u001b[39m criterion(pred_s, gt)\n",
      "File \u001b[1;32mh:\\FESD\\FESDModel\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mh:\\FESD\\FESDModel\\env\\lib\\site-packages\\torch\\nn\\parallel\\data_parallel.py:169\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[1;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    166\u001b[0m     kwargs \u001b[39m=\u001b[39m ({},)\n\u001b[0;32m    168\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice_ids) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 169\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule(\u001b[39m*\u001b[39minputs[\u001b[39m0\u001b[39m], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs[\u001b[39m0\u001b[39m])\n\u001b[0;32m    170\u001b[0m replicas \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreplicate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice_ids[:\u001b[39mlen\u001b[39m(inputs)])\n\u001b[0;32m    171\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparallel_apply(replicas, inputs, kwargs)\n",
      "File \u001b[1;32mh:\\FESD\\FESDModel\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mh:\\FESD\\FESDModel\\model\\model.py:52\u001b[0m, in \u001b[0;36mFESD.forward\u001b[1;34m(self, rgb_image, depth_image, joint_data)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39m# Depth Input convolution with pooling\u001b[39;00m\n\u001b[0;32m     51\u001b[0m y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdepth_conv1(depth_image)\n\u001b[1;32m---> 52\u001b[0m y \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mfunctional\u001b[39m.\u001b[39;49mrelu(y)\n\u001b[0;32m     53\u001b[0m y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdepth_pool1(y)\n\u001b[0;32m     54\u001b[0m y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdepth_conv2(y)\n",
      "File \u001b[1;32mh:\\FESD\\FESDModel\\env\\lib\\site-packages\\torch\\nn\\functional.py:1457\u001b[0m, in \u001b[0;36mrelu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   1455\u001b[0m     result \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrelu_(\u001b[39minput\u001b[39m)\n\u001b[0;32m   1456\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1457\u001b[0m     result \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mrelu(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m   1458\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 352.00 MiB (GPU 0; 8.00 GiB total capacity; 4.94 GiB already allocated; 683.00 MiB free; 5.40 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# routine\n",
    "for epoch in range(1, epochs + 1):\n",
    "    \n",
    "    tic = time()\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    train(train_loader, model, optimizer, CE, scheduler, clip, epoch, epochs, writer)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    val(validation_loader, model, CE, epoch, epochs, writer)\n",
    "    print(f'epoch {epoch}, total time {time() - tic:.2f}, learning_rate {optimizer.param_groups[0][\"lr\"]}')\n",
    "\n",
    "    if (epoch) % 10 == 0:\n",
    "        torch.save(model.state_dict(), os.path.join(CHECKPOINT_DIR, f\"{epoch}_ckpt.pth\"))\n",
    "\n",
    "        print(\"checkpoint saved {}!\".format(os.path.join(CHECKPOINT_DIR, f\"{epoch}_ckpt.pth\")))\n",
    "        \n",
    "torch.save(model.state_dict(), os.path.join(CHECKPOINT_DIR, f\"last_ckpt.pth\"))\n",
    "print(f\"model saved {os.path.join(CHECKPOINT_DIR, f'last_ckpt.pth')}!\")\n",
    "checkpoint = os.path.join(CHECKPOINT_DIR, f\"last_ckpt.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.1502, -8.1108, -8.4876, -7.5053, -8.1098, -8.7455, -8.4399, -8.8540,\n",
       "         -8.3526, -2.0714,  2.2664, -9.5404, -9.1294, -8.4943, -8.1654, -8.7219,\n",
       "          2.3526, -7.6469, -6.7728,  2.6668,  2.2479, -7.7610, -7.5209,  2.8238,\n",
       "          2.2679]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rgb, depth, pose_2d, errors = dataset[0]\n",
    "model(rgb, depth, pose_2d)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "223e9e27cbc670f5c22c33ebc1577b54f1cca48f7f6b973b1c50992e7fee7cdc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
