{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from time import time\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "import torch\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from utils import gt2err, err2gt\n",
    "\n",
    "from utils.mode import Mode\n",
    "from data import FESDDataset\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RECORDING_DIR = Path('D:/Recordings/')\n",
    "CHECKPOINT_DIR = Path('checkpoints')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "Firstly we need to import all the recordings into the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file=\"Exercises.json\", mode='r') as file:\n",
    "  exercises_json = json.load(file)['Exercises']\n",
    "\n",
    "with open(file=\"JointErrors.json\", mode='r') as file:\n",
    "  joint_error_json = json.load(file)\n",
    "\n",
    "with open(file=\"SkeletonErrors.json\", mode='r') as file:\n",
    "  skeleton_error_json = json.load(file)\n",
    "\n",
    "len(exercises_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize = 10\n",
    "im_size = 450\n",
    "use_v2 = False\n",
    "\n",
    "test_exercises = ['E-1.01']#, 'E-1.01', 'E-2.01', 'E-3.01']\n",
    "\n",
    "dataset_train = FESDDataset(RECORDING_DIR, im_size, test_exercises, test=False, randomize_augmentation_params=True)\n",
    "dataset_train.randomize_augmentation_params = True\n",
    "\n",
    "dataset_test = FESDDataset(RECORDING_DIR, im_size, test_exercises, test=True)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset_train, batch_size=batchsize)\n",
    "test_loader = torch.utils.data.DataLoader(dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_names_all = [\"-\", \"Head\", \"Neck\", \"Torso\", \"Waist\", \"Left collar\", \"Left shoulder\", \"Left elbow\", \"Left wrist\", \"Left hand\", \"-\", \"Right collar\", \"Right shoulder\", \"Right elbow\", \"Right wrist\", \"Right hand\", \"-\", \"Left hip\", \"Left knee\", \"Left ankle\", \"-\", \"Right hip\", \"Right knee\", \"Right ankle\", \"-\"]\n",
    "joint_names = [i for i in joint_names_all if i != '-']\n",
    "\n",
    "body_halves = np.array([\"Upper Half\", \"Lower Half\"])\n",
    "body_parts = np.array([\"Head\", \"Torso\", \"Left arm\", \"Right arm\", \"Left leg\", \"Right leg\"])\n",
    "\n",
    "upper_body_i = [0, 1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
    "lower_body_i = [3, 14, 15, 16, 17, 18, 19]\n",
    "\n",
    "torso_i     = [2, 3, 4, 9]\n",
    "head_i      = [0, 1]\n",
    "left_arm_i  = [5, 6, 7, 8]\n",
    "right_arm_i = [10, 11, 12, 13]\n",
    "left_leg_i  = [14, 15, 16]\n",
    "right_leg_i = [17, 18, 19]\n",
    "\n",
    "joint_errors = []\n",
    "for je in joint_error_json:\n",
    "  joint_errors.append(je[\"Name\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentations\n",
    "\n",
    "Here we show the different data augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 285\n",
    "mode = Mode.JOINTS\n",
    "dataset_train.mode = mode\n",
    "dataset_train.use_v2 = False\n",
    "dataset_train.randomize_augmentation_params = False\n",
    "dataset_train.reset_augmentation_params()\n",
    "dataset_train.augmentation_params.crop_pad = 50\n",
    "dataset_train.to_tensor = True\n",
    "show_im = True\n",
    "save_im = False\n",
    "\n",
    "rgb_im, depth_im, pose_2d, gt, session = dataset_train[285]\n",
    "if show_im:\n",
    "  print(rgb_im.size())\n",
    "  #display(rgb_im)\n",
    "# for pack in dataset_train:\n",
    "#   rgb_im, depth_im, pose_2d, gt, session = pack\n",
    "#   if show_im and rgb_im.size != (im_size, im_size):\n",
    "#     display(rgb_im)\n",
    "\n",
    "dataset_train.use_v2 = use_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = -1\n",
    "mode = Mode.JOINTS\n",
    "dataset_train.mode = mode\n",
    "dataset_train.use_v2 = True\n",
    "dataset_train.randomize_augmentation_params = False\n",
    "dataset_train.reset_augmentation_params()\n",
    "dataset_train.augmentation_params.crop_pad = 50\n",
    "dataset_train.to_tensor = False\n",
    "show_im = True\n",
    "save_im = False\n",
    "\n",
    "merged_image, gt, session = dataset_train[i]\n",
    "if dataset_train.augmentation_params.gaussian != True and dataset_train.augmentation_params.crop_random != True and dataset_train.randomize_augmentation_params != True and save_im:\n",
    "  display(merged_image.split()[0])\n",
    "  merged_image.split()[1].save(\"figures/data/depth.png\")\n",
    "  merged_image.split()[2].save(\"figures/data/joint.png\")\n",
    "  merged_image.save(\"figures/data/merged.png\")\n",
    "if show_im:\n",
    "  display(merged_image.split()[0], merged_image.split()[1], merged_image.split()[2])\n",
    "  display(merged_image)\n",
    "\n",
    "dataset_train.use_v2 = use_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import AugmentationParams\n",
    "\n",
    "mode = Mode.JOINTS\n",
    "dataset_train.mode = mode\n",
    "dataset_train.randomize_augmentation_params = False\n",
    "dataset_train.reset_augmentation_params()\n",
    "dataset_train.augmentation_params = AugmentationParams(crop_random=False, crop_pad=50, gaussian=False)\n",
    "dataset_train.use_v2 = False\n",
    "dataset_train.to_tensor = False\n",
    "show_im = False\n",
    "trivial_ims = [10, 38]\n",
    "easy_ims = [126, 137, 188, 197, 230, 301]\n",
    "medium_ims = [365, 385, 440, 487, 500, 590]\n",
    "hard_ims = [530, 535, 635, 685, 695, 725]\n",
    "\n",
    "for i in trivial_ims + easy_ims + medium_ims + hard_ims:\n",
    "  rgb, depth, pose_im, merged_image, session = dataset_train.get_visual_frames(i)\n",
    "  exercise = session['Session Parameters'][\"Exercise\"]\n",
    "  difficulty = \"trivial\" if i in trivial_ims else \"easy\" if i in easy_ims else \"medium\" if i in medium_ims else \"hard\"\n",
    "  directory = f\"figures/data/samples/{difficulty}/{exercise}_{i}.png\"\n",
    "  pose_data = pose_im.getdata()\n",
    "  rgb_data = rgb.getdata()\n",
    "  new_data = []\n",
    "  \n",
    "  for i in range(len(rgb_data)):\n",
    "    if pose_data[i] == (0, 0, 0):\n",
    "      new_data.append(rgb_data[i])\n",
    "    else:\n",
    "      new_data.append((255, 255, 255))\n",
    "      \n",
    "  rgb.putdata(new_data)\n",
    "\n",
    "  if show_im:\n",
    "    display(rgb)\n",
    "    \n",
    "  rgb.save(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 255\n",
    "mode = Mode.JOINTS\n",
    "dataset_train.mode = mode\n",
    "dataset_train.randomize_augmentation_params = False\n",
    "dataset_train.reset_augmentation_params()\n",
    "dataset_train.augmentation_params.crop_pad = 50\n",
    "dataset_train.use_v2 = True\n",
    "\n",
    "def safe_frames(augmentation_type):\n",
    "  rgb, depth, pose_im, merged_im, session = dataset_train.get_visual_frames(i)\n",
    "  rgb.save(f\"figures/augmentation/rgb_{augmentation_type}.png\")\n",
    "  depth.save(f\"figures/augmentation/depth_{augmentation_type}.png\")\n",
    "  pose_im.save(f\"figures/augmentation/pose_{augmentation_type}.png\")\n",
    "  merged_im.save(f\"figures/augmentation/merged_{augmentation_type}.png\")\n",
    "\n",
    "safe_frames(\"original\")\n",
    "\n",
    "dataset_train.augmentation_params.flip = True\n",
    "safe_frames(\"flip\")\n",
    "\n",
    "dataset_train.augmentation_params.flip = False\n",
    "dataset_train.augmentation_params.gaussian = True\n",
    "safe_frames(\"blur\")\n",
    "\n",
    "dataset_train.augmentation_params.gaussian = False\n",
    "dataset_train.augmentation_params.crop_random = True\n",
    "safe_frames(\"crop\")\n",
    "\n",
    "dataset_train.use_v2 = use_v2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis\n",
    "\n",
    "Now that the data is loaded we can analyse it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['Type', 'Session', 'Difficulty', 'Exercise', 'Frame', 'Joint', 'Error', 'mode']\n",
    "df_data = pd.DataFrame(columns=cols)\n",
    "\n",
    "if os.path.exists(\"all_frames.parquet.gzip\"):\n",
    "  df_data = pd.read_parquet(\"all_frames.parquet.gzip\")\n",
    "else:\n",
    "  for i in tqdm(range(len(dataset_train))):\n",
    "    for m in [Mode.FULL_BODY, Mode.HALF_BODY, Mode.BODY_PARTS, Mode.JOINTS]:\n",
    "      dataset_train.mode = m\n",
    "      rgb, depth, pose_2d, gt, session = dataset_train[i]\n",
    "\n",
    "      gt = gt2err(gt, m)[0]\n",
    "      session, frame_i = dataset_train.get_index(i)\n",
    "      frame = dataset_train.frame\n",
    "      session_name = frame.session[\"Name\"]\n",
    "      exercise = frame.session['Session Parameters']['Exercise']\n",
    "      difficulty = int(exercise[2])\n",
    "\n",
    "      for j, err in enumerate(gt):\n",
    "        row = [\"Train\", session_name, difficulty, exercise, frame_i, j, int(err), m.to_str()]\n",
    "        df_data.loc[len(df_data)] = row\n",
    "\n",
    "  for i in tqdm(range(len(dataset_test))):\n",
    "    for m in [Mode.FULL_BODY, Mode.HALF_BODY, Mode.BODY_PARTS, Mode.JOINTS]:\n",
    "      dataset_test.mode = m\n",
    "      _, _, _, gt, _ = dataset_test[i]\n",
    "      gt = gt2err(gt, m)[0]\n",
    "      session, frame_i = dataset_test.get_index(i)\n",
    "      frame = dataset_test.frame\n",
    "      session_name = frame.session[\"Name\"]\n",
    "      exercise = frame.session['Session Parameters']['Exercise']\n",
    "      difficulty = int(exercise[2])\n",
    "\n",
    "      for j, err in enumerate(gt):\n",
    "        row = [\"Test\", session_name, difficulty, exercise, frame_i, j, int(err), m.to_str()]\n",
    "        df_data.loc[len(df_data)] = row\n",
    "\n",
    "  dataset_train.mode = mode\n",
    "  dataset_test.mode = mode\n",
    "  df_data.to_parquet(\"all_frames.parquet.gzip\")\n",
    "\n",
    "\n",
    "df_data[\"mode\"] = df_data[\"mode\"].apply(lambda mode_str: Mode.from_str(mode_str))\n",
    "df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_joints    = df_data[df_data['mode'] == Mode.JOINTS]\n",
    "df_data_body_parts= df_data[df_data['mode'] == Mode.BODY_PARTS]\n",
    "df_data_half_body = df_data[df_data['mode'] == Mode.HALF_BODY]\n",
    "df_data_full_body = df_data[df_data['mode'] == Mode.FULL_BODY]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Distribution Per Joint\n",
    "\n",
    "Here we investigate the distribution of errors based on Body regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_joints[\"pose_id\"]      = df_data_joints[\"Session\"] + \"_\" + df_data_joints[\"Frame\"].astype(str)\n",
    "df_data_joints[\"error_simple\"] = df_data_joints[\"Error\"].apply(lambda x: 1 if x > 0 else 0)\n",
    "df_data_joints[\"joint_name\"]   = df_data_joints[\"Joint\"].apply(lambda x: joint_names[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dict_halves = Mode.HALF_BODY.get_class_dict()\n",
    "df_data_joints[\"Upper Body\"] = df_data_joints[\"Joint\"].apply(lambda x: x in class_dict_halves[\"Upper Body\"])\n",
    "df_data_joints[\"Lower Body\"] = df_data_joints[\"Joint\"].apply(lambda x: x in class_dict_halves[\"Lower Body\"])\n",
    "\n",
    "class_dict_body_parts = Mode.BODY_PARTS.get_class_dict()\n",
    "df_data_joints[\"Torso\"]     = df_data_joints[\"Joint\"].apply(lambda x: x in class_dict_body_parts[\"Torso\"])\n",
    "df_data_joints[\"Head\"]      = df_data_joints[\"Joint\"].apply(lambda x: x in class_dict_body_parts[\"Head\"])\n",
    "df_data_joints[\"Left Arm\"]  = df_data_joints[\"Joint\"].apply(lambda x: x in class_dict_body_parts[\"Left Arm\"])\n",
    "df_data_joints[\"Right Arm\"] = df_data_joints[\"Joint\"].apply(lambda x: x in class_dict_body_parts[\"Right Arm\"])\n",
    "df_data_joints[\"Left Leg\"]  = df_data_joints[\"Joint\"].apply(lambda x: x in class_dict_body_parts[\"Left Leg\"])\n",
    "df_data_joints[\"Right Leg\"] = df_data_joints[\"Joint\"].apply(lambda x: x in class_dict_body_parts[\"Right Leg\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_joints[\"body_half\"] = df_data_joints[\"Joint\"].apply(lambda x: Mode.HALF_BODY.get_class(x))\n",
    "df_data_joints[\"body_part\"] = df_data_joints[\"Joint\"].apply(lambda x: Mode.BODY_PARTS.get_class(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (6, 3))\n",
    "error_distr = df_data_joints.groupby(\"pose_id\")[\"error_simple\"].sum().reset_index()[\"error_simple\"]\n",
    "\n",
    "error_distr.plot.hist(bins = range(0, 20), density=True, align=\"left\")\n",
    "#error_distr.plot(kind = \"kde\", alpha = 0.7)\n",
    "quant = error_distr.quantile(0.5)\n",
    "ax.axvline(quant, color=\"red\", alpha = .7, ymax = 1, linestyle = \":\")\n",
    "ax.set_title(\"Distribution of number of joints with error per pose\")\n",
    "ax.set_xlabel(\"Number of Joints with Error\")\n",
    "ax.set_ylabel(\"Number of poses\")\n",
    "ax.set_xlim(-0.5, 20)\n",
    "ax.set_xticks([0, 5, 10, 15, 20] + [quant])\n",
    "ax.set_yticks([])\n",
    "plt.savefig(\"figures/Data/joint_errors_per_pose/distribution_of_joint_errors_per_pose.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize = (10,3), nrows=1, ncols=2)\n",
    "\n",
    "err_dist_upper = df_data_joints[df_data_joints[\"body_half\"] == \"Upper Body\"].groupby(\"pose_id\")[\"error_simple\"].sum().reset_index()[\"error_simple\"]\n",
    "err_dist_lower = df_data_joints[df_data_joints[\"body_half\"] == \"Lower Body\"].groupby(\"pose_id\")[\"error_simple\"].sum().reset_index()[\"error_simple\"]\n",
    "\n",
    "err_dist_upper.plot.hist(bins = range(0, 16), density=True, ax = axs[0], align=\"left\")\n",
    "#err_dist_upper.plot(kind = \"kde\", ax = axs[0])\n",
    "\n",
    "err_dist_lower.plot.hist(bins = range(0, 11), density=True, ax = axs[1], align=\"left\")\n",
    "#err_dist_lower.plot(kind = \"kde\", ax = axs[1])\n",
    "\n",
    "fig.suptitle(\"Distribution of number of joints with error per pose per body half\")\n",
    "axs[0].set_title(\"Upper Body\")\n",
    "axs[1].set_title(\"Lower Body\")\n",
    "\n",
    "\n",
    "quant = err_dist_upper.quantile(0.5)\n",
    "axs[0].axvline(quant, color=\"red\", alpha = .7, ymax = 1, linestyle = \":\")\n",
    "axs[0].set_xlim(-0.5, 15)\n",
    "axs[0].set_xticks([0, 5, 10, 15] + [quant])\n",
    "\n",
    "quant = err_dist_lower.quantile(0.5)\n",
    "axs[1].axvline(quant, color=\"red\", alpha = .7, ymax = 1, linestyle = \":\")\n",
    "axs[1].set_xlim(-0.5, 10)\n",
    "axs[1].set_xticks([0, 5, 10] + [quant])\n",
    "\n",
    "\n",
    "for ax in axs:\n",
    "  ax.set_ylabel(\"Number of Errors\")\n",
    "  ax.set_xlabel(\"Number of Joints\")\n",
    "  ax.set_yticks([])\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figures/Data/joint_errors_per_pose/distribution_of_joint_errors_per_pose_per_body_half.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize = (6,6), nrows=3, ncols=2)\n",
    "\n",
    "def plot_error_distribution(df, body_part, thresh_dict, ax):\n",
    "  err_dist = df[df[\"body_part\"] == body_part].groupby(\"pose_id\")[\"error_simple\"].sum().reset_index()[\"error_simple\"]\n",
    "\n",
    "  err_dist.plot.hist(bins = range(0, 11), density=True, ax = ax, align='left')\n",
    "  #err_dist.plot(kind = \"kde\", ax = ax)\n",
    "\n",
    "  ax.set_title(body_part)\n",
    "\n",
    "  quant = err_dist.quantile(0.5)\n",
    "  thresh_dict[body_part] = quant\n",
    "  ax.axvline(quant, color=\"red\", alpha = .7, ymax = 1, linestyle = \":\")\n",
    "\n",
    "  ax.set_xlim(-0.5, 5)\n",
    "  ax.set_xticks([0, 1, 2, 3, 4, 5] + [quant])\n",
    "  ax.set_yticks([])\n",
    "\n",
    "\n",
    "thresh_dict = {}\n",
    "\n",
    "fig.suptitle(\"Distribution of number of joints with error per pose per body_part\")\n",
    "\n",
    "plot_error_distribution(df_data_joints, \"Torso\",      thresh_dict, axs[0,0])\n",
    "plot_error_distribution(df_data_joints, \"Head\",       thresh_dict, axs[0,1])\n",
    "plot_error_distribution(df_data_joints, \"Left Arm\",   thresh_dict, axs[1,0])\n",
    "plot_error_distribution(df_data_joints, \"Right Arm\",  thresh_dict, axs[1,1])\n",
    "plot_error_distribution(df_data_joints, \"Left Leg\",   thresh_dict, axs[2,0])\n",
    "plot_error_distribution(df_data_joints, \"Right Leg\",  thresh_dict, axs[2,1])\n",
    "\n",
    "for ax in axs:\n",
    "  ax[0].set_ylabel(\"Number of Errors\")\n",
    "  ax[1].set_ylabel(\"\")\n",
    "\n",
    "axs[2,0].set_xlabel(\"Number of Joints\")\n",
    "axs[2,1].set_xlabel(\"Number of Joints\")\n",
    "  \n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figures/Data/joint_errors_per_pose/distribution_of_joint_errors_per_pose_per_body_part.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(thresh_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Distribution\n",
    "\n",
    "Here we investigate the distribution of errors over the dataset for different body regions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Joints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_joints[\"Joint Name\"] = df_data_joints[\"Joint\"].apply(lambda x: joint_names[x])\n",
    "df_data_joints[\"Simple Error\"] = df_data_joints[\"Error\"] != 0\n",
    "df_data_joints[\"Difficulty Name\"] = df_data_joints[\"Difficulty\"].apply(lambda x: \"Trivial\" if x == 0 else \"Easy\" if x == 1 else \"Medium\" if x == 2 else \"Hard\")\n",
    "df_data_joints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_by_difficulty = df_data_joints[['Difficulty', \"Difficulty Name\", \"Simple Error\"]].groupby([\"Difficulty Name\"]).mean().sort_values(by=\"Difficulty\")[\"Simple Error\"]\n",
    "ax = (error_by_difficulty).plot.bar()\n",
    "ax.set_ylabel(\"Error Rate\")\n",
    "ax.set_xlabel(\"Difficulty\")\n",
    "ax.set_title(\"Error Rate by Difficulty\")\n",
    "ax.set_ylim(0, 1)\n",
    "labels = ['Trivial', 'Easy', 'Medium', 'Hard']\n",
    "ax.set_xticks([0, 1, 2, 3])\n",
    "ax.set_xticklabels(labels, rotation=0, ha='center')\n",
    "plt.savefig(\"figures/Data/dist_joints/Error_Rate_by_Difficulty.png\", dpi=300, bbox_inches='tight')\n",
    "ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_by_difficulty = df_data_joints[['Difficulty', \"Difficulty Name\", \"Simple Error\"]].groupby([\"Difficulty Name\"]).mean().sort_values(by=\"Difficulty\")[\"Simple Error\"]\n",
    "\n",
    "ax = error_by_difficulty.plot.bar()\n",
    "ax.set_ylabel(\"Error Rate\")\n",
    "ax.set_xlabel(\"Difficulty\")\n",
    "ax.set_title(\"Error Rate by Difficulty\")\n",
    "ax.set_ylim(0, 1)\n",
    "labels = ['Trivial', 'Easy', 'Medium', 'Hard']\n",
    "ax.set_xticks([0, 1, 2, 3])\n",
    "ax.set_xticklabels(labels, rotation=0, ha='center')\n",
    "plt.savefig(\"figures/Data/dist_joints/Error_Rate_by_Difficulty.png\", dpi=300, bbox_inches='tight')\n",
    "ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = df_data_joints[[\"Joint\", \"Error\"]].sum()\n",
    "\n",
    "err_joints = df_data_joints[[\"Joint\", \"Error\"]]\n",
    "error_distribution_by_joint = err_joints.groupby([\"Error\"]).count().unstack().T\n",
    "\n",
    "error_distribution_by_joint = (error_distribution_by_joint / error_distribution_by_joint.sum()).T.fillna(0)\n",
    "\n",
    "edj = error_distribution_by_joint.T\n",
    "\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "ax1 = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "axs = edj.plot.pie(ax=ax1, subplots=True, layout=(1, 1), legend=False, title=\"Error Distribution\", fontsize=24, sharex=False, sharey=False, labeldistance=None, explode=(.05,.1,.1,.1), autopct='%1.1f%%', pctdistance=1.15)\n",
    "\n",
    "for ax in axs.flatten():\n",
    "  ax.set_xlabel(ax.get_ylabel())\n",
    "  ax.set_ylabel(\"\")\n",
    "\n",
    "plt.legend(joint_errors, loc='upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "plt.savefig(\"figures/Data/dist_joints/Error_Distribution.png\", dpi=300, bbox_inches='tight')\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_group = df_data_joints[['Difficulty', \"Difficulty Name\", \"Error\"]].groupby([\"Difficulty Name\", \"Error\"]).count()[\"Difficulty\"].unstack().T\n",
    "error_distribution_by_difficulty = (err_group / err_group.sum()).T\n",
    "error_distribution_by_difficulty['sort_key'] = pd.CategoricalIndex(error_distribution_by_difficulty, [3, 0, 2, 1])\n",
    "error_distribution_by_difficulty.sort_values('sort_key', inplace=True)\n",
    "error_distribution_by_difficulty.drop('sort_key', axis=1, inplace=True)\n",
    "\n",
    "axs = error_distribution_by_difficulty.T.plot.pie(subplots=True, figsize=(10, 10), layout=(2, 2), legend=False, title=\"Error Distribution by Difficulty\", labels=joint_errors, autopct='%1.1f%%', pctdistance=1.25, fontsize=12, explode=(.1,.1,.1,.1), sharex=False, sharey=False, labeldistance=None)\n",
    "\n",
    "axs[0, 0].set_xlabel(\"Trivial\")\n",
    "axs[0, 0].set_ylabel(\"\")\n",
    "axs[0, 1].set_xlabel(\"Easy\")\n",
    "axs[0, 1].set_ylabel(\"\")\n",
    "axs[0, 1].legend(bbox_to_anchor=(1, 1.02), loc='center left')\n",
    "axs[1, 0].set_xlabel(\"Medium\")\n",
    "axs[1, 0].set_ylabel(\"\")\n",
    "axs[1, 1].set_xlabel(\"Hard\")\n",
    "axs[1, 1].set_ylabel(\"\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figures/Data/dist_joints/Error_Distribution_by_Difficulty.png\", dpi=300, bbox_inches='tight')\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = df_data_joints[[\"Joint Name\", \"Error\"]].groupby([\"Joint Name\"]).sum()\n",
    "s[\"Sorted Names\"] = joint_names\n",
    "s = s.sort_values(by=\"Error\", ascending=False)\n",
    "\n",
    "err_joints = df_data_joints[[\"Joint\", \"Joint Name\", \"Error\"]]\n",
    "error_distribution_by_joint = err_joints.groupby([\"Joint Name\", \"Error\"]).count().unstack().reindex(s.index).T\n",
    "780 - error_distribution_by_joint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = df_data_joints[[\"Joint\", \"Simple Error\"]].groupby([\"Joint\"]).sum()\n",
    "s[\"Sorted Names\"] = joint_names\n",
    "s = s.sort_values(by=\"Simple Error\", ascending=False)\n",
    "\n",
    "err_joints = df_data_joints[[\"Joint\", \"Joint Name\", \"Error\"]]\n",
    "error_distribution_by_joint = err_joints.groupby([\"Joint\", \"Error\"]).count().unstack().reindex(s.index).T\n",
    "\n",
    "error_distribution_by_joint = (error_distribution_by_joint / error_distribution_by_joint.sum()).T.fillna(0)\n",
    "error_distribution_by_joint.columns = error_distribution_by_joint.columns.droplevel()\n",
    "\n",
    "error_distribution_by_joint.index = s[\"Sorted Names\"].tolist()\n",
    "\n",
    "edj = error_distribution_by_joint.T\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax1 = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "axs = edj.plot.pie(ax=ax1, subplots=True, figsize=(10, 10), layout=(4, 5), legend=False, title=\"Error Distribution by Joint\", fontsize=12, sharex=False, sharey=False, labeldistance=None, explode=(.05,.05,.05,.1))\n",
    "\n",
    "for ax in axs.flatten():\n",
    "  ax.set_xlabel(ax.get_ylabel())\n",
    "  ax.set_ylabel(\"\")\n",
    "\n",
    "plt.legend(joint_errors, loc='upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "ax1.set_title(\"Error Distribution by Joint\")\n",
    "plt.savefig(\"figures/Data/dist_joints/Error_Distribution_by_Joint.png\", dpi=300, bbox_inches='tight')\n",
    "plt.plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full Body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_full_body[\"Joint Name\"] = df_data_full_body[\"Joint\"].apply(lambda x: joint_names[x])\n",
    "df_data_full_body[\"Difficulty Name\"] = df_data_full_body[\"Difficulty\"].apply(lambda x: \"Trivial\" if x == 0 else \"Easy\" if x == 1 else \"Medium\" if x == 2 else \"Hard\")\n",
    "df_data_full_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = df_data_full_body[[\"Joint\", \"Error\"]].sum()\n",
    "\n",
    "err_joints = df_data_full_body[[\"Joint\", \"Error\"]]\n",
    "error_distribution_by_joint = err_joints.groupby([\"Error\"]).count().unstack().T\n",
    "error_distribution_by_joint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = df_data_full_body[[\"Joint\", \"Error\"]].sum()\n",
    "\n",
    "err_joints = df_data_full_body[[\"Joint\", \"Error\"]]\n",
    "error_distribution_by_joint = err_joints.groupby([\"Error\"]).count().unstack().T\n",
    "\n",
    "error_distribution_by_joint = (error_distribution_by_joint / error_distribution_by_joint.sum()).T.fillna(0)\n",
    "\n",
    "edj = error_distribution_by_joint.T\n",
    "\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "ax1 = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "axs = edj.plot.pie(ax=ax1, subplots=True, layout=(1, 1), legend=False, title=\"Error Distribution\", fontsize=12, sharex=False, sharey=False, labeldistance=None, explode=(.05,.1), autopct='%1.1f%%', pctdistance=1.15)\n",
    "\n",
    "for ax in axs.flatten():\n",
    "  ax.set_xlabel(ax.get_ylabel())\n",
    "  ax.set_ylabel(\"\")\n",
    "\n",
    "plt.legend([\"No Error\", \"Error\"], loc='upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "# plt.savefig(\"figures/Data/dist_full_body/Error_Distribution.png\", dpi=300, bbox_inches='tight')\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_by_difficulty = df_data_full_body[['Difficulty', \"Difficulty Name\", \"Error\"]].groupby([\"Difficulty Name\"]).mean().sort_values(by=\"Difficulty\")[\"Error\"]\n",
    "ax = error_by_difficulty.plot.bar()\n",
    "ax.set_ylabel(\"Error Rate\")\n",
    "ax.set_xlabel(\"Difficulty\")\n",
    "ax.set_title(\"Error Rate by Difficulty\")\n",
    "ax.set_ylim(0, 1)\n",
    "labels = ['Trivial', 'Easy', 'Medium', 'Hard']\n",
    "ax.set_xticks([0, 1, 2, 3])\n",
    "ax.set_xticklabels(labels, rotation=0, ha='center')\n",
    "plt.savefig(\"figures/Data/dist_full_body/Error_Rate_by_Difficulty.png\", dpi=300, bbox_inches='tight')\n",
    "ax"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Body Halves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_half_body[\"Joint Name\"] = df_data_half_body[\"Joint\"].apply(lambda x: body_halves[x])\n",
    "df_data_half_body[\"Difficulty Name\"] = df_data_half_body[\"Difficulty\"].apply(lambda x: \"Trivial\" if x == 0 else \"Easy\" if x == 1 else \"Medium\" if x == 2 else \"Hard\")\n",
    "#df_data_half_body[\"Error\"] = df_data_half_body[[\"Joint Name\", \"Error\"]].apply(lambda x: x[\"Joint Name\"] if x[\"Error\"] == 1 else \"No Error\", axis=1)\n",
    "df_data_half_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_by_difficulty = df_data_half_body[['Difficulty', \"Difficulty Name\", \"Error\"]].groupby([\"Difficulty Name\"]).mean().sort_values(by=\"Difficulty\")[\"Error\"]\n",
    "ax = error_by_difficulty.plot.bar()\n",
    "ax.set_ylabel(\"Error Rate\")\n",
    "ax.set_xlabel(\"Difficulty\")\n",
    "ax.set_title(\"Error Rate by Difficulty\")\n",
    "ax.set_ylim(0, 1)\n",
    "labels = ['Trivial', 'Easy', 'Medium', 'Hard']\n",
    "ax.set_xticks([0, 1, 2, 3])\n",
    "ax.set_xticklabels(labels, rotation=0, ha='center')\n",
    "plt.savefig(\"figures/Data/dist_half_body/Error_Rate_by_Difficulty.png\", dpi=300, bbox_inches='tight')\n",
    "ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = df_data_half_body[[\"Joint\", \"Error\"]].groupby([\"Joint\"]).sum()\n",
    "s[\"Sorted Names\"] = body_halves\n",
    "s = s.sort_values(by=\"Error\", ascending=False)\n",
    "\n",
    "err_joints = df_data_half_body[[\"Joint\", \"Joint Name\", \"Error\"]]\n",
    "error_distribution_by_joint = err_joints.groupby([\"Joint\", \"Error\"]).count().unstack().reindex(s.index).T\n",
    "error_distribution_by_joint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = df_data_half_body[[\"Joint\", \"Error\"]].groupby([\"Joint\"]).sum()\n",
    "s[\"Sorted Names\"] = body_halves\n",
    "s = s.sort_values(by=\"Error\", ascending=False)\n",
    "\n",
    "err_joints = df_data_half_body[[\"Joint\", \"Joint Name\", \"Error\"]]\n",
    "error_distribution_by_joint = err_joints.groupby([\"Joint\", \"Error\"]).count().unstack().reindex(s.index).T\n",
    "\n",
    "error_distribution_by_joint = (error_distribution_by_joint / error_distribution_by_joint.sum()).T.fillna(0)\n",
    "error_distribution_by_joint.columns = error_distribution_by_joint.columns.droplevel()\n",
    "\n",
    "error_distribution_by_joint.index = s[\"Sorted Names\"].tolist()\n",
    "\n",
    "edj = error_distribution_by_joint.T\n",
    "\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "ax1 = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "axs = edj.plot.pie(ax=ax1, subplots=True, layout=(1, 2), legend=False, title=\"Error Distribution by Body Half\", fontsize=12, sharex=False, sharey=False, labeldistance=None, explode=(.05,.1), autopct='%1.1f%%', pctdistance=1.25)\n",
    "\n",
    "for ax in axs.flatten():\n",
    "  ax.set_xlabel(ax.get_ylabel())\n",
    "  ax.set_ylabel(\"\")\n",
    "\n",
    "plt.legend([\"No Error\", \"Error\"], loc='upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "ax1.set_title(\"Error Distribution by Body Half\")\n",
    "plt.savefig(\"figures/Data/dist_half_body/Error_Distribution_by_Body_Half.png\", dpi=300, bbox_inches='tight')\n",
    "plt.plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Body parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_body_parts[\"Joint Name\"] = df_data_body_parts[\"Joint\"].apply(lambda x: body_parts[x])\n",
    "df_data_body_parts[\"Difficulty Name\"] = df_data_body_parts[\"Difficulty\"].apply(lambda x: \"Trivial\" if x == 0 else \"Easy\" if x == 1 else \"Medium\" if x == 2 else \"Hard\")\n",
    "df_data_body_parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_by_difficulty = df_data_body_parts[['Difficulty', \"Difficulty Name\", \"Error\"]].groupby([\"Difficulty Name\"]).mean().sort_values(by=\"Difficulty\")[\"Error\"]\n",
    "ax = error_by_difficulty.plot.bar()\n",
    "ax.set_ylabel(\"Error Rate\")\n",
    "ax.set_xlabel(\"Difficulty\")\n",
    "ax.set_title(\"Error Rate by Difficulty\")\n",
    "ax.set_ylim(0, 1)\n",
    "labels = ['Trivial', 'Easy', 'Medium', 'Hard']\n",
    "ax.set_xticks([0, 1, 2, 3])\n",
    "ax.set_xticklabels(labels, rotation=0, ha='center')\n",
    "plt.savefig(\"figures/Data/dist_body_parts/Error_Rate_by_Difficulty.png\", dpi=300, bbox_inches='tight')\n",
    "ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = df_data_body_parts[[\"Joint Name\", \"Error\"]].groupby([\"Joint Name\"]).sum()\n",
    "s[\"Sorted Names\"] = body_parts\n",
    "s = s.sort_values(by=\"Error\", ascending=False)\n",
    "\n",
    "err_joints = df_data_body_parts[[\"Joint\", \"Joint Name\", \"Error\"]]\n",
    "error_distribution_by_joint = err_joints.groupby([\"Joint Name\", \"Error\"]).count().unstack().reindex(s.index).T\n",
    "error_distribution_by_joint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = df_data_body_parts[[\"Joint\", \"Error\"]].groupby([\"Joint\"]).sum()\n",
    "s[\"Sorted Names\"] = body_parts\n",
    "s = s.sort_values(by=\"Error\", ascending=False)\n",
    "\n",
    "err_joints = df_data_body_parts[[\"Joint\", \"Joint Name\", \"Error\"]]\n",
    "error_distribution_by_joint = err_joints.groupby([\"Joint\", \"Error\"]).count().unstack().reindex(s.index).T\n",
    "\n",
    "error_distribution_by_joint = (error_distribution_by_joint / error_distribution_by_joint.sum()).T.fillna(0)\n",
    "error_distribution_by_joint.columns = error_distribution_by_joint.columns.droplevel()\n",
    "\n",
    "error_distribution_by_joint.index = s[\"Sorted Names\"].tolist()\n",
    "\n",
    "edj = error_distribution_by_joint.T\n",
    "\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "ax1 = fig.add_subplot(2, 3, 1)\n",
    "\n",
    "axs = edj.plot.pie(ax=ax1, subplots=True, layout=(2, 3), legend=False, title=\"Error Distribution by Body part\", fontsize=12, sharex=False, sharey=False, labeldistance=None, explode=(.05,.1), autopct='%1.1f%%', pctdistance=1.4)\n",
    "\n",
    "for ax in axs.flatten():\n",
    "  ax.set_xlabel(ax.get_ylabel())\n",
    "  ax.set_ylabel(\"\")\n",
    "\n",
    "plt.legend([\"No Error\", \"Error\"], loc='upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "ax1.set_title(\"Error Distribution by Body part\")\n",
    "plt.savefig(\"figures/Data/dist_body_parts/Error_Distribution_by_Body part.png\", dpi=300, bbox_inches='tight')\n",
    "plt.plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighing the Dataset\n",
    "\n",
    "The dataset is quite unbalanced and it needs to be rebalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_counts = df_data_full_body[df_data_full_body[\"Type\"] == \"Train\"].Error.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_distr = lambda s1, s2: s1 + s2 \n",
    "\n",
    "# use cout as a class\n",
    "uh = df_data_half_body[df_data_half_body[\"Type\"] == \"Train\"][df_data_half_body[\"Joint Name\"] == \"Upper Half\"].reset_index()[\"Error\"]\n",
    "lh = df_data_half_body[df_data_half_body[\"Type\"] == \"Train\"][df_data_half_body[\"Joint Name\"] == \"Lower Half\"].reset_index()[\"Error\"]\n",
    "hb_comb = uh.combine(lh, get_distr)\n",
    "hb_counts = hb_comb.value_counts()\n",
    "hb_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_train = df_data_body_parts[\"Type\"] == \"Train\"\n",
    "\n",
    "joint = df_data_body_parts[\"Joint Name\"] == body_parts[0]  \n",
    "crit = is_train & joint\n",
    "lb_comb = df_data_body_parts[crit].reset_index()[\"Error\"]\n",
    "\n",
    "for body_part in body_parts[1:]:\n",
    "  joint = df_data_body_parts[\"Joint Name\"] == body_part  \n",
    "  crit = is_train & joint\n",
    "  lb_comb = lb_comb.combine(df_data_body_parts[crit].reset_index()[\"Error\"], get_distr)\n",
    "lb_counts = lb_comb.value_counts()\n",
    "lb_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint = df_data_joints[\"Joint Name\"] == joint_names[0]  \n",
    "is_train = df_data_joints[\"Type\"] == \"Train\"\n",
    "crit = is_train & joint\n",
    "jt_comb = df_data_joints[crit].reset_index()[\"error_simple\"]\n",
    "\n",
    "for joint_name in joint_names[1:]:\n",
    "  joint = df_data_joints[\"Joint Name\"] == joint_name  \n",
    "  crit = is_train & joint\n",
    "  jt_comb = jt_comb.combine(df_data_joints[crit].reset_index()[\"error_simple\"], get_distr)\n",
    "\n",
    "jt_counts = jt_comb.value_counts()\n",
    "jt_counts[jt_counts < 10] = 10\n",
    "jt_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {\n",
    "  \"Full Body\":  [1/fb_counts[i] for i in df_data_full_body[df_data_full_body[\"Type\"] == \"Train\"].Error.values],\n",
    "  \"Half Body\":  [1/hb_counts[i] for i in hb_comb.values],\n",
    "  \"Body parts\":      [1/lb_counts[i] for i in lb_comb.values],\n",
    "  \"Joints\":     [1/jt_counts[i] for i in jt_comb.values]\n",
    "}\n",
    "\n",
    "with open(\"weights.json\", 'w') as fp:\n",
    "  json.dump(weights, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import WeightedRandomSampler\n",
    "from utils import gts2errs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def visualise_dataloader(dl, id_to_label=None, with_outputs=True, save_plot=False, plot_name=\"test.png\", mode=Mode.FULL_BODY):\n",
    "    total_num_images = len(dl.dataset)\n",
    "    idxs_seen = []\n",
    "    class_0_batch_counts = []\n",
    "    class_1_batch_counts = []\n",
    "\n",
    "    for i, pack in enumerate(tqdm(dl)):\n",
    "        merged_image, gt, session = pack\n",
    "        for j in range(len(gt[0])):\n",
    "            idxs = gt[:, j].tolist()\n",
    "\n",
    "            class_ids, class_counts = idxs.unique(return_counts=True)\n",
    "            class_ids = set(class_ids.tolist())\n",
    "            class_counts = class_counts.tolist()\n",
    "\n",
    "            if len(class_ids) == 2:\n",
    "                class_0_batch_counts.append(class_counts[0])\n",
    "                class_1_batch_counts.append(class_counts[1])\n",
    "            elif len(class_ids) == 1 and 0 in class_ids:\n",
    "                class_0_batch_counts.append(class_counts[0])\n",
    "                class_1_batch_counts.append(0)\n",
    "            elif len(class_ids) == 1 and 1 in class_ids:\n",
    "                class_0_batch_counts.append(0)\n",
    "                class_1_batch_counts.append(class_counts[0])\n",
    "            else:\n",
    "                raise ValueError(\"More than two classes detected\")\n",
    "        \n",
    "        # print(gt)\n",
    "        # idxs = gt[:, 0].tolist()\n",
    "        # print(idxs)\n",
    "        # break\n",
    "        # classes = gt[:, 1]\n",
    "        # class_ids, class_counts = classes.unique(return_counts=True)\n",
    "        # class_ids = set(class_ids.tolist())\n",
    "        # class_counts = class_counts.tolist()\n",
    "        \n",
    "        idxs_seen.extend(idxs)\n",
    "        \n",
    "\n",
    "    if with_outputs:\n",
    "        fig, ax = plt.subplots(1, figsize=(5, 15))\n",
    "\n",
    "        ind = np.arange(len(class_0_batch_counts))\n",
    "        width = 0.35\n",
    "\n",
    "        ax.bar(\n",
    "            ind,\n",
    "            class_0_batch_counts,\n",
    "            width,\n",
    "            label=(id_to_label[0] if id_to_label is not None else \"0\"),\n",
    "        )\n",
    "        ax.bar(\n",
    "            ind + width,\n",
    "            class_1_batch_counts,\n",
    "            width,\n",
    "            label=(id_to_label[1] if id_to_label is not None else \"1\"),\n",
    "        )\n",
    "        ax.set_xticks(ind, ind + 1)\n",
    "        ax.set_xlabel(\"Batch index\", fontsize=12)\n",
    "        ax.set_ylabel(\"No. of images in batch\", fontsize=12)\n",
    "        ax.set_aspect(\"equal\")\n",
    "\n",
    "        plt.legend()\n",
    "        if save_plot:\n",
    "            plt.savefig(plot_name)\n",
    "        else:\n",
    "            plt.show()\n",
    "\n",
    "        num_images_seen = len(idxs_seen)\n",
    "\n",
    "        print(\n",
    "            f'Avg Proportion of {(id_to_label[0] if id_to_label is not None else \"Class 0\")} per batch: {(np.array(class_0_batch_counts) / 10).mean()}'\n",
    "        )\n",
    "        print(\n",
    "            f'Avg Proportion of {(id_to_label[1] if id_to_label is not None else \"Class 1\")} per batch: {(np.array(class_1_batch_counts) / 10).mean()}'\n",
    "        )\n",
    "        print(\"=============\")\n",
    "        print(f\"Num. unique images seen: {len(set(idxs_seen))}/{total_num_images}\")\n",
    "    return class_0_batch_counts, class_1_batch_counts, idxs_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, pack in enumerate(tqdm(dl)):\n",
    "        merged_image, gt, session = pack\n",
    "        for j in len(gt[0]):\n",
    "            idxs = gt[:, j].tolist()\n",
    "\n",
    "            class_ids, class_counts = idxs.unique(return_counts=True)\n",
    "            class_ids = set(class_ids.tolist())\n",
    "            class_counts = class_counts.tolist()\n",
    "            print(ids, class_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_sets = [(Mode.HALF_BODY, \"Half Body\"), (Mode.FULL_BODY, \"Full Body\"), (Mode.BODY_PARTS, \"Body parts\"), (Mode.JOINTS, \"Joints\")]\n",
    "types = [\"Train\", \"Test\"]\n",
    "use_samplers = [False, True]\n",
    "\n",
    "batchsize = 32\n",
    "\n",
    "for mode, problem_set in tqdm(problem_sets):\n",
    "  for t in types:\n",
    "    for use_sampler in use_samplers:\n",
    "      dataset = FESDDataset(RECORDING_DIR, im_size, test_exercises=test_exercises, mode=mode, randomize_augmentation_params=True, use_v2=True, test=t==\"Test\")\n",
    "\n",
    "      w = weights[problem_set]\n",
    "      if use_sampler:\n",
    "        sampler = WeightedRandomSampler(weights=w, num_samples=dataset.size)\n",
    "        dl  = torch.utils.data.DataLoader(dataset, sampler=sampler, batch_size=batchsize)\n",
    "      else:\n",
    "        dl  = torch.utils.data.DataLoader(dataset, shuffle=True, batch_size=batchsize)\n",
    "\n",
    "      visualise_dataloader(dl, {0: \"No Error\", 1: \"Error\"}, True, True, f\"figures/redistribution/{problem_set.lower().replace(' ', '_')}_{t}{'_sampled' if use_sampler else ''}.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
