{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FESDModel - Model Evaluation\n",
    "\n",
    "FESD - Fault estimation for skeleton detection - is a suite that aims at finding faults in joints of skeletons, which are detected by human pose estimatiors.\n",
    "\n",
    "FESDData is the sister project to this notebook, which aims at recording depth and rgb data, as well as populating the data with human poses from variing human pose estimators.\n",
    "\n",
    "Furthermore, FESTData augments all data based on joint confidence.\n",
    "\n",
    "FFESDModel aims to develop and evaluate a model based on the faulty and augmented joint data as well as RGBD data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries\n",
    "\n",
    "We need a range of libraries which are imported here. We also define some constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from time import time\n",
    "\n",
    "from data import FESDDataset\n",
    "from data import Frame, AugmentationParams\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "import cv2\n",
    "\n",
    "from model import FESD, train, val, test\n",
    "import copy\n",
    "\n",
    "import scipy\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "import datetime\n",
    "\n",
    "import torch\n",
    "\n",
    "from utils.mode import Mode\n",
    "from utils import err2gt, gt2err\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RECORDING_DIR = Path('D:/Recordings/')\n",
    "CHECKPOINT_DIR = Path('checkpoints')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata Loading\n",
    "\n",
    "Firstly we need to import all the recordings into the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file=\"Exercises.json\", mode='r') as file:\n",
    "  exercises_json = json.load(file)['Exercises']\n",
    "\n",
    "with open(file=\"JointErrors.json\", mode='r') as file:\n",
    "  joint_error_json = json.load(file)\n",
    "\n",
    "with open(file=\"SkeletonErrors.json\", mode='r') as file:\n",
    "  skeleton_error_json = json.load(file)\n",
    "\n",
    "len(exercises_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_names_all = [\"-\", \"Head\", \"Neck\", \"Torso\", \"Waist\", \"Left collar\", \"Left shoulder\", \"Left ebpow\", \"Left wrist\", \"Left hand\", \"-\", \"Right collar\", \"Right shoulder\", \"Right elbow\", \"Right wrist\", \"Right hand\", \"-\", \"Left hip\", \"Left knee\", \"Left ankle\", \"-\", \"Right hip\", \"Right knee\", \"Right ankle\", \"-\"]\n",
    "joint_names = [i for i in joint_names_all if i != '-']\n",
    "\n",
    "body_halves = np.array([\"Upper Half\", \"Lower Half\"])\n",
    "body_parts = np.array([\"Head\", \"Torso\", \"Left arm\", \"Right arm\", \"Left leg\", \"Right leg\"])\n",
    "\n",
    "upper_body_i = [0, 1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
    "lower_body_i = [3, 14, 15, 16, 17, 18, 19]\n",
    "\n",
    "torso_i     = [2, 3, 4, 9]\n",
    "head_i      = [0, 1]\n",
    "left_arm_i  = [5, 6, 7, 8]\n",
    "right_arm_i = [10, 11, 12, 13]\n",
    "left_leg_i  = [14, 15, 16]\n",
    "right_leg_i = [17, 18, 19]\n",
    "\n",
    "joint_errors = []\n",
    "for je in joint_error_json:\n",
    "  joint_errors.append(je[\"Name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "Here we create all graphs and analysis for the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_result_dir(result_dir):\n",
    "  result_dir.mkdir(parents=True, exist_ok=True)\n",
    "  (result_dir / \"fb\").mkdir(parents=True, exist_ok=True)\n",
    "  (result_dir / \"hb\").mkdir(parents=True, exist_ok=True)\n",
    "  (result_dir / \"bp\").mkdir(parents=True, exist_ok=True)\n",
    "  (result_dir / \"jt\").mkdir(parents=True, exist_ok=True)\n",
    "  (result_dir / \"roc\").mkdir(parents=True, exist_ok=True)\n",
    "  (result_dir / \"confusion\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = os.listdir(\"./results\")\n",
    "all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_i = 1\n",
    "result_version = all_results[result_i][:2]\n",
    "result_dir = Path(f\"./figures/results_all/{all_results[result_i]}\")\n",
    "result_dir_general = Path(f\"./figures/results/{result_version}\")\n",
    "\n",
    "prepare_result_dir(result_dir)\n",
    "prepare_result_dir(result_dir_general)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the results of training and testing\n",
    "\n",
    "df_model = pd.read_parquet(f'./results/{all_results[result_i]}/ModelAnalysis.parquet.gzip')\n",
    "df_model[\"difficulty\"] = df_model.apply(lambda x: int(x[\"exercise\"][2]), axis=1)\n",
    "df_model[\"mode\"] = df_model[\"mode\"].apply(lambda x: Mode.from_str(mode_str=x))\n",
    "df_model[\"mode_str\"] = df_model[\"mode\"].apply(lambda x: x.to_str())\n",
    "df_model.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = df_model[\"tp\"]\n",
    "tn = df_model[\"tn\"]\n",
    "fp = df_model[\"fp\"]\n",
    "fn = df_model[\"fn\"]\n",
    "\n",
    "df_model[\"p\"] = df_model[\"tp\"] + df_model[\"fp\"]\n",
    "df_model[\"n\"] = df_model[\"tn\"] + df_model[\"fn\"]\n",
    "df_model[\"p/(p+n)\"] = df_model[\"p\"] / (df_model[\"n\"] + df_model[\"p\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coi = [\"difficulty\", \"p/(p+n)\", \"accuracy\", \"precision\", \"recall\", \"f1\", \"mode_str\"]\n",
    "coi_tf_pn = [\"tp\", \"tn\", \"fp\", \"fn\", \"p\", \"n\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_train = df_model#[df_model[\"train_test\"] == \"train\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full Body Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_fb = df_model[df_model_train[\"mode\"] == Mode.FULL_BODY]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_fb[\"color\"] = df_model_fb[\"train_test\"].apply(lambda x: \"C0\" if x == \"train\" else \"C1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=2, ncols=2, sharey=True, sharex=True, figsize=(15, 5))\n",
    "\n",
    "fb_train = df_model_fb[df_model_fb[\"train_test\"] == \"train\"]\n",
    "fb_test = df_model_fb[df_model_fb[\"train_test\"] == \"test\"]\n",
    "\n",
    "fb_train[[\"epoch\", \"loss\"]].groupby(\"epoch\").mean().plot(y=\"loss\", figsize=(15, 5), title=\"Cross-Entropy Loss\", ax=axs[0][0])\n",
    "fb_test[[\"epoch\", \"loss\"]].groupby(\"epoch\").mean().plot(y=\"loss\", figsize=(15, 5), title=\"Cross-Entropy Loss\", ax=axs[0][0])\n",
    "\n",
    "fb_train[[\"epoch\", \"accuracy\"]].groupby(\"epoch\").mean().plot(y=\"accuracy\" , figsize=(15, 5), title=\"Accuracy\", ax=axs[0][1])\n",
    "fb_test[[\"epoch\", \"accuracy\"]].groupby(\"epoch\").mean().plot(y=\"accuracy\" , figsize=(15, 5), title=\"Accuracy\", ax=axs[0][1])\n",
    "\n",
    "fb_train[[\"epoch\", \"f1\"]].groupby(\"epoch\").mean().plot(y=\"f1\" , figsize=(15, 5), title=\"F1\", ax=axs[1][0])\n",
    "fb_test[[\"epoch\", \"f1\"]].groupby(\"epoch\").mean().plot(y=\"f1\" , figsize=(15, 5), title=\"F1\", ax=axs[1][0])\n",
    "\n",
    "fb_train[[\"epoch\", \"p/(p+n)\"]].groupby(\"epoch\").mean().plot(y=\"p/(p+n)\" , figsize=(15, 5), title=\"Positive guesses\", ax=axs[1][1])\n",
    "fb_test[[\"epoch\", \"p/(p+n)\"]].groupby(\"epoch\").mean().plot(y=\"p/(p+n)\" , figsize=(15, 5), title=\"Positive guesses\", ax=axs[1][1])\n",
    "\n",
    "for ax in axs.flatten():\n",
    "  ax.legend([\"Train\", \"Test\"])\n",
    "\n",
    "fig.suptitle(\"Full Body Error Estimation\")\n",
    "\n",
    "fig.tight_layout(pad=.5)\n",
    "# fig.savefig(result_dir_general / \"fb/FullBody_ErrorEstimation.png\", dpi=300, bbox_inches='tight')\n",
    "# fig.savefig(result_dir / \"fb/FullBody_ErrorEstimation.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Half Body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_hb = df_model[df_model[\"mode\"] == Mode.HALF_BODY]\n",
    "df_model_hb[\"joint\"] = df_model_hb[\"joint_id\"].apply(lambda x: body_halves[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_uh, axs_uh = plt.subplots(nrows=2, ncols=2, sharey=True, sharex=True, figsize=(15, 10))\n",
    "fig_lh, axs_lh = plt.subplots(nrows=2, ncols=2, sharey=True, sharex=True, figsize=(15, 10))\n",
    "\n",
    "vals_lh_tr = df_model_hb[df_model_hb[\"joint\"] == \"Lower Half\"][df_model_hb[\"train_test\"] == \"train\"]\n",
    "vals_lh_te = df_model_hb[df_model_hb[\"joint\"] == \"Lower Half\"][df_model_hb[\"train_test\"] == \"test\"]\n",
    "vals_uh_tr = df_model_hb[df_model_hb[\"joint\"] == \"Upper Half\"][df_model_hb[\"train_test\"] == \"train\"]\n",
    "vals_uh_te = df_model_hb[df_model_hb[\"joint\"] == \"Upper Half\"][df_model_hb[\"train_test\"] == \"test\"]\n",
    "\n",
    "def plot(df, axs):\n",
    "  df = df[[\"epoch\", \"loss\", \"accuracy\", \"f1\", \"p/(p+n)\", \"Avg loss\"]].groupby([\"epoch\"]).mean()\n",
    "  \n",
    "  df.plot(y=\"loss\", figsize=(15, 5), title=\"Loss\", ax=axs[0][0])\n",
    "  df.plot(y=\"accuracy\" , figsize=(15, 5), title=\"Accuracy\", ax=axs[0][1])\n",
    "  df.plot(y=\"f1\" , figsize=(15, 5), title=\"F1\", ax=axs[1][0])\n",
    "  df.plot(y=\"p/(p+n)\" , figsize=(15, 5), title=\"Positive guesses\", ax=axs[1][1])\n",
    "\n",
    "plot(vals_lh_tr, axs_lh)\n",
    "plot(vals_lh_te, axs_lh)\n",
    "\n",
    "for ax in axs_lh.flatten():\n",
    "  ax.legend([\"Train\", \"Test\"])\n",
    "\n",
    "fig_lh.suptitle(\"Lower Half Error Estimation\")\n",
    "fig_lh.tight_layout(pad=.5)\n",
    "# fig.savefig(result_dir_general / \"hb/LowerBody_ErrorEstimation.png\", dpi=300, bbox_inches='tight')\n",
    "# fig.savefig(result_dir / \"hb/LowerBody_ErrorEstimation.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "plot(vals_uh_tr, axs_uh)\n",
    "plot(vals_uh_te, axs_uh)\n",
    "\n",
    "for ax in axs_uh.flatten():\n",
    "  ax.legend([\"Train\", \"Test\"])\n",
    "\n",
    "fig_uh.suptitle(\"Upper Half Error Estimation\")\n",
    "\n",
    "fig_uh.tight_layout(pad=.5)\n",
    "# fig_lh.savefig(result_dir_general / \"hb/UpperBody_ErrorEstimation.png\", dpi=300, bbox_inches='tight')\n",
    "# fig_lh.savefig(result_dir / \"hb/UpperBody_ErrorEstimation.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Body parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_bp = df_model[df_model[\"mode\"] == Mode.BODY_PARTS]\n",
    "df_model_bp[\"joint\"] = df_model_bp[\"joint_id\"].apply(lambda x: body_parts[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot(df, axs, body_part):\n",
    "  df = df[[\"epoch\", \"loss\", \"accuracy\", \"f1\", \"p/(p+n)\", \"Avg loss\"]].groupby([\"epoch\"]).mean()\n",
    "  \n",
    "  df.plot(y=\"loss\",         figsize=(15, 5), title=\"Cross-Entropy Loss\", ax=axs[0][0], legend=False)\n",
    "  df.plot(y=\"accuracy\",     figsize=(15, 5), title=\"Accuracy\", ax=axs[0][1], legend=False)\n",
    "  df.plot(y=\"f1\",           figsize=(15, 5), title=\"F1\", ax=axs[1][0], legend=False)\n",
    "  df.plot(y=\"p/(p+n)\" ,     figsize=(15, 5), title=\"Positive guesses\", ax=axs[1][1], legend=False)\n",
    "\n",
    "for body_part in body_parts:\n",
    "  fig, axs = plt.subplots(nrows=2, ncols=2, sharex=True, sharey=True, figsize=(15, 10))\n",
    "\n",
    "  crit_1 = df_model_bp[\"joint\"] == body_part\n",
    "  crit_train = df_model_bp[\"train_test\"] == \"train\"\n",
    "  crit_test = df_model_bp[\"train_test\"] == \"test\"\n",
    "  crit = crit_1 & crit_train\n",
    "  df_train = df_model_bp[crit]\n",
    "  crit = crit_1 & crit_test\n",
    "  df_test = df_model_bp[crit]\n",
    "\n",
    "  plot(df_train, axs, body_part)\n",
    "  plot(df_test, axs, body_part)\n",
    "  \n",
    "  for ax in axs.flatten():\n",
    "    ax.legend([\"Train\", \"Test\"])\n",
    "    \n",
    "  #fig.tight_layout(pad=.5)\n",
    "  fig.suptitle(f\"Body part Error Estimation ({body_part})\")\n",
    "# fig.savefig(result_dir_general / f\"bp/{body_part}_ErrorEstimation.png\", dpi=300, bbox_inches='tight')\n",
    "# fig.savefig(result_dir / f\"bp/{body_part}_ErrorEstimation.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Joints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_jt = df_model[df_model[\"mode\"] == Mode.JOINTS]\n",
    "df_model_jt[\"joint\"] = df_model_jt[\"joint_id\"].apply(lambda x: joint_names[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(df, axs):\n",
    "  df = df[[\"epoch\", \"loss\", \"accuracy\", \"f1\", \"p/(p+n)\", \"Avg loss\"]].groupby([\"epoch\"]).mean()\n",
    "  \n",
    "  df.plot(y=\"loss\", figsize=(15, 5), title=\"Loss\", ax=axs[0][0], legend=False)\n",
    "  df.plot(y=\"accuracy\" , figsize=(15, 5), title=\"Accuracy\", ax=axs[0][1], legend=False)\n",
    "  df.plot(y=\"f1\" , figsize=(15, 5), title=\"F1\", ax=axs[1][0], legend=False)\n",
    "  df.plot(y=\"p/(p+n)\" , figsize=(15, 5), title=\"Positive guesses\", ax=axs[1][1], legend=False)\n",
    "\n",
    "\n",
    "for joint in joint_names:\n",
    "  fig, axs = plt.subplots(nrows=2, ncols=2, sharex=True, sharey=True, figsize=(15, 10))\n",
    "\n",
    "  crit = df_model_jt[\"joint\"] == joint\n",
    "  df_joint = df_model_jt[crit]\n",
    "  \n",
    "  df_train = df_joint[df_joint[\"train_test\"] == \"train\"]\n",
    "  df_test = df_joint[df_joint[\"train_test\"] == \"test\"]\n",
    "\n",
    "  plot(df_train, axs)\n",
    "  plot(df_test, axs)\n",
    "  for ax in axs.flatten():\n",
    "    ax.legend([\"Train\", \"Test\"])\n",
    "\n",
    "  fig.suptitle(f\"Joint Error Estimation ({joint.replace('ebpow', 'elbow')})\")\n",
    "# fig.savefig(result_dir_general / f\"jt/{joint}_ErrorEstimation.png\", dpi=300, bbox_inches='tight')\n",
    "# fig.savefig(result_dir / f\"jt/{joint}_ErrorEstimation.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_test = df_model[df_model[\"train_test\"] == \"test\"][df_model[\"epoch\"] == 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = df_model_test[[\"mode_str\", \"tp\",\"tn\",\"fp\",\"fn\"]].groupby(\"mode_str\").sum()\n",
    "tp = confusion[\"tp\"]\n",
    "tn = confusion[\"tn\"]\n",
    "fp = confusion[\"fp\"]\n",
    "fn = confusion[\"fn\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fbeta(beta, precision, recall):\n",
    "  return ((1 + beta**2) * precision * recall) / (beta**2 * precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "groups = df_model_test[coi].groupby(\"mode_str\").mean()\n",
    "groups = groups.drop([\"difficulty\", \"precision\", \"recall\"], axis=1)\n",
    "groups[\"p/(p+n)\"] *= 100\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "groups[\"f0.5\"] = fbeta(0.5, precision, recall)\n",
    "groups[\"f1\"] = fbeta(1, precision, recall)\n",
    "groups[\"f2\"] = fbeta(2, precision, recall)\n",
    "groups[\"binary_kappa\"] = (2 * ((tp * tn) - (fn * fp))) / ((tp + fp) * (fp + tn) + (tp + fn) * (fn + tn))\n",
    "groups = groups.sort_index(key=lambda x: [[\"full_body\", \"half_body\", \"body_parts\", \"joints\"].index(i) for i in x])\n",
    "\n",
    "table = groups.rename(columns={\n",
    "        \"p/(p+n)\": \"PPG\",\n",
    "        \"accuracy\": \"Acc\",\n",
    "        \"f1\": \"F1\",\n",
    "        \"f0.5\": \"F0.5\",\n",
    "        \"f2\": \"F2\",\n",
    "        \"binary_kappa\": \"$\\\\kappa$\"\n",
    "      },\n",
    "      index=lambda id: id.replace(\"_\", \" \").title()\n",
    "  ).to_latex(float_format=\"{:.2f}\".format, escape=False)\n",
    "  \n",
    "table = table.replace(\"toprule\", \"hline\")  \n",
    "table = table.replace(\"midrule\", \"hline\")  \n",
    "table = table.replace(\"bottomrule\", \"hline\")  \n",
    "table = table.replace(\"mode_str\",\"Problem Set\")\n",
    "table = table.replace(\"lrrrrrr\", \"p{0.13\\linewidth}\"*7)\n",
    "\n",
    "caption = '{' + f\"The test results of FESDModel{result_version} after 50 epochs of training. Showing the Percentage of Positive Guesses (PPG), the Accuracy (Acc), the F-$\\\\beta$ score calculated with $\\\\beta = [1, 0.5, 2]$ (F1, F0.5, F2 respectively) and the Cohen\\'s Kappa Coefficient ($\\\\kappa$).\" + '}'\n",
    "\n",
    "table = f\"\"\"\\\n",
    "  \\\\begin{'{table}'}[!htbp]\n",
    "      \\\\caption[Test Results of FESDModel{result_version}]{caption}\n",
    "      \\\\label{'{tab:res_'}{result_version}{'}'}\n",
    "      {table}\n",
    "  \\\\end{'{table}'}\n",
    "\"\"\"\n",
    "table\n",
    "with open(\"res_table.tex\", \"w\") as f:\n",
    "  f.write(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = df_model_test[coi].groupby(\"mode_str\").mean()\n",
    "groups = groups.drop([\"difficulty\", \"precision\", \"recall\"], axis=1)\n",
    "groups[\"p/(p+n)\"] *= 100\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "groups[\"f0.5\"] = fbeta(0.5, precision, recall)\n",
    "groups[\"f1\"] = fbeta(1, precision, recall)\n",
    "groups[\"f2\"] = fbeta(2, precision, recall)\n",
    "groups[\"binary_kappa\"] = (2 * ((tp * tn) - (fn * fp))) / ((tp + fp) * (fp + tn) + (tp + fn) * (fn + tn))\n",
    "\n",
    "table = groups.rename(columns={\n",
    "        \"p/(p+n)\": \"Percentage of positive guesses\",\n",
    "        \"accuracy\": \"Accuracy\",\n",
    "        \"f1\": \"F1-Score\",\n",
    "        \"f2\": \"F2-Score\",\n",
    "        \"binary_kappa\": \"Cohen's Kappa Coefficient\"\n",
    "      },\n",
    "      index=lambda id: id.replace(\"_\", \" \").title()\n",
    "  ).to_latex(float_format=\"{:.3f}\".format, escape=False)\n",
    "  \n",
    "table = table.replace(\"toprule\", \"hline\")  \n",
    "table = table.replace(\"midrule\", \"hline\")  \n",
    "table = table.replace(\"bottomrule\", \"hline\")  \n",
    "table = table.replace(\"mode_str\",\"Problem Set\")\n",
    "table = table.replace(\"lllll\", \"p{0.15\\linewidth}p{0.15\\linewidth}p{0.15\\linewidth}p{0.15\\linewidth}p{0.15\\linewidth}p{0.15\\linewidth}\")\n",
    "table = f\"\"\"\\\n",
    "  \\\\begin{'{table}'}[!htbp]\n",
    "      \\\\caption[Test Results of FESDModel{result_version}]{'{The test results of FESDModel'}{result_version}{' after 50 epochs of training.}'}\n",
    "      \\\\label{'{tab:res_'}{result_version}{'}'}\n",
    "      {table}\n",
    "  \\\\end{'{table}'}\n",
    "\"\"\"\n",
    "table\n",
    "# with open(result_dir / \"res_table.tex\", \"w\") as f:\n",
    "#   f.write(table)\n",
    "# with open(result_dir_general / \"res_table.tex\", \"w\") as f:\n",
    "#   f.write(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab = groups.rename(columns={\n",
    "        \"p/(p+n)\": \"Percentage of positive guesses\",\n",
    "        \"accuracy\": \"Accuracy\",\n",
    "        \"f0.5\": \"F0.5-Score\",\n",
    "        \"f1\": \"F1-Score\",\n",
    "        \"f2\": \"F2-Score\",\n",
    "        \"binary_kappa\": \"Cohen's Kappa Coefficient\"\n",
    "      },\n",
    "      index=lambda id: id.replace(\"_\", \" \").title()\n",
    "  )\n",
    "tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full Body Evaluation\n",
    "\n",
    "The evaluation of the whole body as an error boolean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_full_body_test = df_model_test[df_model_test[\"mode\"] == Mode.FULL_BODY]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_full_body_test[coi].groupby([\"difficulty\"]).mean().sort_values(by=\"difficulty\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_full_body_test[coi_tf_pn + [\"difficulty\"]].groupby([\"difficulty\"]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 4, figsize=(7, 2.5), sharey=True)\n",
    "fig.suptitle(f\"Full Body\")\n",
    "difficulty_names = [\"Trivial\", \"Easy\", \"Medium\", \"Hard\"]\n",
    "for difficulty in [0, 1, 2, 3]:\n",
    "  gts = [\"No Error\" if x[0] == 0 else \"Error\" for x in df_model_full_body_test[df_model_full_body_test[\"difficulty\"] == difficulty][\"gts\"].values.tolist()]\n",
    "  preds = [\"No Error\" if x[0] == 0 else \"Error\" for x in df_model_full_body_test[df_model_full_body_test[\"difficulty\"] == difficulty][\"preds\"].values.tolist()]\n",
    "  cm = confusion_matrix(gts, preds, labels=[\"No Error\", \"Error\"])\n",
    "  disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"No Error\", \"Error\"])\n",
    "  disp.plot(ax=axs[difficulty], colorbar=False)\n",
    "  axs[difficulty].set_title(difficulty_names[difficulty])\n",
    "  axs[difficulty].grid(False)\n",
    "  if difficulty != 0:\n",
    "    axs[difficulty].set_ylabel(\"\")\n",
    "\n",
    "# fig.savefig(result_dir_general / \"confusion/full_difficulty.png\", dpi=300, bbox_inches='tight')\n",
    "# fig.savefig(result_dir / \"confusion/full_difficulty.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_confusion(df, result_dir, ps_name, file_name):\n",
    "  fig, axs = plt.subplots(1, 1, figsize=(7, 2.5), sharey=True)\n",
    "\n",
    "  gts = [\"No Error\" if x[0] == 0 else \"Error\" for x in df[\"gts\"].values.tolist()]\n",
    "  preds = [\"No Error\" if x[0] == 0 else \"Error\" for x in df[\"preds\"].values.tolist()]\n",
    "  cm = confusion_matrix(gts, preds, labels=[\"No Error\", \"Error\"])\n",
    "  disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"No Error\", \"Error\"])\n",
    "  disp.plot(ax=axs, colorbar=False)\n",
    "\n",
    "  axs.grid(False)\n",
    "\n",
    "  fig.suptitle(f\"{ps_name}\\nConfusion Matrix\")\n",
    "  plt.tight_layout()\n",
    "  return cm\n",
    "\n",
    "# fig.savefig(result_dir_general / f\"confusion/{file_name}\", dpi=300, bbox_inches='tight')\n",
    "# fig.savefig(result_dir / f\"confusion/{file_name}\", dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = single_confusion(df_model_full_body_test, result_dir, \"Full Body\", \"full_together.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Half Body Evaluation\n",
    "\n",
    "The evaluation of the body split into two parts (upper and lower body), each as an error boolean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "\n",
    "df_model_half_body_test = df_model_test[df_model_test[\"mode\"] == Mode.HALF_BODY]\n",
    "df_model_half_body_test[\"joint_names\"] = df_model_half_body_test[\"joint_id\"].apply(lambda x: body_halves[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_half_body_test[coi + [\"joint_names\"]].groupby([\"joint_names\", \"difficulty\"]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_half_body_test[coi_tf_pn + [\"joint_names\", \"difficulty\"]].groupby([\"joint_names\", \"difficulty\"]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_confusion(df_model_half_body_test, result_dir, \"Half Body\", \"half_together.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_model_half_body_test\n",
    "areas = body_halves\n",
    "\n",
    "rows = 1\n",
    "cols = 2\n",
    "assert len(areas) == rows * cols\n",
    "\n",
    "figure = plt.figure(layout='constrained', figsize=(4 * cols, 4.5 * rows))\n",
    "subfigs = figure.subfigures(rows, cols)\n",
    "\n",
    "for i, area in enumerate(areas):\n",
    "  axs = subfigs.flatten()[i].subplots(1, 1)\n",
    "  #subfigs.flatten()[i].suptitle(area, fontsize='x-large')\n",
    "\n",
    "  crit = df[\"joint_names\"] == area\n",
    "  gts = [\"No Error\" if x[0] == 0 else \"Error\" for x in df[crit][\"gts\"].values.tolist()]\n",
    "  preds = [\"No Error\" if x[0] == 0 else \"Error\" for x in df[crit][\"preds\"].values.tolist()]\n",
    "\n",
    "  cm = confusion_matrix(gts, preds, labels=[\"No Error\", \"Error\"])\n",
    "  disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"No Error\", \"Error\"])\n",
    "  disp.plot(ax=axs, colorbar=False)\n",
    "\n",
    "  axs.grid(False)\n",
    "  axs.set_title(area, fontsize='x-large')\n",
    "\n",
    "#plt.tight_layout()\n",
    "# fig.savefig(result_dir_general / \"confusion/body_halves_half.png\", dpi=300, bbox_inches='tight')\n",
    "# fig.savefig(result_dir / \"confusion/body_halves_half.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(layout='constrained', figsize=(7, 2.5 * len(body_halves)))\n",
    "subfigs = figure.subfigures(2, 1)\n",
    "\n",
    "difficulty_names = [\"Trivial\", \"Easy\", \"Medium\", \"Hard\"]\n",
    "for i, half in enumerate(body_halves):\n",
    "  axs = subfigs[i].subplots(1, 4, sharey=True)\n",
    "  subfigs[i].suptitle(half, fontsize='x-large')\n",
    "  for difficulty in [0, 1, 2, 3]:  \n",
    "    crit_1 = df_model_half_body_test[\"difficulty\"] == difficulty\n",
    "    crit_2 = df_model_half_body_test[\"joint_names\"] == half\n",
    "    crit = crit_1 & crit_2\n",
    "    gts = [\"No Error\" if x[0] == 0 else \"Error\" for x in df_model_half_body_test[crit][\"gts\"].values.tolist()]\n",
    "    preds = [\"No Error\" if x[0] == 0 else \"Error\" for x in df_model_half_body_test[crit][\"preds\"].values.tolist()]\n",
    "\n",
    "    cm = confusion_matrix(gts, preds, labels=[\"No Error\", \"Error\"])\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"No Error\", \"Error\"])\n",
    "    disp.plot(ax=axs[difficulty], colorbar=False)\n",
    "\n",
    "    axs[difficulty].grid(False)\n",
    "    axs[difficulty].set_title(difficulty_names[difficulty])\n",
    "    \n",
    "    if i < len(body_halves) - 1:\n",
    "      axs[difficulty].set_xlabel(\"\") \n",
    "      axs[difficulty].set_xticks([]) \n",
    "    if difficulty != 0:\n",
    "      axs[difficulty].set_ylabel(\"\")\n",
    "\n",
    "#plt.tight_layout()\n",
    "# fig.savefig(result_dir_general / \"confusion/body_halves_difficulty.png\", dpi=300, bbox_inches='tight')\n",
    "# fig.savefig(result_dir / \"confusion/body_halves_difficulty.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Body part Evaluation\n",
    "\n",
    "The evaluation of the body split into body_parts (Head, Torso, Left and Right Arm, and Left and Right Leg), each as an error boolean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "\n",
    "df_model_body_parts_test = df_model_test[df_model_test[\"mode\"] == Mode.BODY_PARTS]\n",
    "df_model_body_parts_test[\"joint_names\"] = df_model_body_parts_test[\"joint_id\"].apply(lambda x: body_parts[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_body_parts_test[coi + [\"joint_names\"]].groupby([\"joint_names\", \"difficulty\"]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_body_parts_test[coi_tf_pn + [\"joint_names\", \"difficulty\"]].groupby([\"joint_names\", \"difficulty\"]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_confusion(df_model_body_parts_test, result_dir, \"Body Parts\", \"body_parts_together.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = 2\n",
    "cols = 3\n",
    "\n",
    "figure = plt.figure(layout='constrained', figsize=(4 * cols, 4.5 * rows))\n",
    "subfigs = figure.subfigures(rows, cols)\n",
    "\n",
    "for i, body_part in enumerate(body_parts):\n",
    "  axs = subfigs.flatten()[i].subplots(1, 1)\n",
    "\n",
    "  crit = df_model_body_parts_test[\"joint_names\"] == body_part\n",
    "  gts = [\"No Error\" if x[0] == 0 else \"Error\" for x in df_model_body_parts_test[crit][\"gts\"].values.tolist()]\n",
    "  preds = [\"No Error\" if x[0] == 0 else \"Error\" for x in df_model_body_parts_test[crit][\"preds\"].values.tolist()]\n",
    "\n",
    "  cm = confusion_matrix(gts, preds, labels=[\"No Error\", \"Error\"])\n",
    "  disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"No Error\", \"Error\"])\n",
    "  disp.plot(ax=axs, colorbar=False)\n",
    "\n",
    "  axs.grid(False)\n",
    "  axs.set_title(body_part, fontsize='x-large')\n",
    "\n",
    "#plt.tight_layout()\n",
    "# fig.savefig(result_dir_general / \"confusion/body_parts_part.png\", dpi=300, bbox_inches='tight')\n",
    "# fig.savefig(result_dir / \"confusion/body_parts_part.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = 3\n",
    "cols = 2\n",
    "assert rows * cols == len(body_parts)\n",
    "\n",
    "figure = plt.figure(layout='constrained', figsize=(7 * cols, 2.5 * rows))\n",
    "subfigs = figure.subfigures(nrows=rows, ncols=cols)\n",
    "\n",
    "difficulty_names = [\"Trivial\", \"Easy\", \"Medium\", \"Hard\"]\n",
    "for i, body_part in enumerate(body_parts):\n",
    "  axs = subfigs.flatten()[i].subplots(1, 4, sharey=True)\n",
    "  subfigs.flatten()[i].suptitle(body_part, fontsize='x-large')\n",
    "  for difficulty in [0, 1, 2, 3]:  \n",
    "    crit_1 = df_model_body_parts_test[\"difficulty\"] == difficulty\n",
    "    crit_2 = df_model_body_parts_test[\"joint_names\"] == body_part\n",
    "    crit = crit_1 & crit_2\n",
    "    gts = [\"No Error\" if x[0] == 0 else \"Error\" for x in df_model_body_parts_test[crit][\"gts\"].values.tolist()]\n",
    "    preds = [\"No Error\" if x[0] == 0 else \"Error\" for x in df_model_body_parts_test[crit][\"preds\"].values.tolist()]\n",
    "\n",
    "    cm = confusion_matrix(gts, preds, labels=[\"No Error\", \"Error\"])\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"No Error\", \"Error\"])\n",
    "    disp.plot(ax=axs[difficulty], colorbar=False)\n",
    "\n",
    "    axs[difficulty].grid(False)\n",
    "    axs[difficulty].set_title(difficulty_names[difficulty])\n",
    "    if i < len(body_parts) - 2:\n",
    "      axs[difficulty].set_xlabel(\"\") \n",
    "      axs[difficulty].set_xticks([]) \n",
    "    if difficulty != 0 or i % 2 == 1:\n",
    "      axs[difficulty].set_ylabel(\"\")\n",
    "      axs[difficulty].set_yticks([]) \n",
    "\n",
    "#plt.tight_layout()\n",
    "# fig.savefig(result_dir_general / \"confusion/body_parts_difficulty.png\", dpi=300, bbox_inches='tight')\n",
    "# fig.savefig(result_dir / \"confusion/body_parts_difficulty.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Joint Evaluation\n",
    "\n",
    "The evaluation of the body as an error class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "\n",
    "df_model_joints_test = df_model_test[df_model_test[\"mode\"] == Mode.JOINTS]\n",
    "df_model_joints_test[\"joint_names\"] = df_model_joints_test[\"joint_id\"].apply(lambda x: joint_names[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_joints_test[coi + [\"joint_names\"]].groupby([\"joint_names\", \"difficulty\"]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_joints_test[coi_tf_pn + [\"difficulty\", \"joint_names\"]].groupby([\"joint_names\", \"difficulty\"]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_joints_test[coi + [\"joint_names\"]].groupby([\"joint_names\", \"difficulty\"]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_confusion(df_model_joints_test, result_dir, \"Joints\", \"joints_together.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = 5\n",
    "cols = 4\n",
    "assert rows * cols == len(joint_names)\n",
    "\n",
    "figure = plt.figure(layout='constrained', figsize=(4 * cols, 4.5 * rows))\n",
    "subfigs = figure.subfigures(rows, cols)\n",
    "\n",
    "for i, body_part in enumerate(joint_names):\n",
    "  axs = subfigs.flatten()[i].subplots(1, 1)\n",
    "\n",
    "  crit = df_model_joints_test[\"joint_names\"] == body_part\n",
    "  gts = [\"No Error\" if x[0] == 0 else \"Error\" for x in df_model_joints_test[crit][\"gts\"].values.tolist()]\n",
    "  preds = [\"No Error\" if x[0] == 0 else \"Error\" for x in df_model_joints_test[crit][\"preds\"].values.tolist()]\n",
    "\n",
    "  cm = confusion_matrix(gts, preds, labels=[\"No Error\", \"Error\"])\n",
    "  disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"No Error\", \"Error\"])\n",
    "  disp.plot(ax=axs, colorbar=False)\n",
    "\n",
    "  axs.grid(False)\n",
    "  axs.set_title(body_part, fontsize='x-large')\n",
    "\n",
    "#plt.tight_layout()\n",
    "# fig.savefig(result_dir_general / \"confusion/joints_joint.png\", dpi=300, bbox_inches='tight')\n",
    "# fig.savefig(result_dir / \"confusion/joints_joint.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = 5\n",
    "cols = 4\n",
    "assert rows * cols == len(joint_names)\n",
    "\n",
    "figure = plt.figure(layout='constrained', figsize=(7 * cols, 2.5 * rows))\n",
    "subfigs = figure.subfigures(nrows=rows, ncols=cols)\n",
    "\n",
    "difficulty_names = [\"Trivial\", \"Easy\", \"Medium\", \"Hard\"]\n",
    "for i, joint_name in enumerate(joint_names):\n",
    "  axs = subfigs.flatten()[i].subplots(1, 4, sharey=True)\n",
    "  subfigs.flatten()[i].suptitle(joint_name, fontsize='x-large')\n",
    "  for difficulty in [0, 1, 2, 3]:  \n",
    "    crit_1 = df_model_joints_test[\"difficulty\"] == difficulty\n",
    "    crit_2 = df_model_joints_test[\"joint_names\"] == joint_name\n",
    "    crit = crit_1 & crit_2\n",
    "    gts = [\"No Error\" if x[0] == 0 else \"Error\" for x in df_model_joints_test[crit][\"gts\"].values.tolist()]\n",
    "    preds = [\"No Error\" if x[0] == 0 else \"Error\" for x in df_model_joints_test[crit][\"preds\"].values.tolist()]\n",
    "\n",
    "    cm = confusion_matrix(gts, preds, labels=[\"No Error\", \"Error\"])\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"No Error\", \"Error\"])\n",
    "    disp.plot(ax=axs[difficulty], colorbar=False)\n",
    "\n",
    "    axs[difficulty].grid(False)\n",
    "    axs[difficulty].set_title(difficulty_names[difficulty])\n",
    "    if i < (rows - 1) * cols:\n",
    "      axs[difficulty].set_xlabel(\"\") \n",
    "      axs[difficulty].set_xticks([]) \n",
    "    if difficulty != 0 or i % cols != 0:\n",
    "      axs[difficulty].set_ylabel(\"\")\n",
    "      axs[difficulty].set_yticks([]) \n",
    "\n",
    "#plt.tight_layout()\n",
    "# fig.savefig(result_dir_general / \"confusion/joints_difficulty.png\", dpi=300, bbox_inches='tight')\n",
    "# fig.savefig(result_dir / \"confusion/joints_difficulty.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_auc(y_true, y_score, ax, obj, i):\n",
    "  if i == 0:\n",
    "    ns_probs = [0 for _ in range(len(y_true))]\n",
    "    ns_auc = roc_auc_score(y_true, ns_probs)\n",
    "    ns_fpr, ns_tpr, _ = roc_curve(y_true, ns_probs)\n",
    "    plt.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill', color=\"C0\")\n",
    "  lr_auc = roc_auc_score(y_true, y_score)\n",
    "  # summarize scores\n",
    "  print('ROC AUC=%.3f' % (lr_auc))\n",
    "  # calculate roc curves\n",
    "  lr_fpr, lr_tpr, _ = roc_curve(y_true, y_score)\n",
    "  # plot the roc curve for the model\n",
    "  plt.plot(lr_fpr, lr_tpr, label=obj, color=f\"C{i+1}\")\n",
    "  # axis labels\n",
    "  plt.xlabel('False Positive Rate')\n",
    "  plt.ylabel('True Positive Rate')\n",
    "  # show the legend\n",
    "  plt.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fb = df_model[df_model[\"mode\"] == Mode.FULL_BODY]\n",
    "df_fb = df_fb[df_fb[\"train_test\"] == \"test\"]\n",
    "df_fb = df_fb[df_fb[\"epoch\"] == 50]\n",
    "y_true = (df_fb[\"gts\"].apply(lambda x: int(x[0]))).to_list()\n",
    "y_score = (df_fb[\"confidences\"].apply(lambda x: float(x[0]))).to_list()\n",
    "\n",
    "plot_roc_auc(y_true, y_score, ax, \"Full Body\", 0)\n",
    "fig = plt.gcf()\n",
    "# fig.savefig(result_dir_general / \"roc/fb.png\", dpi=300, bbox_inches='tight')\n",
    "# fig.savefig(result_dir / \"roc/fb.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Half Body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, obj in enumerate(body_halves):\n",
    "  df_hb = df_model[df_model[\"mode\"] == Mode.HALF_BODY]\n",
    "  df_hb = df_hb[df_hb[\"joint_id\"] == i]\n",
    "  df_hb = df_hb[df_hb[\"train_test\"] == \"test\"]\n",
    "  df_hb = df_hb[df_hb[\"epoch\"] == 50]\n",
    "  y_true = (df_hb[\"gts\"].apply(lambda x: 1 if int(x[0]) == 2 else int(x[0]))).to_numpy()\n",
    "  y_score = (df_hb[\"confidences\"].apply(lambda x: float(x[0]))).to_numpy()\n",
    "\n",
    "  plot_roc_auc(y_true, y_score, ax, obj, i)\n",
    "fig = plt.gcf()\n",
    "# fig.savefig(result_dir_general / \"roc/hb.png\", dpi=300, bbox_inches='tight')\n",
    "# fig.savefig(result_dir / \"roc/hb.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Body Parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, obj in enumerate(body_parts):\n",
    "  df = df_model[df_model[\"mode\"] == Mode.BODY_PARTS]\n",
    "  df = df[df[\"joint_id\"] == i]\n",
    "  df = df[df[\"train_test\"] == \"test\"]\n",
    "  df = df[df[\"epoch\"] == 50]\n",
    "  y_true = (df[\"gts\"].apply(lambda x: int(x[0]))).to_numpy()\n",
    "  y_score = (df[\"confidences\"].apply(lambda x: float(x[0]))).to_numpy()\n",
    "\n",
    "  plot_roc_auc(y_true, 1- y_score, ax, obj, i)\n",
    "fig = plt.gcf()\n",
    "# fig.savefig(result_dir_general / \"roc/bp.png\", dpi=300, bbox_inches='tight')\n",
    "# fig.savefig(result_dir / \"roc/bp.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_model[df_model[\"mode\"] == Mode.JOINTS]\n",
    "df = df[df[\"joint_id\"] == i]\n",
    "df = df[df[\"train_test\"] == \"test\"]\n",
    "df = df[df[\"epoch\"] == 50]\n",
    "y_true = (df[\"gts\"].apply(lambda x: int(x[0] != 0))).to_numpy()\n",
    "y_score = (df[\"confidences\"].apply(lambda x: (x[0]))).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, obj in enumerate(joint_names):\n",
    "  df = df_model[df_model[\"mode\"] == Mode.JOINTS]\n",
    "  df = df[df[\"joint_id\"] == i]\n",
    "  df = df[df[\"train_test\"] == \"test\"]\n",
    "  df = df[df[\"epoch\"] == 50]\n",
    "  y_true = (df[\"gts\"].apply(lambda x: int(x[0] != 0))).to_numpy()\n",
    "  y_score = (df[\"confidences\"].apply(lambda x: (x[0]))).to_numpy()\n",
    "\n",
    "  plot_roc_auc(y_true, y_score, ax, obj, i)\n",
    "  \n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=3)\n",
    "fig = plt.gcf()\n",
    "# fig.savefig(result_dir_general / \"roc/jt.png\", dpi=300, bbox_inches='tight')\n",
    "# fig.savefig(result_dir / \"roc/jt.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
